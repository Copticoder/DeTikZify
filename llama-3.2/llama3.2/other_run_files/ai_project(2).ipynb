{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/omar.el-herraoui/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moe2015\u001b[0m (\u001b[33moe2015-mbzuai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/omar.el-herraoui/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/omar.el-herraoui/Downloads/ai_project/DeTikZify/detikzify/evaluate/wandb/run-20241108_143914-r5p52k6q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oe2015-mbzuai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/r5p52k6q' target=\"_blank\">peach-dream-13</a></strong> to <a href='https://wandb.ai/oe2015-mbzuai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oe2015-mbzuai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset' target=\"_blank\">https://wandb.ai/oe2015-mbzuai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oe2015-mbzuai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/r5p52k6q' target=\"_blank\">https://wandb.ai/oe2015-mbzuai/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/r5p52k6q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token = \"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\")\n",
    "\n",
    "wandb.login(key=\"0689fc9447a488ce270ed7b6220641e9eb65a62d\")\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "new_model = \"llama-3.2-3b-detikzfy\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "0.42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omarelherraoui/miniconda3/envs/gpu/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m \u001b[39mdel\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/site-packages/transformers/modeling_utils.py:3479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283c02125a1748279f1c85462b2eb329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/auto/processing_auto.py:230: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# QLoRA config\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# %pip install -U bitsandbytes\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch_dtype,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "   model_id,\n",
    "   use_auth_token = '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',  # Insert your Hugging Face token here\n",
    "   torch_dtype=torch.bfloat16,\n",
    "   device_map=\"auto\",\n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "   model_id,\n",
    "   use_auth_token = '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',  # Insert your Hugging Face token here\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 18:24:57,226 - INFO - PyTorch version 2.4.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\documentclass[10pt, oneside]{amsart}\n",
      "\\usepackage{amssymb}\n",
      "\\usepackage{amsmath}\n",
      "\\usepackage{color}\n",
      "\\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage[colorlinks=true, pdfstartview=FitV,linkcolor=ForestGreen,citecolor=ForestGreen, urlcolor=black]{hyperref}\n",
      "\\usepackage{tikz}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\begin{tikzpicture}[scale =1]\n",
      "\\draw[dashed] (-6, 0) -- (6, 0) node[anchor=north, scale=1] {$Q_1$};\n",
      "% big square\n",
      "  \\draw [blue] (-3.5,0) -- (3.5,0); \n",
      "   \\draw[blue] (-3.5,-7) -- (3.5,-7);\n",
      "  \\draw [blue](-3.5,0) -- (-3.5,-7);\n",
      "  \\draw [blue](3.5,0) -- (3.5,-7) node[anchor=north, scale=1] {$Q_{\\frac{1}{2}}$};\n",
      "  %medium\n",
      "   \\draw [violet](-1,0) -- (1, 0);\n",
      "   \\draw [violet](-1,-2) -- (1, -2);\n",
      "   \\draw [violet](-1,0) -- (-1, -2);\n",
      "   \\draw [violet](1,0) -- (1, -2)  node[anchor=north, scale=1] {$Q_{r_0}$};\n",
      "   %medium past\n",
      "    \\draw[violet] (-1,-4) -- (1, -4);\n",
      "   \\draw [violet](-1,-6) -- (1, -6);\n",
      "   \\draw [violet](-1,-4) -- (-1, -6);\n",
      "   \\draw [violet](1,-4) -- (1, -6)  node[anchor=north, scale=1] {$Q^-_{r_0}$};\n",
      "     \\draw [orange](-0.5,0) -- (0.5, 0);\n",
      "   \\draw[orange] (-0.5,-1) -- (0.5, -1);\n",
      "   \\draw [orange](-0.5,0) -- (-0.5, -1);\n",
      "   \\draw [orange](0.5,0) -- (0.5, -1)  node[anchor=north, scale=0.71] {$Q_{\\frac{r_0}{2}}$};\n",
      "    \\draw[orange] {(-0.5,-4.5) -- (0.5, -4.5)};\n",
      "   \\draw [orange](-0.5,-5.5) -- (0.5, -5.5);\n",
      "   \\draw [orange](-0.5,-4.5) -- (-0.5, -5.5);\n",
      "   \\draw [orange](0.5,-4.5) -- (0.5, -5.5) node[anchor=north, scale=0.71] {$\\tilde Q^-_{\\frac{r_0}{2}}$};\n",
      "     \\draw [red](-0.25,0) -- (0.25, 0);\n",
      "   \\draw [red](-0.25,-0.5) -- (0.25, -0.5);\n",
      "   \\draw[red] (-0.25,0) -- (-0.25, -0.5);\n",
      "   \\draw[red] (0.25,0) -- (0.25, -0.5) node[anchor=north, scale=0.7] {$Q_{\\frac{r_0}{4}}$};\n",
      "     \\draw[red] (-0.25,-4.5) -- (0.25, -4.5);\n",
      "  \\draw[red] (-0.25,-5) -- (0.25, -5);\n",
      "   \\draw[red] (-0.25,-4.5) -- (-0.25, -5);\n",
      "   \\draw[red] (0.25,-4.5) -- (0.25, -5)node[anchor=north, scale=0.7] {$\\tilde Q^-_{\\frac{r_0}{4}}$};\n",
      "\\end{tikzpicture}\n",
      "\n",
      "\\end{document}\n",
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=384x384 at 0x7F35042B8C10>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# full dataset\n",
    "ds_test = load_dataset(\"nllg/datikz-v2\", split=\"test\")\n",
    "# only the train split\n",
    "# ds_train = load_dataset(\"nllg/datikz-v2\", split=\"train\")\n",
    "# for sample in ds:\n",
    "#     print(sample[\"code\"])\n",
    "print(ds_test[0]['code'])\n",
    "print(ds_test[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import Metrics\n",
    "from torchmetrics.image.kid import KernelInceptionDistance as KID\n",
    "from torchmetrics.text import ExtendedEditDistance\n",
    "from eed import TexEditDistance\n",
    "# from dreamsim1 import DreamSim  # Ensure dreamsim is installed and imported\n",
    "from crystalbleu1 import CrystalBLEU  # Ensure crystalbleu is installed and imported\n",
    "\n",
    "# Define Metric Classes\n",
    "# class TexEditDistance(ExtendedEditDistance):\n",
    "#     def compute(self, preds, target):\n",
    "#         return super().compute(preds, target)\n",
    "\n",
    "class ImageSim:\n",
    "    def compute(self, img1, img2):\n",
    "        return torch.cosine_similarity(img1, img2).item()\n",
    "\n",
    "class DreamSim:\n",
    "    def __init__(self, model_name=\"ensemble\"):\n",
    "        self.model, self.processor = model, processor\n",
    "\n",
    "    def compute(self, img1, img2):\n",
    "        img1, img2 = self.processor(img1), self.processor(img2)\n",
    "        with torch.no_grad():\n",
    "            return 1 - self.model(img1, img2).item()  # Higher is better\n",
    "\n",
    "# class CrystalBLEU:\n",
    "#     def __init__(self, corpus, k=500, n=4):\n",
    "#         self.corpus = corpus\n",
    "#         self.k = k\n",
    "#         self.n = n\n",
    "\n",
    "#     def compute(self, references, hypotheses):\n",
    "#         return CrystalBLEU(list_of_references=references, hypotheses=hypotheses)\n",
    "\n",
    "# Initialize Metrics\n",
    "kid_metric = KID()\n",
    "tex_edit_distance = TexEditDistance()\n",
    "image_sim_metric = ImageSim()\n",
    "dreamsim_metric = DreamSim()\n",
    "corpus_bleu_metric = CrystalBLEU(corpus=[ex[\"code\"] for ex in ds_test])  # Corpus from the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1000x600 at 0x7F1E7D1C53D0>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcz-J3iR2bEGcCSLzay07Rqfj5tTakp2EMTTN0x6nKYGLS5yWl0unoSpj2S0-mrWpDtMqjl1fAgH6pVkKJekQEY_kwzL6QNOdf143Yt66znQ0EpfLvx6CLFOqw41oeOYmhPZ6Qrlb5AjEr4AenIOgBMTWTD?key=vhLUYntaS9QOx531XpJH3g\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|image|><|begin_of_text|>Describe the Image in 10 words or less. I'm not able to provide information about the image. I can give you an idea of what's happening in the image, but not who's in it. I can provide a general description of the image, but not names. I can give you an idea of the image's style, but not who's in it. I can provide a brief summary of the image, but not names. I can give you an idea of the image's content, but not who's in it. I can provide a general description of the image, but not names. I can give you an idea of the image's style, but not who's in it. I can provide a brief summary of the image, but not names. I can give you an idea of the image's content, but not who's in it. I can provide a general description of the image, but not names. I can give you an idea of the image's style, but not who's in it. I can provide a brief summary of the image, but not names. I can give you an idea of the image's content, but not who's in it. I can provide a general description of the image, but not names. I can give\n"
     ]
    }
   ],
   "source": [
    "url = \"https://d2sofvawe08yqg.cloudfront.net/quickstartwithai/s_hero?1728376971\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = ds_test[0]['image']\n",
    "prompt = \"<|image|><|begin_of_text|>Describe the Image in 10 words\"\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=250)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token not found in tokenizer vocabulary.\n"
     ]
    }
   ],
   "source": [
    "eos_token = \"<|endoftext|>\"  # Example token, replace with the actual token used by your model\n",
    "if eos_token in tokenizer.get_vocab():\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "    model.config.eos_token_id = eos_token_id\n",
    "    print(\"EOS token ID set to:\", eos_token_id)\n",
    "else:\n",
    "    print(\"EOS token not found in tokenizer vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Assuming ds_test[0]['image'] is a PIL image\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m generated_code \u001b[39m=\u001b[39m generate_tikz_code(ds_test[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(generated_code)\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mgenerate_tikz_code\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      8\u001b[0m messages \u001b[39m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     10\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m},  \u001b[39m# Placeholder, actual image tensor will be used in processor\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mGenerate LaTeX code that draws this scientific figure using TikZ. Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. Don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt forget to include \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39musepackage\u001b[39m\u001b[39m{tikz}\u001b[39;00m\u001b[39m! Return your result in a ```latex code block. The output should not be more than 300 words.\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     12\u001b[0m     ]}\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[39m# Prepare inputs for the model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Assuming 'processor' can handle such structured input appropriately:\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m input_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mapply_chat_template(messages, add_generation_prompt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# Adjust this call based on your actual processing function\u001b[39;00m\n\u001b[1;32m     18\u001b[0m inputs \u001b[39m=\u001b[39m processor(image\u001b[39m=\u001b[39mimage, text\u001b[39m=\u001b[39minput_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Generate the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1801\u001b[0m     tokenizer_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1803\u001b[0m chat_template \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_chat_template(chat_template, tools)\n\u001b[1;32m   1805\u001b[0m \u001b[39mif\u001b[39;00m return_assistant_tokens_mask \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m%\u001b[39m\u001b[39m-?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*generation\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*-?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m%\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m, chat_template):\n\u001b[1;32m   1806\u001b[0m     logger\u001b[39m.\u001b[39mwarning_once(\n\u001b[1;32m   1807\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[39m{\u001b[39m\u001b[39m% g\u001b[39;00m\u001b[39meneration \u001b[39m\u001b[39m%\u001b[39m\u001b[39m}` keyword.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1808\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1967\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1965\u001b[0m         chat_template \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_template\n\u001b[1;32m   1966\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1968\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1969\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39margument was passed! For information about writing templates and setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1971\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1972\u001b[0m         )\n\u001b[1;32m   1974\u001b[0m \u001b[39mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_tikz_code(image):\n",
    "    # Ensure the image is in RGB for consistency\n",
    "    # image = image.convert(\"RGB\")\n",
    "\n",
    "    # Apply transformation\n",
    "\n",
    "    # Constructing the chat-like prompt format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},  # Placeholder, actual image tensor will be used in processor\n",
    "            {\"type\": \"text\", \"text\": \"Generate LaTeX code that draws this scientific figure using TikZ. Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. Don't forget to include \\\\usepackage{tikz}! Return your result in a ```latex code block. The output should not be more than 300 words.\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # Prepare inputs for the model\n",
    "    # Assuming 'processor' can handle such structured input appropriately:\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)  # Adjust this call based on your actual processing function\n",
    "    inputs = processor(image=image, text=input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the output\n",
    "    output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "    # Decode and return the output\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "# Assuming ds_test[0]['image'] is a PIL image\n",
    "generated_code = generate_tikz_code(ds_test[0]['image'])\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, [128000, 12840, 374, 420, 2217, 30]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m input_ids \u001b[39m=\u001b[39m [cls_token_id] \u001b[39m+\u001b[39m input_ids\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(input_ids)\n\u001b[0;32m---> 16\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(input_ids)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(pixel_values\u001b[39m=\u001b[39mpixel_values, input_ids\u001b[39m=\u001b[39minput_ids, max_length\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(processor\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "\n",
    "pixel_values = processor(images=ds_test[0]['image'], return_tensors=\"pt\").pixel_values\n",
    "\n",
    "question = \"what is this image?\"\n",
    "\n",
    "input_ids = processor(text=question, add_special_tokens=False).input_ids\n",
    "\n",
    "cls_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "if cls_token_id is None:\n",
    "    cls_token_id = processor.tokenizer.convert_tokens_to_ids(processor.tokenizer.cls_token)\n",
    "\n",
    "input_ids = [cls_token_id] + input_ids\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)\n",
    "\n",
    "print(processor.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n"
     ]
    },
    {
     "ename": "KaggleApiHTTPError",
     "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/metaresearch/llama-3.2-vision/pyTorch/11b-vision/1\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/kagglehub/exceptions.py:58\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/models/metaresearch/llama-3.2-vision/pyTorch/11b-vision/1/files?page_size=25",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKaggleApiHTTPError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkagglehub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Download latest version\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m path \u001b[39m=\u001b[39m kagglehub\u001b[39m.\u001b[39;49mmodel_download(\u001b[39m\"\u001b[39;49m\u001b[39mmetaresearch/llama-3.2-vision/pyTorch/11b-vision\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPath to model files:\u001b[39m\u001b[39m\"\u001b[39m, path)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/kagglehub/models.py:30\u001b[0m, in \u001b[0;36mmodel_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     28\u001b[0m h \u001b[39m=\u001b[39m parse_model_handle(handle)\n\u001b[1;32m     29\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading Model: \u001b[39m\u001b[39m{\u001b[39;00mh\u001b[39m.\u001b[39mto_url()\u001b[39m}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m\"\u001b[39m, extra\u001b[39m=\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mmodel_resolver(h, path, force_download\u001b[39m=\u001b[39;49mforce_download)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/kagglehub/registry.py:23\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m impl \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impls):\n\u001b[1;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m impl\u001b[39m.\u001b[39mis_supported(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 23\u001b[0m         \u001b[39mreturn\u001b[39;00m impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         fails\u001b[39m.\u001b[39mappend(\u001b[39mtype\u001b[39m(impl)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/kagglehub/http_resolver.py:103\u001b[0m, in \u001b[0;36mModelHttpResolver.__call__\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m     98\u001b[0m     api_client\u001b[39m.\u001b[39mdownload_file(url_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m path, out_path, h)\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39m# List the files and decide how to download them:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# - <= 25 files: Download files in parallel\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# > 25 files: Download the archive and uncompress\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     (files, has_more) \u001b[39m=\u001b[39m _list_files(api_client, h)\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m has_more:\n\u001b[1;32m    105\u001b[0m         \u001b[39m# Downloading the full archived bundle.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m         archive_path \u001b[39m=\u001b[39m get_cached_archive_path(h)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/kagglehub/http_resolver.py:173\u001b[0m, in \u001b[0;36m_list_files\u001b[0;34m(api_client, h)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_list_files\u001b[39m(api_client: KaggleApiV1Client, h: ModelHandle) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]:\n\u001b[0;32m--> 173\u001b[0m     json_response \u001b[39m=\u001b[39m api_client\u001b[39m.\u001b[39;49mget(_build_list_model_instance_version_files_url_path(h), h)\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfiles\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m json_response:\n\u001b[1;32m    175\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInvalid ListModelInstanceVersionFiles API response. Expected to include a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfiles\u001b[39m\u001b[39m'\u001b[39m\u001b[39m field\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/kagglehub/clients.py:110\u001b[0m, in \u001b[0;36mKaggleApiV1Client.get\u001b[0;34m(self, path, resource_handle)\u001b[0m\n\u001b[1;32m    103\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_url(path)\n\u001b[1;32m    104\u001b[0m \u001b[39mwith\u001b[39;00m requests\u001b[39m.\u001b[39mget(\n\u001b[1;32m    105\u001b[0m     url,\n\u001b[1;32m    106\u001b[0m     headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: get_user_agent()},\n\u001b[1;32m    107\u001b[0m     auth\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_auth(),\n\u001b[1;32m    108\u001b[0m     timeout\u001b[39m=\u001b[39m(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[1;32m    109\u001b[0m ) \u001b[39mas\u001b[39;00m response:\n\u001b[0;32m--> 110\u001b[0m     kaggle_api_raise_for_status(response, resource_handle)\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_for_version_update(response)\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/kagglehub/exceptions.py:80\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     72\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m     73\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResource not found at URL: \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m \u001b[39m# Default handling\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mraise\u001b[39;00m KaggleApiHTTPError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mKaggleApiHTTPError\u001b[0m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/metaresearch/llama-3.2-vision/pyTorch/11b-vision/1\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"metaresearch/llama-3.2-vision/pyTorch/11b-vision\")\n",
    "\n",
    "base_model = \"/kaggle/input/llama-3.2-vision/transformers/11b-vision-instruct/1\"\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tikz_code(image):\n",
    "    prompt = (\n",
    "        \"This is a picture of a scientific figure. <|image|> Generate LaTeX code that draws this scientific figure using TikZ. \"\n",
    "        \"Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. \"\n",
    "        \"Don't forget to include \\\\usepackage{tikz}! Return your result in a ```latex code block. The output should not be more than 300 words.\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"This is a picture of a scientific figure. <|image|> describe this image.\"\n",
    "    )\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=200)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|>This is a picture of a scientific figure. <|image|> describe this image. I'm not able to provide information about the image. I can give you an idea of what's happening in the image, but not names of individuals. I can provide some context, but not specific names. I can give you an idea of what's happening in the image, but not specific names. I can provide some context, but not specific names. I can give you an idea of what's happening in the image, but not specific names. I can provide some context, but not specific names. I can give you an idea of what's happening in the image, but not specific names. I can provide some context, but not specific names. I can give you an idea of what's happening in the image, but not specific names. I can provide some context, but not specific names. I can give you an idea of what's happening in the image, but not specific names. I can provide some context, but not specific names. I can give you an idea of what\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generated_code = generate_tikz_code(ds_test[0]['image'])\n",
    "print(generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 18:38:10,171 - INFO - Application started.\n",
      "2024-11-08 18:38:10,172 - INFO - Switched to Session 1\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/tmp/ipykernel_660248/3462888522.py\", line 146, in load_image\n",
      "    label = tk.Label(self.chat_sessions[self.active_session]['widgets']['chat_history_container'], image=photo)\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 3148, in __init__\n",
      "    Widget.__init__(self, master, 'label', cnf, kw)\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 2572, in __init__\n",
      "    self.tk.call(\n",
      "_tkinter.TclError: image \"pyimage10\" doesn't exist\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/tmp/ipykernel_660248/3462888522.py\", line 146, in load_image\n",
      "    label = tk.Label(self.chat_sessions[self.active_session]['widgets']['chat_history_container'], image=photo)\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 3148, in __init__\n",
      "    Widget.__init__(self, master, 'label', cnf, kw)\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 2572, in __init__\n",
      "    self.tk.call(\n",
      "_tkinter.TclError: image \"pyimage11\" doesn't exist\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/tmp/ipykernel_660248/3462888522.py\", line 146, in load_image\n",
      "    label = tk.Label(self.chat_sessions[self.active_session]['widgets']['chat_history_container'], image=photo)\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 3148, in __init__\n",
      "    Widget.__init__(self, master, 'label', cnf, kw)\n",
      "  File \"/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py\", line 2572, in __init__\n",
      "    self.tk.call(\n",
      "_tkinter.TclError: image \"pyimage12\" doesn't exist\n",
      "2024-11-08 18:38:53,299 - ERROR - Error during text generation: CUDA out of memory. Tried to allocate 1.23 GiB. GPU 0 has a total capacity of 23.62 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 21.76 GiB memory in use. Of the allocated memory 20.10 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2024-11-08 18:38:53,300 - INFO - An error occurred during text generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 337\u001b[0m\n\u001b[1;32m    335\u001b[0m app \u001b[39m=\u001b[39m Application(master\u001b[39m=\u001b[39mroot)\n\u001b[1;32m    336\u001b[0m root\u001b[39m.\u001b[39mprotocol(\u001b[39m\"\u001b[39m\u001b[39mWM_DELETE_WINDOW\u001b[39m\u001b[39m\"\u001b[39m, app\u001b[39m.\u001b[39mon_closing)\n\u001b[0;32m--> 337\u001b[0m app\u001b[39m.\u001b[39;49mmainloop()\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/tkinter/__init__.py:1429\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmainloop\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m   1428\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtk\u001b[39m.\u001b[39;49mmainloop(n)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import date\n",
    "from PIL import Image, ImageTk\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk, messagebox\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Get today's date\n",
    "date_string: str = date.today().strftime(\"%d %b %Y\")\n",
    "# model_id = \"mylesgoose/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# # Load the model and processor\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "class Application(tk.Frame):\n",
    "    def __init__(self, master=None):\n",
    "        super().__init__(master)\n",
    "        self.master = master\n",
    "        self.pack(fill=\"both\", expand=True)\n",
    "        self.current_images = []   # Images for the current message\n",
    "        self.chat_sessions = {}    # Dictionary to hold multiple chat sessions\n",
    "        self.active_session = \"Session 1\"\n",
    "        self.create_widgets()\n",
    "        self.update_status(\"Application started.\")\n",
    "\n",
    "    def create_widgets(self):\n",
    "        # Create a style for ttk widgets\n",
    "        style = ttk.Style()\n",
    "        style.configure('TButton', font=('Helvetica', 10))\n",
    "        style.configure('TLabel', font=('Helvetica', 10))\n",
    "        style.configure('TNotebook.Tab', font=('Helvetica', 10))\n",
    "\n",
    "        # Create a menu bar\n",
    "        menu_bar = tk.Menu(self.master)\n",
    "        self.master.config(menu=menu_bar)\n",
    "\n",
    "        # Create the File menu\n",
    "        file_menu = tk.Menu(menu_bar, tearoff=0)\n",
    "        menu_bar.add_cascade(label=\"File\", menu=file_menu)\n",
    "        file_menu.add_command(label=\"New Session\", command=self.create_new_session)\n",
    "        file_menu.add_command(label=\"Load Session\", command=self.load_chat_session)\n",
    "        file_menu.add_command(label=\"Save Session\", command=self.save_current_chat)\n",
    "        file_menu.add_separator()\n",
    "        file_menu.add_command(label=\"Exit\", command=self.on_closing)\n",
    "\n",
    "        # Create a Notebook for multiple sessions\n",
    "        self.notebook = ttk.Notebook(self)\n",
    "        self.notebook.pack(side=\"top\", fill=\"both\", expand=True)\n",
    "        self.notebook.bind(\"<<NotebookTabChanged>>\", self.change_session)\n",
    "\n",
    "        # Initialize the first session\n",
    "        self.create_new_session()\n",
    "\n",
    "        # Status bar\n",
    "        self.status_bar = ttk.Label(self, text=\"Status: Ready\", anchor=\"w\")\n",
    "        self.status_bar.pack(side=\"bottom\", fill=\"x\")\n",
    "\n",
    "    def create_new_session(self, session_name=None):\n",
    "        if not session_name:\n",
    "            session_name = f\"Session {len(self.chat_sessions) + 1}\"\n",
    "        frame = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(frame, text=session_name)\n",
    "        self.chat_sessions[session_name] = {\n",
    "            \"frame\": frame,\n",
    "            \"chat_history\": [],\n",
    "            \"widgets\": {}\n",
    "        }\n",
    "        self.active_session = session_name\n",
    "        self.build_session_widgets(frame, session_name)\n",
    "\n",
    "    def build_session_widgets(self, frame, session_name):\n",
    "        widgets = {}\n",
    "\n",
    "        # Text Entry\n",
    "        widgets['text_entry_label'] = ttk.Label(frame, text=\"Enter your message:\")\n",
    "        widgets['text_entry_label'].pack(side=\"top\", anchor=\"w\", padx=10, pady=(10, 0))\n",
    "\n",
    "        widgets['text_entry'] = tk.Text(frame, height=5, width=80)\n",
    "        widgets['text_entry'].pack(side=\"top\", fill=\"x\", padx=10, pady=5)\n",
    "        widgets['text_entry'].bind(\"<Return>\", self.generate_text_from_entry)\n",
    "\n",
    "        # Buttons Frame\n",
    "        widgets['buttons_frame'] = ttk.Frame(frame)\n",
    "        widgets['buttons_frame'].pack(side=\"top\", fill=\"x\", padx=10, pady=5)\n",
    "\n",
    "        widgets['load_image_button'] = ttk.Button(widgets['buttons_frame'], text=\"Load Image\", command=self.load_image)\n",
    "        widgets['load_image_button'].pack(side=\"left\", padx=5)\n",
    "\n",
    "        widgets['remove_image_button'] = ttk.Button(widgets['buttons_frame'], text=\"Remove Images\", command=self.remove_images)\n",
    "        widgets['remove_image_button'].pack(side=\"left\", padx=5)\n",
    "\n",
    "        widgets['generate_text_button'] = ttk.Button(widgets['buttons_frame'], text=\"Send\", command=self.generate_text)\n",
    "        widgets['generate_text_button'].pack(side=\"left\", padx=5)\n",
    "\n",
    "        widgets['reset_button'] = ttk.Button(widgets['buttons_frame'], text=\"Reset Chat\", command=self.reset_chat)\n",
    "        widgets['reset_button'].pack(side=\"left\", padx=5)\n",
    "\n",
    "        widgets['save_chat_button'] = ttk.Button(widgets['buttons_frame'], text=\"Save Chat\", command=self.save_current_chat)\n",
    "        widgets['save_chat_button'].pack(side=\"left\", padx=5)\n",
    "\n",
    "        # Chat History\n",
    "        widgets['chat_history_frame'] = ttk.Frame(frame)\n",
    "        widgets['chat_history_frame'].pack(side=\"top\", fill=\"both\", expand=True, padx=10, pady=5)\n",
    "\n",
    "        widgets['chat_history_canvas'] = tk.Canvas(widgets['chat_history_frame'])\n",
    "        widgets['chat_history_canvas'].pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "        widgets['chat_history_scrollbar'] = ttk.Scrollbar(widgets['chat_history_frame'], orient=\"vertical\", command=widgets['chat_history_canvas'].yview)\n",
    "        widgets['chat_history_scrollbar'].pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "        widgets['chat_history_canvas'].configure(yscrollcommand=widgets['chat_history_scrollbar'].set)\n",
    "        widgets['chat_history_container'] = ttk.Frame(widgets['chat_history_canvas'])\n",
    "        widgets['chat_history_canvas'].create_window((0, 0), window=widgets['chat_history_container'], anchor='nw')\n",
    "\n",
    "        widgets['chat_history_container'].bind(\"<Configure>\", lambda event: widgets['chat_history_canvas'].configure(scrollregion=widgets['chat_history_canvas'].bbox(\"all\")))\n",
    "\n",
    "        self.chat_sessions[session_name]['widgets'] = widgets\n",
    "\n",
    "    def change_session(self, event):\n",
    "        selected_tab = event.widget.select()\n",
    "        self.active_session = event.widget.tab(selected_tab, \"text\")\n",
    "        self.update_status(f\"Switched to {self.active_session}\")\n",
    "\n",
    "    def update_status(self, message):\n",
    "        self.status_bar.config(text=f\"Status: {message}\")\n",
    "        logging.info(message)\n",
    "\n",
    "    def load_image(self):\n",
    "        image_paths = filedialog.askopenfilenames()\n",
    "        for image_path in image_paths:\n",
    "            image = Image.open(image_path)\n",
    "            image.thumbnail((100, 100))\n",
    "            photo = ImageTk.PhotoImage(image)\n",
    "            label = tk.Label(self.chat_sessions[self.active_session]['widgets']['chat_history_container'], image=photo)\n",
    "            label.image = photo\n",
    "            label.pack(side=\"top\", anchor=\"w\", padx=5, pady=5)\n",
    "            self.current_images.append({'image': image, 'path': image_path})\n",
    "        self.update_status(f\"Loaded {len(image_paths)} image(s).\")\n",
    "\n",
    "    def remove_images(self):\n",
    "        self.current_images = []\n",
    "        self.update_status(\"All images removed from the current message.\")\n",
    "\n",
    "    def generate_text(self, event=None):\n",
    "        user_text = self.chat_sessions[self.active_session]['widgets']['text_entry'].get(\"1.0\", tk.END).strip()\n",
    "        if not user_text and not self.current_images:\n",
    "            self.update_status(\"Please enter a message or load images.\")\n",
    "            return\n",
    "\n",
    "        # Display user's message and images in chat history\n",
    "        self.display_message(\"User\", user_text, self.current_images)\n",
    "\n",
    "        session_data = self.chat_sessions[self.active_session]\n",
    "\n",
    "        # Prepare message content\n",
    "        message_content = []\n",
    "\n",
    "        if self.current_images:\n",
    "            message_content.append({\"type\": \"image\"})\n",
    "        # Add the text content\n",
    "        message_content.append({\"type\": \"text\", \"text\": user_text})\n",
    "\n",
    "        # Append the message to the chat history, including image paths\n",
    "        session_data['chat_history'].append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message_content,\n",
    "            \"images\": [img['path'] for img in self.current_images]  # Store image paths\n",
    "        })\n",
    "\n",
    "        # Build messages for the processor\n",
    "        messages = [{\"role\": message[\"role\"], \"content\": message[\"content\"]} for message in session_data['chat_history']] + \\\n",
    "            [{\"role\": \"system\", \"content\": [{\"You are a helpful and creative AI assistant.\"}]}]\n",
    "\n",
    "        try:\n",
    "            # Generate the input text for the processor\n",
    "            input_text = processor.apply_chat_template(messages, add_generation_prompt=True, date_string=date_string)\n",
    "\n",
    "            # Build all_images by collecting images from chat history\n",
    "            all_images = []\n",
    "            for message in session_data['chat_history']:\n",
    "                if 'images' in message and message['images']:\n",
    "                    for img_path in message['images']:\n",
    "                        try:\n",
    "                            img = Image.open(img_path)\n",
    "                            all_images.append(img)\n",
    "                            print(img)\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "                            self.update_status(f\"Error loading image {img_path}\")\n",
    "\n",
    "            # Ensure the number of images matches the number of image tokens\n",
    "            total_image_tokens = input_text.count(processor.image_token)\n",
    "            if total_image_tokens != len(all_images):\n",
    "                self.update_status(f\"Mismatch between image tokens ({total_image_tokens}) and images provided ({len(all_images)}).\")\n",
    "                return\n",
    "\n",
    "            # Prepare inputs for the model\n",
    "            inputs = processor(images=all_images, text=input_text, return_tensors=\"pt\").to(model.device)\n",
    "            # Generate the assistant's response\n",
    "            output = model.generate(**inputs, max_new_tokens=1000)\n",
    "            generated_text = processor.decode(output[0][inputs['input_ids'].shape[-1]:])\n",
    "\n",
    "            # Update chat history and UI with the assistant's response\n",
    "            session_data['chat_history'].append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": generated_text}],\n",
    "                \"images\": []\n",
    "            })\n",
    "            self.display_message(\"Assistant\", generated_text)\n",
    "\n",
    "            # Clear the text entry and current images\n",
    "            self.chat_sessions[self.active_session]['widgets']['text_entry'].delete(\"1.0\", tk.END)\n",
    "            self.current_images = []\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during text generation: {e}\")\n",
    "            self.update_status(\"An error occurred during text generation.\")\n",
    "\n",
    "    def display_message(self, sender, text, images=[]):\n",
    "        container = self.chat_sessions[self.active_session]['widgets']['chat_history_container']\n",
    "        frame = ttk.Frame(container)\n",
    "        frame.pack(fill=\"x\", pady=5)\n",
    "\n",
    "        label = ttk.Label(frame, text=f\"{sender}:\", font=('Helvetica', 10, 'bold'))\n",
    "        label.pack(side=\"top\", anchor=\"w\")\n",
    "\n",
    "        if images:\n",
    "            images_frame = ttk.Frame(frame)\n",
    "            images_frame.pack(side=\"top\", fill=\"x\")\n",
    "            for img_item in images:\n",
    "                if isinstance(img_item, dict):\n",
    "                    img = img_item['image']\n",
    "                elif isinstance(img_item, str):\n",
    "                    try:\n",
    "                        img = Image.open(img_item)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error loading image {img_item}: {e}\")\n",
    "                        self.update_status(f\"Error loading image {img_item}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    img = img_item\n",
    "                image = img.copy()\n",
    "                image.thumbnail((100, 100))\n",
    "                photo = ImageTk.PhotoImage(image)\n",
    "                img_label = ttk.Label(images_frame, image=photo)\n",
    "                img_label.image = photo\n",
    "                img_label.pack(side=\"left\", padx=5)\n",
    "\n",
    "        message_label = ttk.Label(frame, text=text, wraplength=500, justify=\"left\")\n",
    "        message_label.pack(side=\"top\", anchor=\"w\")\n",
    "\n",
    "        # Scroll to the bottom\n",
    "        canvas = self.chat_sessions[self.active_session]['widgets']['chat_history_canvas']\n",
    "        canvas.update_idletasks()\n",
    "        canvas.yview_moveto(1.0)\n",
    "\n",
    "    def generate_text_from_entry(self, event=None):\n",
    "        self.generate_text()\n",
    "        return \"break\"  # Prevents the Text widget from inserting a newline\n",
    "\n",
    "    def reset_chat(self):\n",
    "        confirm = messagebox.askyesno(\"Reset Chat\", \"Are you sure you want to reset the chat?\")\n",
    "        if confirm:\n",
    "            session_data = self.chat_sessions[self.active_session]\n",
    "            session_data['chat_history'] = []\n",
    "            self.current_images = []\n",
    "\n",
    "            # Clear chat history UI\n",
    "            container = session_data['widgets']['chat_history_container']\n",
    "            for widget in container.winfo_children():\n",
    "                widget.destroy()\n",
    "\n",
    "            self.update_status(\"Chat reset.\")\n",
    "\n",
    "    def save_current_chat(self):\n",
    "        session_data = self.chat_sessions[self.active_session]\n",
    "        if not session_data['chat_history']:\n",
    "            messagebox.showinfo(\"Save Chat\", \"No chat history to save.\")\n",
    "            return\n",
    "        filename = filedialog.asksaveasfilename(defaultextension=\".json\", initialfile=f\"{self.active_session}.json\", filetypes=[(\"JSON files\", \"*.json\")])\n",
    "        if filename:\n",
    "            self.save_chat_history(filename)\n",
    "            self.update_status(f\"Chat history saved to {filename}\")\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        session_data = self.chat_sessions[self.active_session]\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(session_data['chat_history'], f)\n",
    "\n",
    "    def load_chat_session(self):\n",
    "        filename = filedialog.askopenfilename(defaultextension=\".json\", filetypes=[(\"JSON files\", \"*.json\")])\n",
    "        if filename:\n",
    "            session_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.create_new_session(session_name)\n",
    "            self.load_chat_history(filename, session_name)\n",
    "            self.update_status(f\"Chat session {session_name} loaded.\")\n",
    "\n",
    "    def load_chat_history(self, filename, session_name):\n",
    "        with open(filename, \"r\") as f:\n",
    "            chat_history = json.load(f)\n",
    "        session_data = self.chat_sessions[session_name]\n",
    "        session_data['chat_history'] = chat_history\n",
    "        # Update UI with loaded chat history\n",
    "        for message in chat_history:\n",
    "            sender = message['role'].capitalize()\n",
    "            content = message['content']\n",
    "            images = []\n",
    "            if 'images' in message and message['images']:\n",
    "                images = message['images']\n",
    "            text = \"\"\n",
    "            for item in content:\n",
    "                if item.get('type') == 'text':\n",
    "                    text = item.get('text', '')\n",
    "                    break\n",
    "            self.display_message(sender, text, images)\n",
    "\n",
    "    def on_closing(self):\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to quit?\"):\n",
    "            self.master.destroy()\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"LLM Chat Application\")\n",
    "app = Application(master=root)\n",
    "root.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 18:47:05,282 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2024-11-08 18:47:05,352 - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2136fabc3d6b40208ee43b1c370f3291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import date\n",
    "from PIL import Image, ImageTk\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk, messagebox\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Get today's date\n",
    "date_string: str = date.today().strftime(\"%d %b %Y\")\n",
    "model_id = \"mylesgoose/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# Load the model and processor\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.23 GiB. GPU 0 has a total capacity of 23.62 GiB of which 1.00 GiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 368.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m inputs \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39mimages, text\u001b[39m=\u001b[39minput_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Generate the assistant's response\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m     31\u001b[0m generated_text \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(generated_text)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[39m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   2216\u001b[0m         input_ids,\n\u001b[1;32m   2217\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   2218\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   2219\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   2220\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   2221\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   2222\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   2223\u001b[0m     )\n\u001b[1;32m   2225\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39moutput_hidden_states\u001b[39m\u001b[39m\"\u001b[39m: output_hidden_states} \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3208\u001b[0m \u001b[39m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:2098\u001b[0m, in \u001b[0;36mMllamaForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, aspect_ratio_mask, aspect_ratio_ids, attention_mask, cross_attention_mask, cross_attention_states, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2096\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`aspect_ratio_ids` must be provided if `pixel_values` is provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2097\u001b[0m \u001b[39m# get vision tokens from vision model\u001b[39;00m\n\u001b[0;32m-> 2098\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m   2099\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   2100\u001b[0m     aspect_ratio_ids\u001b[39m=\u001b[39;49maspect_ratio_ids,\n\u001b[1;32m   2101\u001b[0m     aspect_ratio_mask\u001b[39m=\u001b[39;49maspect_ratio_mask,\n\u001b[1;32m   2102\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2103\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2104\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   2105\u001b[0m )\n\u001b[1;32m   2106\u001b[0m cross_attention_states \u001b[39m=\u001b[39m vision_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2107\u001b[0m cross_attention_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_modal_projector(cross_attention_states)\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m   2108\u001b[0m     \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cross_attention_states\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size\n\u001b[1;32m   2109\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:1454\u001b[0m, in \u001b[0;36mMllamaVisionModel.forward\u001b[0;34m(self, pixel_values, aspect_ratio_ids, aspect_ratio_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39m# Apply encoder\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m hidden_state \u001b[39m=\u001b[39m hidden_state\u001b[39m.\u001b[39mview(batch_size \u001b[39m*\u001b[39m num_concurrent_media, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dim)\n\u001b[0;32m-> 1454\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1455\u001b[0m     hidden_state,\n\u001b[1;32m   1456\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1457\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1458\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1459\u001b[0m )\n\u001b[1;32m   1460\u001b[0m hidden_state \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1462\u001b[0m hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm_post(hidden_state)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:389\u001b[0m, in \u001b[0;36mMllamaVisionEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    382\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    383\u001b[0m         encoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    384\u001b[0m         hidden_states,\n\u001b[1;32m    385\u001b[0m         attention_mask,\n\u001b[1;32m    386\u001b[0m         output_attentions,\n\u001b[1;32m    387\u001b[0m     )\n\u001b[1;32m    388\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    390\u001b[0m         hidden_state\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    391\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    392\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    396\u001b[0m     all_attentions \u001b[39m=\u001b[39m all_attentions \u001b[39m+\u001b[39m (layer_outputs[\u001b[39m1\u001b[39m],)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:302\u001b[0m, in \u001b[0;36mMllamaVisionEncoderLayer.forward\u001b[0;34m(self, hidden_state, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    300\u001b[0m residual \u001b[39m=\u001b[39m hidden_state\n\u001b[1;32m    301\u001b[0m hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_state)\n\u001b[0;32m--> 302\u001b[0m hidden_state, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(hidden_state, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_gated:\n\u001b[1;32m    304\u001b[0m     hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_attn\u001b[39m.\u001b[39mtanh() \u001b[39m*\u001b[39m hidden_state\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:261\u001b[0m, in \u001b[0;36mMllamaVisionSdpaAttention.forward\u001b[0;34m(self, hidden_state, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    258\u001b[0m key \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    259\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m attn_output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mscaled_dot_product_attention(query, key, value, attn_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m    263\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    264\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mreshape(batch_size, q_seq_len, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.23 GiB. GPU 0 has a total capacity of 23.62 GiB of which 1.00 GiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 368.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Assuming 'processor' and 'model' are already initialized and configured\n",
    "text_query = \"describe this image <|image|>\"\n",
    "chat_template = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI.\"},\n",
    "    {\"role\": \"user\", \"content\": text_query}\n",
    "]\n",
    "\n",
    "# Load the image using PIL\n",
    "image_path = \"output_meme.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Ensure it is in RGB format\n",
    "\n",
    "# Apply the chat template\n",
    "input_text = processor.apply_chat_template(chat_template, add_generation_prompt=True)\n",
    "\n",
    "# Since the processor expects a list of images or a single image, wrap it in a list if necessary\n",
    "images = [image]  # Or just use 'image' if the processor can handle single image objects directly\n",
    "\n",
    "# Check if the number of image tokens in the text matches the number of provided images\n",
    "# This is important for models expecting specific tokens to align with image inputs\n",
    "total_image_tokens = input_text.count(processor.image_token)\n",
    "if total_image_tokens != len(images):\n",
    "    print(f\"Mismatch between image tokens ({total_image_tokens}) and images provided ({len(images)}).\")\n",
    "\n",
    "# Prepare inputs for the model\n",
    "inputs = processor(images=images, text=input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate the assistant's response\n",
    "output = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(requests\u001b[39m.\u001b[39mget(url, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mraw)\n\u001b[1;32m      7\u001b[0m messages \u001b[39m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m      9\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     10\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDescribe the tutorial feature image.\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     11\u001b[0m     ]}\n\u001b[1;32m     12\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m input_text \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39;49mapply_chat_template(messages, add_generation_prompt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     14\u001b[0m inputs \u001b[39m=\u001b[39m processor(image, input_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     16\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m120\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/processing_utils.py:1101\u001b[0m, in \u001b[0;36mProcessorMixin.apply_chat_template\u001b[0;34m(self, conversation, chat_template, tokenize, **kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1097\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNo chat template is set for this processor. Please either set the `chat_template` attribute, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor provide a chat template as an argument. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1099\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/main/en/chat_templating for more information.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1100\u001b[0m         )\n\u001b[0;32m-> 1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mapply_chat_template(\n\u001b[1;32m   1102\u001b[0m     conversation, chat_template\u001b[39m=\u001b[39;49mchat_template, tokenize\u001b[39m=\u001b[39;49mtokenize, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1103\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1811\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     logger\u001b[39m.\u001b[39mwarning_once(\n\u001b[1;32m   1807\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[39m{\u001b[39m\u001b[39m% g\u001b[39;00m\u001b[39meneration \u001b[39m\u001b[39m%\u001b[39m\u001b[39m}` keyword.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1808\u001b[0m     )\n\u001b[1;32m   1810\u001b[0m \u001b[39m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[0;32m-> 1811\u001b[0m compiled_template \u001b[39m=\u001b[39m _compile_jinja_template(chat_template)\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(conversation, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m   1814\u001b[0m     \u001b[39misinstance\u001b[39m(conversation[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mor\u001b[39;00m \u001b[39mhasattr\u001b[39m(conversation[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     conversations \u001b[39m=\u001b[39m conversation\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcz-J3iR2bEGcCSLzay07Rqfj5tTakp2EMTTN0x6nKYGLS5yWl0unoSpj2S0-mrWpDtMqjl1fAgH6pVkKJekQEY_kwzL6QNOdf143Yt66znQ0EpfLvx6CLFOqw41oeOYmhPZ6Qrlb5AjEr4AenIOgBMTWTD?key=vhLUYntaS9QOx531XpJH3g\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe the tutorial feature image.\"}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=120)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.7098), tensor(0.8177), tensor(0.7890), tensor(0.7330), tensor(0.6542), tensor(0.7098)]\n",
      "Current list_of_references: [[['\\\\documentclass', '&#91;', '10pt', ',', 'oneside', '&#93;', '{', 'amsart', '}', '\\\\usepackage', '{', 'amssymb', '}', '\\\\usepackage', '{', 'amsmath', '}', '\\\\usepackage', '{', 'color', '}', '\\\\usepackage', '&#91;', 'usenames', ',', 'dvipsnames', ',', 'svgnames', ',', 'table', '&#93;', '{', 'xcolor', '}', '\\\\usepackage', '&#91;', 'utf8', '&#93;', '{', 'inputenc', '}', '\\\\usepackage', '&#91;', 'colorlinks', '=', 'true', ',', 'pdfstartview', '=', 'FitV', ',', 'linkcolor', '=', 'ForestGreen', ',', 'citecolor', '=', 'ForestGreen', ',', 'urlcolor', '=', 'black', '&#93;', '{', 'hyperref', '}', '\\\\usepackage', '{', 'tikz', '}', '\\\\begin', '{', 'document', '}', '\\\\begin', '{', 'tikzpicture', '}', '&#91;', 'scale', '=', '1', '&#93;', '\\\\draw', '&#91;', 'dashed', '&#93;', '(', '-6', ',', '0', ')', '--', '(', '6', ',', '0', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '_', '1', '$', '}', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '-3.5,0', ')', '--', '(', '3.5,0', ')', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '-3.5', ',', '-7', ')', '--', '(', '3.5', ',', '-7', ')', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '-3.5,0', ')', '--', '(', '-3.5', ',', '-7', ')', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '3.5,0', ')', '--', '(', '3.5', ',', '-7', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '_', '{', '\\\\frac', '{', '1', '}', '{', '2', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1,0', ')', '--', '(', '1', ',', '0', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-2', ')', '--', '(', '1', ',', '-2', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1,0', ')', '--', '(', '-1', ',', '-2', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '1,0', ')', '--', '(', '1', ',', '-2', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '_', '{', 'r', '_', '0', '}', '$', '}', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-4', ')', '--', '(', '1', ',', '-4', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-6', ')', '--', '(', '1', ',', '-6', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-4', ')', '--', '(', '-1', ',', '-6', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '1', ',', '-4', ')', '--', '(', '1', ',', '-6', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '^', '-', '_', '{', 'r', '_', '0', '}', '$', '}', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5,0', ')', '--', '(', '0.5', ',', '0', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5', ',', '-1', ')', '--', '(', '0.5', ',', '-1', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5,0', ')', '--', '(', '-0.5', ',', '-1', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '0.5,0', ')', '--', '(', '0.5', ',', '-1', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.71', '&#93;', '{', '$', 'Q', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '2', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '{', '(', '-0.5', ',', '-4.5', ')', '--', '(', '0.5', ',', '-4.5', ')', '}', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5', ',', '-5.5', ')', '--', '(', '0.5', ',', '-5.5', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5', ',', '-4.5', ')', '--', '(', '-0.5', ',', '-5.5', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '0.5', ',', '-4.5', ')', '--', '(', '0.5', ',', '-5.5', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.71', '&#93;', '{', '$', '\\\\tilde', 'Q', '^', '-', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '2', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25,0', ')', '--', '(', '0.25', ',', '0', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-0.5', ')', '--', '(', '0.25', ',', '-0.5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25,0', ')', '--', '(', '-0.25', ',', '-0.5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '0.25,0', ')', '--', '(', '0.25', ',', '-0.5', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.7', '&#93;', '{', '$', 'Q', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '4', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-4.5', ')', '--', '(', '0.25', ',', '-4.5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-5', ')', '--', '(', '0.25', ',', '-5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-4.5', ')', '--', '(', '-0.25', ',', '-5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '0.25', ',', '-4.5', ')', '--', '(', '0.25', ',', '-5', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.7', '&#93;', '{', '$', '\\\\tilde', 'Q', '^', '-', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '4', '}', '}', '$', '}', ';', '\\\\end', '{', 'tikzpicture', '}', '\\\\end', '{', 'document', '}']], [['\\\\documentclass', '&#91;', 'review', '&#93;', '{', 'elsarticle', '}', '\\\\usepackage', '{', 'tikz', '}', '\\\\usepackage', '{', 'amsmath', '}', '\\\\usetikzlibrary', '{', 'arrows', '}', '\\\\begin', '{', 'document', '}', '\\\\begin', '{', 'tikzpicture', '}', '\\\\draw', '(', '0,0', ')', 'circle', '(', '1.5cm', ')', ';', '\\\\fill', '&#91;', 'black', '!', '70', '&#93;', '(', '0,0', ')', 'circle', '(', '1.5cm', ')', ';', '\\\\fill', '&#91;', 'white', '&#93;', '(', '0,0', ')', '--', '(', '-90', ':', '1.5cm', ')', 'arc', '(', '-90', ':', '90', ':', '1.5cm', ')', '--', 'cycle', ';', '\\\\draw', '(', '0,0', ')', '--', '(', '1.06,1.06', ')', ';', '\\\\draw', 'node', 'at', '(', '0.5,0.1', ')', '{', '$', 'R', '_', 'p', '$', '}', ';', '\\\\draw', '&#91;', 'dashed', '&#93;', '(', '4,0', ')', 'ellipse', '(', '0.5cm', 'and', '1.5cm', ')', ';', '\\\\draw', '&#91;', 'dashed', '&#93;', '(', '4,0', ')', '--', '(', '4,1.5', ')', ';', '\\\\draw', 'node', 'at', '(', '4.3,0.5', ')', '{', '$', 'R', '_', 'p', '$', '}', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '9,3', ')', 'node', '&#91;', 'right', '&#93;', '{', 'star', '}', 'arc', '(', '150', ':', '200', ':', '7cm', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '&lt;', '-', '&#93;', '(', '1.6,0', ')', '--', '(', '8,0', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '&lt;', '-', '&#93;', '(', '0.5,1.5', ')', '--', '(', '8.4,1.5', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '&lt;', '-', '&#93;', '(', '0.5', ',', '-1.5', ')', '--', '(', '8', ',', '-1.5', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '&lt;', '-', '&gt;', '&#93;', '(', '0', ',', '-2.1', ')', '--', '(', '8', ',', '-2.1', ')', ';', '\\\\draw', 'node', 'at', '(', '4', ',', '-2.3', ')', '{', 'a', '}', ';', '\\\\draw', '&#91;', 'thick', ',', '-', '&gt;', '&#93;', '(', '0.1,1.5', ')', '--', '(', '0.4,2', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '-', '&gt;', '&#93;', '(', '1.25,1.05', ')', '--', '(', '1.7,1.4', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '-', '&gt;', '&#93;', '(', '1.4,0.5', ')', '--', '(', '2.0,0.8', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '-', '&gt;', '&#93;', '(', '1.4', ',', '-0.5', ')', '--', '(', '2.0', ',', '-0.8', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '-', '&gt;', '&#93;', '(', '1.25', ',', '-1.05', ')', '--', '(', '1.7', ',', '-1.4', ')', ';', '\\\\draw', '&#91;', 'thick', ',', '-', '&gt;', '&#93;', '(', '0.1', ',', '-1.5', ')', '--', '(', '0.4', ',', '-2', ')', ';', '\\\\draw', '&#91;', 'thick', ',', 'green', ',', '-', '&gt;', '&#93;', '(', '0.7', ',', '-0.3', ')', 'arc', '(', '0', ':', '-190', ':', '0.7', ')', ';', '\\\\draw', '&#91;', 'thick', ',', 'green', ',', '&lt;', '-', '&#93;', '(', '0.7,0.3', ')', 'arc', '(', '0', ':', '190', ':', '0.7', ')', ';', '\\\\end', '{', 'tikzpicture', '}', '\\\\end', '{', 'document', '}']], [['\\\\documentclass', '&#91;', 'reqno', ',', '12pt', ',', 'letterpaper', '&#93;', '{', 'amsart', '}', '\\\\usepackage', '{', 'amsmath', ',', 'amssymb', ',', 'amsthm', ',', 'graphicx', ',', 'mathrsfs', ',', 'url', '}', '\\\\usepackage', '&#91;', 'usenames', ',', 'dvipsnames', '&#93;', '{', 'color', '}', '\\\\usepackage', '&#91;', 'colorlinks', '=', 'true', ',', 'linkcolor', '=', 'Red', ',', 'citecolor', '=', 'Green', '&#93;', '{', 'hyperref', '}', '\\\\usepackage', '{', 'tikz', '}', '\\\\usetikzlibrary', '{', 'arrows.meta', '}', '\\\\begin', '{', 'document', '}', '\\\\begin', '{', 'tikzpicture', '}', '\\\\node', 'at', '(', '0,0', ')', '{', '\\\\includegraphics', '&#91;', 'trim', '=', '{', '2.1cm', '0', '0', '0', '}', ',', 'width', '=', '7.5cm', '&#93;', '{', 'example-image', '}', '}', ';', '\\\\node', 'at', '(', '7.6,0', ')', '{', '\\\\includegraphics', '&#91;', 'trim', '=', '{', '1.7cm', '0', '0', '0', '}', ',', 'width', '=', '7.5cm', '&#93;', '{', 'example-image', '}', '}', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '-0.7,0', ')', '--', '(', '-0.7,2.5', ')', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '-0.7,0', ')', '--', '(', '-0.7', ',', '-2.5', ')', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '-2.9-0.6', ',', '-0.1', ')', '--', '(', '-1', ',', '-0.1', ')', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '2.7-0.6', ',', '-0.1', ')', '--', '(', '-0.7', ',', '-0.1', ')', ';', '\\\\node', 'at', '(', '-0.7', '+', '1.3,0.3', '+', '0.2', ')', '{', '\\\\small', 'Outgoing', 'DPs', '}', ';', '\\\\node', 'at', '(', '-2-0.6', ',', '-0.4', ')', '{', '\\\\small', 'Incoming', 'DP', '}', ';', '\\\\node', 'at', '(', '2-0.6', ',', '-0.4', ')', '{', '\\\\small', 'Incoming', 'DP', '}', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '-0.8', '+', '8,2.5', ')', '--', '(', '-0.8', '+', '8,0.1', ')', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '-0.8', '+', '8', ',', '-2.5', ')', '--', '(', '-0.8', '+', '8,0.1', ')', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '-1', '+', '8', ',', '-0.1', ')', '--', '(', '-2.9-0.6', '+', '8', ',', '-0.1', ')', ';', '\\\\draw', '&#91;', '-', '{', 'Stealth', '&#91;', 'length', '=', '3mm', ',', 'width', '=', '2mm', '&#93;', '}', '&#93;', '(', '-1', '+', '8', ',', '-0.1', ')', '--', '(', '2.7-0.6', '+', '8', ',', '-0.1', ')', ';', '\\\\node', 'at', '(', '-0.7', '+', '1.3', '+', '8,0.45', ')', '{', '\\\\small', 'Incoming', 'DPs', '}', ';', '\\\\node', 'at', '(', '-2-0.6', '+', '8', ',', '-0.4', ')', '{', '\\\\small', 'Outgoing', 'DP', '}', ';', '\\\\node', 'at', '(', '2-0.6', '+', '7.4', ',', '-0.4', ')', '{', '\\\\small', 'Outgoing', 'DP', '}', ';', '\\\\node', 'at', '(', '-1', ',', '-3', ')', '{', '$', '\\\\Re', 'k', '$', '}', ';', '\\\\node', 'at', '(', '-4.5,0.5', ')', '{', '$', '\\\\Im', 'k', '$', '}', ';', '\\\\node', 'at', '(', '7', ',', '-3', ')', '{', '$', '\\\\Re', 'k', '$', '}', ';', '\\\\node', 'at', '(', '2.5', ',', '-3', ')', '{', '$', '\\\\alpha', '$', '}', ';', '\\\\node', 'at', '(', '7.6', '+', '2.5', ',', '-3', ')', '{', '$', '\\\\alpha', '$', '}', ';', '\\\\end', '{', 'tikzpicture', '}', '\\\\end', '{', 'document', '}']], [['\\\\documentclass', '&#91;', 'reqno', ',', '12pt', '&#93;', '{', 'amsart', '}', '\\\\usepackage', '{', 'amsmath', ',', 'graphicx', ',', 'amssymb', ',', 'amsthm', '}', '\\\\usepackage', '{', 'amsmath', '}', '\\\\usepackage', '{', 'amssymb', '}', '\\\\usepackage', '{', 'color', '}', '\\\\usepackage', '{', 'tikz-cd', '}', '\\\\begin', '{', 'document', '}', '\\\\begin', '{', 'tikzpicture', '}', '&#91;', 'line', 'width', '=', '0.8pt', '&#93;', '\\\\draw', '&#91;', '-', '&gt;', '&#93;', '(', '0', ',', '-2', ')', '--', '(', '1,0', ')', ';', '\\\\draw', '&#91;', '-', '&gt;', '&#93;', '(', '1', ',', '-2', ')', '--', '(', '2,0', ')', ';', '\\\\draw', '&#91;', '-', '&gt;', ',', 'dashed', '&#93;', '(', '2', ',', '-2', ')', '--', '(', '0,0', ')', ';', '\\\\node', 'at', '(', '3', ',', '-1', ')', '{', '$', '\\\\stackrel', '{', '\\\\varsigma', '_', '3', '^', '3', '}', '{', '\\\\longrightarrow', '}', '$', '}', ';', '\\\\node', 'at', '(', '0', ',', '-2.25', ')', '{', '$', '1', '$', '}', ';', '\\\\node', 'at', '(', '0,0.25', ')', '{', '$', '1', '$', '}', ';', '\\\\node', 'at', '(', '1', ',', '-2.25', ')', '{', '$', '2', '$', '}', ';', '\\\\node', 'at', '(', '1,0.25', ')', '{', '$', '2', '$', '}', ';', '\\\\node', 'at', '(', '2', ',', '-2.25', ')', '{', '$', '3', '$', '}', ';', '\\\\node', 'at', '(', '2,0.25', ')', '{', '$', '3', '$', '}', ';', '\\\\begin', '{', 'scope', '}', '&#91;', 'xshift', '=', '4cm', '&#93;', '\\\\draw', '&#91;', '-', '&gt;', '&#93;', '(', '0', ',', '-2', ')', '--', '(', '3,0', ')', ';', '\\\\draw', '(', '1', ',', '-2', ')', '&#91;', '-', '&gt;', '&#93;', '--', '(', '4,0', ')', ';', '\\\\draw', '(', '2', ',', '-2', ')', '&#91;', '-', '&gt;', ',', 'dashed', '&#93;', '--', '(', '0,0', ')', ';', '\\\\draw', '(', '3', ',', '-2', ')', '&#91;', '-', '&gt;', ',', 'dashed', '&#93;', '--', '(', '1,0', ')', ';', '\\\\draw', '(', '4', ',', '-2', ')', '&#91;', '-', '&gt;', ',', 'dashed', '&#93;', '--', '(', '2,0', ')', ';', '\\\\node', 'at', '(', '0', ',', '-2.25', ')', '{', '$', '1', '$', '}', ';', '\\\\node', 'at', '(', '0,0.25', ')', '{', '$', '1', '$', '}', ';', '\\\\node', 'at', '(', '1', ',', '-2.25', ')', '{', '$', '2', '$', '}', ';', '\\\\node', 'at', '(', '1,0.25', ')', '{', '$', '2', '$', '}', ';', '\\\\node', 'at', '(', '2', ',', '-2.25', ')', '{', '$', '3', '$', '}', ';', '\\\\node', 'at', '(', '2,0.25', ')', '{', '$', '3', '$', '}', ';', '\\\\node', 'at', '(', '3', ',', '-2.25', ')', '{', '$', '4', '$', '}', ';', '\\\\node', 'at', '(', '3,0.25', ')', '{', '$', '4', '$', '}', ';', '\\\\node', 'at', '(', '4', ',', '-2.25', ')', '{', '$', '5', '$', '}', ';', '\\\\node', 'at', '(', '4,0.25', ')', '{', '$', '5', '$', '}', ';', '\\\\end', '{', 'scope', '}', '\\\\end', '{', 'tikzpicture', '}', '\\\\end', '{', 'document', '}']], [['\\\\documentclass', '&#91;', '10pt', ',', 'journal', ',', 'compsoc', '&#93;', '{', 'IEEEtran', '}', '\\\\usepackage', '{', 'amsmath', ',', 'amssymb', ',', 'amsfonts', ',', 'chemarrow', ',', 'balance', '}', '\\\\usepackage', '{', 'xcolor', '}', '\\\\usepackage', '{', 'pgfplots', '}', '\\\\usepackage', '{', 'pgfplotstable', '}', '\\\\usepackage', '{', 'pgfplots', '}', '\\\\usetikzlibrary', '{', 'patterns', '}', '\\\\begin', '{', 'document', '}', '\\\\begin', '{', 'tikzpicture', '}', '\\\\begin', '{', 'axis', '}', '&#91;', 'ybar', ',', 'enlargelimits', '=', '0.07', ',', 'ylabel', '=', '{', 'Percentage', '}', ',', 'symbolic', 'x', 'coords', '=', '{', 'Business', ',', 'Education', ',', 'Adult', ',', 'Entertainment', ',', 'Illegal', 'Content', ',', 'Media', ',', 'Tech', 'Info', ',', 'Shopping', ',', 'Sport', ',', 'Uncategorized', '}', ',', 'xtick', '=', 'data', ',', 'x', '=', '0.7cm', ',', 'bar', 'width', '=', '2.8mm', ',', 'legend', 'style', '=', '{', 'at', '=', '{', '(', '0.3,1.05', ')', '}', ',', 'anchor', '=', 'west', ',', 'legend', 'columns', '=', '2', '}', ',', 'xticklabel', 'style', '=', '{', 'rotate', '=', '30', ',', 'anchor', '=', 'east', ',', 'align', '=', 'right', '}', ',', 'grid', '=', 'both', ',', 'grid', 'style', '=', '{', 'line', 'width', '=', '.1pt', ',', 'draw', '=', 'gray', '!', '10', '}', ',', 'major', 'grid', 'style', '=', '{', 'line', 'width', '=', '.1pt', ',', 'draw', '=', 'black', '!', '10', '}', '&#93;', '\\\\addplot', '&#91;', 'color', '=', 'black', ',', 'pattern', '=', 'crosshatch', '&#93;', 'coordinates', '{', '(', 'Business', ',', '21', ')', '(', 'Education', ',', '19', ')', '(', 'Adult', ',', '12', ')', '(', 'Entertainment', ',', '9', ')', '(', 'Illegal', 'Content', ',', '6', ')', '(', 'Media', ',', '2', ')', '(', 'Tech', 'Info', ',', '5', ')', '(', 'Shopping', ',', '2', ')', '(', 'Sport', ',', '1', ')', '(', 'Uncategorized', ',', '23', ')', '}', ';', '\\\\addlegendentry', '{', 'Complete', '}', ';', '\\\\addplot', '&#91;', 'color', '=', 'black', ',', 'pattern', '=', 'north', 'west', 'lines', '&#93;', 'coordinates', '{', '(', 'Business', ',', '24', ')', '(', 'Education', ',', '24', ')', '(', 'Adult', ',', '3', ')', '(', 'Entertainment', ',', '12', ')', '(', 'Illegal', 'Content', ',', '6', ')', '(', 'Media', ',', '4', ')', '(', 'Tech', 'Info', ',', '3', ')', '(', 'Shopping', ',', '5', ')', '(', 'Sport', ',', '2', ')', '(', 'Uncategorized', ',', '17', ')', '}', ';', '\\\\addlegendentry', '{', 'New', '}', ';', '\\\\end', '{', 'axis', '}', '\\\\end', '{', 'tikzpicture', '}', '\\\\end', '{', 'document', '}']], [['\\\\documentclass', '&#91;', '10pt', ',', 'oneside', '&#93;', '{', 'amsart', '}', '\\\\usepackage', '{', 'amssymb', '}', '\\\\usepackage', '{', 'amsmath', '}', '\\\\usepackage', '{', 'color', '}', '\\\\usepackage', '&#91;', 'usenames', ',', 'dvipsnames', ',', 'svgnames', ',', 'table', '&#93;', '{', 'xcolor', '}', '\\\\usepackage', '&#91;', 'utf8', '&#93;', '{', 'inputenc', '}', '\\\\usepackage', '&#91;', 'colorlinks', '=', 'true', ',', 'pdfstartview', '=', 'FitV', ',', 'linkcolor', '=', 'ForestGreen', ',', 'citecolor', '=', 'ForestGreen', ',', 'urlcolor', '=', 'black', '&#93;', '{', 'hyperref', '}', '\\\\usepackage', '{', 'tikz', '}', '\\\\begin', '{', 'document', '}', '\\\\begin', '{', 'tikzpicture', '}', '&#91;', 'scale', '=', '1', '&#93;', '\\\\draw', '&#91;', 'dashed', '&#93;', '(', '-6', ',', '0', ')', '--', '(', '6', ',', '0', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '_', '1', '$', '}', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '-3.5,0', ')', '--', '(', '3.5,0', ')', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '-3.5', ',', '-7', ')', '--', '(', '3.5', ',', '-7', ')', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '-3.5,0', ')', '--', '(', '-3.5', ',', '-7', ')', ';', '\\\\draw', '&#91;', 'blue', '&#93;', '(', '3.5,0', ')', '--', '(', '3.5', ',', '-7', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '_', '{', '\\\\frac', '{', '1', '}', '{', '2', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1,0', ')', '--', '(', '1', ',', '0', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-2', ')', '--', '(', '1', ',', '-2', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1,0', ')', '--', '(', '-1', ',', '-2', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '1,0', ')', '--', '(', '1', ',', '-2', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '_', '{', 'r', '_', '0', '}', '$', '}', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-4', ')', '--', '(', '1', ',', '-4', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-6', ')', '--', '(', '1', ',', '-6', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '-1', ',', '-4', ')', '--', '(', '-1', ',', '-6', ')', ';', '\\\\draw', '&#91;', 'violet', '&#93;', '(', '1', ',', '-4', ')', '--', '(', '1', ',', '-6', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '1', '&#93;', '{', '$', 'Q', '^', '-', '_', '{', 'r', '_', '0', '}', '$', '}', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5,0', ')', '--', '(', '0.5', ',', '0', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5', ',', '-1', ')', '--', '(', '0.5', ',', '-1', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5,0', ')', '--', '(', '-0.5', ',', '-1', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '0.5,0', ')', '--', '(', '0.5', ',', '-1', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.71', '&#93;', '{', '$', 'Q', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '2', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '{', '(', '-0.5', ',', '-4.5', ')', '--', '(', '0.5', ',', '-4.5', ')', '}', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5', ',', '-5.5', ')', '--', '(', '0.5', ',', '-5.5', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '-0.5', ',', '-4.5', ')', '--', '(', '-0.5', ',', '-5.5', ')', ';', '\\\\draw', '&#91;', 'orange', '&#93;', '(', '0.5', ',', '-4.5', ')', '--', '(', '0.5', ',', '-5.5', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.71', '&#93;', '{', '$', '\\\\tilde', 'Q', '^', '-', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '2', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25,0', ')', '--', '(', '0.25', ',', '0', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-0.5', ')', '--', '(', '0.25', ',', '-0.5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25,0', ')', '--', '(', '-0.25', ',', '-0.5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '0.25,0', ')', '--', '(', '0.25', ',', '-0.5', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.7', '&#93;', '{', '$', 'Q', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '4', '}', '}', '$', '}', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-4.5', ')', '--', '(', '0.25', ',', '-4.5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-5', ')', '--', '(', '0.25', ',', '-5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '-0.25', ',', '-4.5', ')', '--', '(', '-0.25', ',', '-5', ')', ';', '\\\\draw', '&#91;', 'red', '&#93;', '(', '0.25', ',', '-4.5', ')', '--', '(', '0.25', ',', '-5', ')', 'node', '&#91;', 'anchor', '=', 'north', ',', 'scale', '=', '0.7', '&#93;', '{', '$', '\\\\tilde', 'Q', '^', '-', '_', '{', '\\\\frac', '{', 'r', '_', '0', '}', '{', '4', '}', '}', '$', '}', ';', '\\\\end', '{', 'tikzpicture', '}', '\\\\end', '{', 'document', '}']]]\n",
      "Current hypotheses: [['This', 'is', 'a', 'picture', 'of', 'a', 'scientific', 'figure', '.', 'Generate', 'LaTeX', 'code', 'that', 'draws', 'this', 'scientific', 'figure', 'using', 'TikZ', '.', 'Ensure', 'that', 'the', 'LaTeX', 'code', 'is', 'self-contained', 'and', 'does', 'not', 'require', 'any', 'packages', 'except', 'TikZ-related', 'imports', '.', 'Don', '&apos;t', 'forget', 'to', 'include', '\\\\usepackage', '{', 'tikz', '}', '!', 'Return', 'your', 'result', 'in', 'a', '```latex', 'code', 'block', '.', '\\\\begin', '{', 'tikzpicture', '}', '\\\\draw', '&#91;', 'step', '=', '1cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.5cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.25cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.03125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.015625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0078125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00390625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.001953125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0009765625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00048828125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000244140625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0001220703125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00006103515625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000030517578125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000152587890625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000762939453125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000003814697265625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000019073486328125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000095367431640625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000476837158203125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000002384185791015625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000001192092900390625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000059604938046875cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000298024697265625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000014901234567890625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000000745058024609375cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000037252981220703125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000000186293906103515625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000000931470030517578125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000004657350152587890625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000002328675076291025390625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '('], ['This', 'is', 'a', 'picture', 'of', 'a', 'scientific', 'figure', '.', 'Generate', 'LaTeX', 'code', 'that', 'draws', 'this', 'scientific', 'figure', 'using', 'TikZ', '.', 'Ensure', 'that', 'the', 'LaTeX', 'code', 'is', 'self-contained', 'and', 'does', 'not', 'require', 'any', 'packages', 'except', 'TikZ-related', 'imports', '.', 'Don', '&apos;t', 'forget', 'to', 'include', '\\\\usepackage', '{', 'tikz', '}', '!', 'Return', 'your', 'result', 'in', 'a', '```latex', 'code', 'block', '.', '&lt;', 'OCR', '/', '&gt;', 'star', 'Rp', 'Rp', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'], ['This', 'is', 'a', 'picture', 'of', 'a', 'scientific', 'figure', '.', 'Generate', 'LaTeX', 'code', 'that', 'draws', 'this', 'scientific', 'figure', 'using', 'TikZ', '.', 'Ensure', 'that', 'the', 'LaTeX', 'code', 'is', 'self-contained', 'and', 'does', 'not', 'require', 'any', 'packages', 'except', 'TikZ-related', 'imports', '.', 'Don', '&apos;t', 'forget', 'to', 'include', '\\\\usepackage', '{', 'tikz', '}', '!', 'Return', 'your', 'result', 'in', 'a', '```latex', 'code', 'block', '.', '&lt;', 'OCR', '/', '&gt;', 'Outgoing', 'DPs', 'DP', 'DPs', 'DP', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'Rk', 'R'], ['This', 'is', 'a', 'picture', 'of', 'a', 'scientific', 'figure', '.', 'Generate', 'LaTeX', 'code', 'that', 'draws', 'this', 'scientific', 'figure', 'using', 'TikZ', '.', 'Ensure', 'that', 'the', 'LaTeX', 'code', 'is', 'self-contained', 'and', 'does', 'not', 'require', 'any', 'packages', 'except', 'TikZ-related', 'imports', '.', 'Don', '&apos;t', 'forget', 'to', 'include', '\\\\usepackage', '{', 'tikz', '}', '!', 'Return', 'your', 'result', 'in', 'a', '```latex', 'code', 'block', '.', '&lt;', 'OCR', '/', '&gt;', '1', '2', '3', '1', '2', '3', '4', '1', '2', '3', '1', '2', '3', '1', '2', '3', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5'], ['This', 'is', 'a', 'picture', 'of', 'a', 'scientific', 'figure', '.', 'Generate', 'LaTeX', 'code', 'that', 'draws', 'this', 'scientific', 'figure', 'using', 'TikZ', '.', 'Ensure', 'that', 'the', 'LaTeX', 'code', 'is', 'self-contained', 'and', 'does', 'not', 'require', 'any', 'packages', 'except', 'TikZ-related', 'imports', '.', 'Don', '&apos;t', 'forget', 'to', 'include', '\\\\usepackage', '{', 'tikz', '}', '!', 'Return', 'your', 'result', 'in', 'a', '```latex', 'code', 'block', '.', 'The', 'figure', 'is', 'a', 'bar', 'chart', 'with', 'the', 'x-axis', 'labeled', 'with', 'the', 'following', 'categories', ':', 'Business', ',', 'Education', ',', 'Adult', ',', 'Entertainment', ',', 'Illegal', 'Content', ',', 'Media', ',', 'Tech', 'Info', ',', 'Shopping', ',', 'Sport', ',', 'Uncategorized', '.', 'The', 'y-axis', 'is', 'labeled', 'Percentage', 'and', 'ranges', 'from', '0', 'to', '25', '.', 'The', 'bars', 'are', 'labeled', 'with', 'the', 'following', 'values', ':', '22', ',', '21', ',', '19', ',', '4', ',', '9', ',', '1', ',', '5', ',', '3', ',', '1', ',', '1', '.', 'The', 'bars', 'are', 'colored', 'as', 'follows', ':', 'Business', ':', 'black', ',', 'Education', ':', 'black', ',', 'Adult', ':', 'black', ',', 'Entertainment', ':', 'black', ',', 'Illegal', 'Content', ':', 'black', ',', 'Media', ':', 'black', ',', 'Tech', 'Info', ':', 'black', ',', 'Shopping', ':', 'black', ',', 'Sport', ':', 'black', ',', 'Uncategorized', ':', 'black', '.', 'The', 'background', 'is', 'white', '.', '&lt;', 'OCR', '/', '&gt;', '25', '20', '15', '10', '5', 'Business', 'Education', 'Adult', 'Adult', 'Entertainment', 'Content', 'Media', 'Tech', 'Info', 'Shopping', 'Sport', 'Uncategorized', 'Illegal', 'TechInfo', 'TechInfo', 'Shopping', 'Sport', 'Uncategorized', '0', 'Business', 'Education', 'Adult', 'Entertainment', 'Content', 'Media', 'Tech', 'Info', 'Shopping', 'Sport', 'Uncategorized', '0', '5', '10', '15', '20', '25', '0', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25', '0', '5', '10', '15', '20', '25'], ['This', 'is', 'a', 'picture', 'of', 'a', 'scientific', 'figure', '.', 'Generate', 'LaTeX', 'code', 'that', 'draws', 'this', 'scientific', 'figure', 'using', 'TikZ', '.', 'Ensure', 'that', 'the', 'LaTeX', 'code', 'is', 'self-contained', 'and', 'does', 'not', 'require', 'any', 'packages', 'except', 'TikZ-related', 'imports', '.', 'Don', '&apos;t', 'forget', 'to', 'include', '\\\\usepackage', '{', 'tikz', '}', '!', 'Return', 'your', 'result', 'in', 'a', '```latex', 'code', 'block', '.', '\\\\begin', '{', 'tikzpicture', '}', '\\\\draw', '&#91;', 'step', '=', '1cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.5cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.25cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.03125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.015625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0078125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00390625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.001953125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0009765625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00048828125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000244140625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0001220703125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00006103515625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000030517578125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000152587890625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000762939453125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000003814697265625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000019073486328125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000095367431640625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000476837158203125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000002384185791015625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000001192092900390625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000059604938046875cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000298024697265625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000014901234567890625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000000745058024609375cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000037252981220703125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.00000000186293906103515625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.000000000931470030517578125cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000004657350152587890625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(', '0,0', ')', 'grid', '(', '1,1', ')', ';', '\\\\draw', '&#91;', 'step', '=', '0.0000000002328675076291025390625cm', ',', 'gray', ',', 'very', 'thin', '&#93;', '(']]\n",
      "done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m image_url \u001b[39m=\u001b[39m example[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[39m# Generate TikZ code\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m generated_code \u001b[39m=\u001b[39m generate_tikz_code(image_url)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Kernel Inception Distance (KID) for generated vs. reference image\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# kid_metric.update(generated_code, reference_code)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m generated_code \u001b[39m=\u001b[39m extract_latex_code(generated_code)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mgenerate_tikz_code\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[39m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThis is a picture of a scientific figure. <|image|> Generate LaTeX code that draws this scientific figure using TikZ. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mEnsure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt forget to include \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39musepackage\u001b[39m\u001b[39m{tikz}\u001b[39;00m\u001b[39m! Return your result in a ```latex code block.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m inputs \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39mimage, text\u001b[39m=\u001b[39mprompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[39m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   2216\u001b[0m         input_ids,\n\u001b[1;32m   2217\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   2218\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   2219\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   2220\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   2221\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   2222\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   2223\u001b[0m     )\n\u001b[1;32m   2225\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39moutput_hidden_states\u001b[39m\u001b[39m\"\u001b[39m: output_hidden_states} \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3208\u001b[0m \u001b[39m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:2124\u001b[0m, in \u001b[0;36mMllamaForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, aspect_ratio_mask, aspect_ratio_ids, attention_mask, cross_attention_mask, cross_attention_states, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2121\u001b[0m     cross_attention_mask \u001b[39m=\u001b[39m cross_attention_mask[:, :, cache_position]\n\u001b[1;32m   2122\u001b[0m     full_text_row_masked_out_mask \u001b[39m=\u001b[39m full_text_row_masked_out_mask[:, :, cache_position]\n\u001b[0;32m-> 2124\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlanguage_model(\n\u001b[1;32m   2125\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   2126\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   2127\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   2128\u001b[0m     cross_attention_states\u001b[39m=\u001b[39;49mcross_attention_states,\n\u001b[1;32m   2129\u001b[0m     cross_attention_mask\u001b[39m=\u001b[39;49mcross_attention_mask,\n\u001b[1;32m   2130\u001b[0m     full_text_row_masked_out_mask\u001b[39m=\u001b[39;49mfull_text_row_masked_out_mask,\n\u001b[1;32m   2131\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   2132\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   2133\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   2134\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   2135\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2136\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2137\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   2138\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   2139\u001b[0m     num_logits_to_keep\u001b[39m=\u001b[39;49mnum_logits_to_keep,\n\u001b[1;32m   2140\u001b[0m )\n\u001b[1;32m   2142\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:1932\u001b[0m, in \u001b[0;36mMllamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1929\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1931\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1932\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1933\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1934\u001b[0m     cross_attention_states\u001b[39m=\u001b[39;49mcross_attention_states,\n\u001b[1;32m   1935\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1936\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1937\u001b[0m     cross_attention_mask\u001b[39m=\u001b[39;49mcross_attention_mask,\n\u001b[1;32m   1938\u001b[0m     full_text_row_masked_out_mask\u001b[39m=\u001b[39;49mfull_text_row_masked_out_mask,\n\u001b[1;32m   1939\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1940\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1941\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1942\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1943\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1944\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1945\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1946\u001b[0m )\n\u001b[1;32m   1948\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1949\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states[:, \u001b[39m-\u001b[39mnum_logits_to_keep:, :])\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:1672\u001b[0m, in \u001b[0;36mMllamaTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1658\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1659\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         position_embeddings,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1672\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1673\u001b[0m         hidden_states,\n\u001b[1;32m   1674\u001b[0m         cross_attention_states\u001b[39m=\u001b[39;49mcross_attention_states,\n\u001b[1;32m   1675\u001b[0m         cross_attention_mask\u001b[39m=\u001b[39;49mcross_attention_mask,\n\u001b[1;32m   1676\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m   1677\u001b[0m         full_text_row_masked_out_mask\u001b[39m=\u001b[39;49mfull_text_row_masked_out_mask,\n\u001b[1;32m   1678\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1679\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1680\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1681\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1682\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1683\u001b[0m         position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m   1684\u001b[0m     )\n\u001b[1;32m   1686\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1688\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/transformers/models/mllama/modeling_mllama.py:900\u001b[0m, in \u001b[0;36mMllamaSelfAttentionDecoderLayer.forward\u001b[0;34m(self, hidden_states, cross_attention_states, cross_attention_mask, attention_mask, full_text_row_masked_out_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    890\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    891\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    892\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m     position_embeddings\u001b[39m=\u001b[39mposition_embeddings,\n\u001b[1;32m    899\u001b[0m )\n\u001b[0;32m--> 900\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39;49m hidden_states\n\u001b[1;32m    902\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[1;32m    903\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_tikz_code(image):\n",
    "    prompt = (\n",
    "        \"This is a picture of a scientific figure. <|image|> Generate LaTeX code that draws this scientific figure using TikZ. \"\n",
    "        \"Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. \"\n",
    "        \"Don't forget to include \\\\usepackage{tikz}! Return your result in a ```latex code block.\"\n",
    "    )\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=1000)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def extract_latex_code(full_output):\n",
    "    # Example: Assume the actual code starts after a specific marker or pattern\n",
    "    marker = \"Return your result in a ```latex code block.\"\n",
    "    try:\n",
    "        start = full_output.index(marker) + len(marker)\n",
    "        # Extract until the end or until another specific point\n",
    "        end = full_output.index('End of LaTeX code', start)  # If you have an end marker\n",
    "        return full_output[start:end].strip()\n",
    "    except ValueError:\n",
    "        return full_output  # fallback if no markers found\n",
    "\n",
    "\n",
    "# Evaluation Loop\n",
    "for example in ds_test:\n",
    "    reference_code = example['code']\n",
    "    image_url = example['image']\n",
    "\n",
    "    # Generate TikZ code\n",
    "    generated_code = generate_tikz_code(image_url)\n",
    "    \n",
    "    # Kernel Inception Distance (KID) for generated vs. reference image\n",
    "    # kid_metric.update(generated_code, reference_code)\n",
    "    generated_code = extract_latex_code(generated_code)\n",
    "    \n",
    "    # TEX Edit Distance\n",
    "    tex_edit_distance.update([generated_code], [[reference_code]])\n",
    "    print(tex_edit_distance.sentence_eed)\n",
    "\n",
    "    # Image Similarity\n",
    "    # img_sim_score = image_sim_metric.compute(generated_code, reference_code)\n",
    "    \n",
    "    # DreamSim Metric\n",
    "    # dreamsim_score = dreamsim_metric.compute(generated_code, reference_code)\n",
    "\n",
    "    # CrystalBLEU\n",
    "    corpus_bleu_metric.update(\n",
    "        list_of_references=[[reference_code]],\n",
    "        hypotheses=[generated_code]\n",
    "    )\n",
    "    print(\"Current list_of_references:\", corpus_bleu_metric.list_of_references)\n",
    "    print(\"Current hypotheses:\", corpus_bleu_metric.hypotheses)\n",
    "\n",
    "    \n",
    "    print(\"done\")\n",
    "\n",
    "# Compute and print final scores\n",
    "# print(\"Kernel Inception Distance:\", kid_metric.compute())\n",
    "# print(\"TEX Edit Distance:\", tex_edit_distance.compute())\n",
    "# # print(\"Image Similarity:\", img_sim_score)\n",
    "# # print(\"DreamSim Score:\", dreamsim_score)\n",
    "# print(\"CrystalBLEU Score:\", corpus_bleu_metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEX Edit Distance: 0.0\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'corpus_bleu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(tex_edit_distance\u001b[39m.\u001b[39msentence_eed)\n\u001b[1;32m      3\u001b[0m \u001b[39m# print(\"Image Similarity:\", img_sim_score)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# print(\"DreamSim Score:\", dreamsim_score)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCrystalBLEU Score:\u001b[39m\u001b[39m\"\u001b[39m, corpus_bleu_metric\u001b[39m.\u001b[39;49mcompute())\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torchmetrics/metric.py:633\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[39m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_context(\n\u001b[1;32m    629\u001b[0m     dist_sync_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_fn,\n\u001b[1;32m    630\u001b[0m     should_sync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync,\n\u001b[1;32m    631\u001b[0m     should_unsync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_unsync,\n\u001b[1;32m    632\u001b[0m ):\n\u001b[0;32m--> 633\u001b[0m     value \u001b[39m=\u001b[39m _squeeze_if_scalar(compute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    634\u001b[0m     \u001b[39m# clone tensor to avoid in-place operations after compute, altering already computed results\u001b[39;00m\n\u001b[1;32m    635\u001b[0m     value \u001b[39m=\u001b[39m apply_to_collection(value, Tensor, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mclone())\n",
      "File \u001b[0;32m~/Downloads/ai_project/DeTikZify/detikzify/evaluate/crystalbleu1.py:103\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist_of_references\u001b[39m.\u001b[39mextend([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenize(ref) \u001b[39mfor\u001b[39;00m ref \u001b[39min\u001b[39;00m refs] \u001b[39mfor\u001b[39;00m refs \u001b[39min\u001b[39;00m list_of_references)\n\u001b[1;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypotheses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenize(hyp) \u001b[39mfor\u001b[39;00m hyp \u001b[39min\u001b[39;00m hypotheses)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m corpus_bleu(\n\u001b[1;32m    105\u001b[0m         list_of_references\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist_of_references,\n\u001b[1;32m    106\u001b[0m         hypotheses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypotheses,\n\u001b[1;32m    107\u001b[0m         ignoring\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrivially_shared_ngrams\n\u001b[1;32m    108\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_bleu' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"TEX Edit Distance:\", tex_edit_distance.compute())\n",
    "print(tex_edit_distance.sentence_eed)\n",
    "# print(\"Image Similarity:\", img_sim_score)\n",
    "# print(\"DreamSim Score:\", dreamsim_score)\n",
    "print(\"CrystalBLEU Score:\", corpus_bleu_metric.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
