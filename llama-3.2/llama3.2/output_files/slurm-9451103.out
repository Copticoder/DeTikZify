Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:13,  3.28s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:04,  1.44s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:01,  1.17it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.73it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  2.45it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.30it/s]
Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:23<03:04, 23.02s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:45<02:40, 22.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [01:08<02:17, 22.93s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [01:31<01:54, 22.97s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [01:54<01:31, 22.81s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [02:38<01:29, 29.91s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [03:21<01:08, 34.29s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [03:37<00:28, 28.47s/it]Loading checkpoint shards: 100%|██████████| 9/9 [03:48<00:00, 22.95s/it]Loading checkpoint shards: 100%|██████████| 9/9 [03:48<00:00, 25.35s/it]
Some weights of the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant were not used when initializing MllamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.cross_attn.q_proj.base_layer.weight', 'model.layers.13.cross_attn.q_proj.lora_A.default.weight', 'model.layers.13.cross_attn.q_proj.lora_B.default.weight', 'model.layers.13.cross_attn.v_proj.base_layer.weight', 'model.layers.13.cross_attn.v_proj.lora_A.default.weight', 'model.layers.13.cross_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.cross_attn.q_proj.base_layer.weight', 'model.layers.18.cross_attn.q_proj.lora_A.default.weight', 'model.layers.18.cross_attn.q_proj.lora_B.default.weight', 'model.layers.18.cross_attn.v_proj.base_layer.weight', 'model.layers.18.cross_attn.v_proj.lora_A.default.weight', 'model.layers.18.cross_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.cross_attn.q_proj.base_layer.weight', 'model.layers.23.cross_attn.q_proj.lora_A.default.weight', 'model.layers.23.cross_attn.q_proj.lora_B.default.weight', 'model.layers.23.cross_attn.v_proj.base_layer.weight', 'model.layers.23.cross_attn.v_proj.lora_A.default.weight', 'model.layers.23.cross_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.cross_attn.q_proj.base_layer.weight', 'model.layers.28.cross_attn.q_proj.lora_A.default.weight', 'model.layers.28.cross_attn.q_proj.lora_B.default.weight', 'model.layers.28.cross_attn.v_proj.base_layer.weight', 'model.layers.28.cross_attn.v_proj.lora_A.default.weight', 'model.layers.28.cross_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.cross_attn.q_proj.base_layer.weight', 'model.layers.3.cross_attn.q_proj.lora_A.default.weight', 'model.layers.3.cross_attn.q_proj.lora_B.default.weight', 'model.layers.3.cross_attn.v_proj.base_layer.weight', 'model.layers.3.cross_attn.v_proj.lora_A.default.weight', 'model.layers.3.cross_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.cross_attn.q_proj.base_layer.weight', 'model.layers.33.cross_attn.q_proj.lora_A.default.weight', 'model.layers.33.cross_attn.q_proj.lora_B.default.weight', 'model.layers.33.cross_attn.v_proj.base_layer.weight', 'model.layers.33.cross_attn.v_proj.lora_A.default.weight', 'model.layers.33.cross_attn.v_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.cross_attn.q_proj.base_layer.weight', 'model.layers.38.cross_attn.q_proj.lora_A.default.weight', 'model.layers.38.cross_attn.q_proj.lora_B.default.weight', 'model.layers.38.cross_attn.v_proj.base_layer.weight', 'model.layers.38.cross_attn.v_proj.lora_A.default.weight', 'model.layers.38.cross_attn.v_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.cross_attn.q_proj.base_layer.weight', 'model.layers.8.cross_attn.q_proj.lora_A.default.weight', 'model.layers.8.cross_attn.q_proj.lora_B.default.weight', 'model.layers.8.cross_attn.v_proj.base_layer.weight', 'model.layers.8.cross_attn.v_proj.lora_A.default.weight', 'model.layers.8.cross_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']
- This IS expected if you are initializing MllamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MllamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MllamaForCausalLM were not initialized from the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.cross_attn.q_proj.weight', 'model.layers.13.cross_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.cross_attn.q_proj.weight', 'model.layers.18.cross_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.cross_attn.q_proj.weight', 'model.layers.23.cross_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.cross_attn.q_proj.weight', 'model.layers.28.cross_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.cross_attn.q_proj.weight', 'model.layers.3.cross_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.cross_attn.q_proj.weight', 'model.layers.33.cross_attn.v_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.cross_attn.q_proj.weight', 'model.layers.38.cross_attn.v_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.cross_attn.q_proj.weight', 'model.layers.8.cross_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from '/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant' to 'mylesgoose/Llama-3.2-11B-Vision-Instruct'. Please ensure that the correct base model is loaded when loading this checkpoint.
  warnings.warn(
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)  # noqa: B028
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00001-of-00009.safetensors
Loading weights for: base_model.model.model.embed_tokens.weight
Loading weights for: base_model.model.model.layers.0.input_layernorm.weight
Loading weights for: base_model.model.model.layers.0.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.0.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.0.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.input_layernorm.weight
Loading weights for: base_model.model.model.layers.1.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.1.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.1.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.input_layernorm.weight
Loading weights for: base_model.model.model.layers.2.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.2.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.2.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.3.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.3.input_layernorm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00002-of-00009.safetensors
Loading weights for: base_model.model.model.layers.3.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.3.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.input_layernorm.weight
Loading weights for: base_model.model.model.layers.4.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.4.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.input_layernorm.weight
Loading weights for: base_model.model.model.layers.5.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.5.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.5.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.input_layernorm.weight
Loading weights for: base_model.model.model.layers.6.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.6.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.6.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.input_layernorm.weight
Loading weights for: base_model.model.model.layers.7.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.7.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.7.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.8.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.8.input_layernorm.weight
Loading weights for: base_model.model.model.layers.8.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.8.mlp.up_proj.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00003-of-00009.safetensors
Loading weights for: base_model.model.model.layers.10.input_layernorm.weight
Loading weights for: base_model.model.model.layers.10.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.10.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.10.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.input_layernorm.weight
Loading weights for: base_model.model.model.layers.11.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.11.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.11.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.input_layernorm.weight
Loading weights for: base_model.model.model.layers.12.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.12.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.12.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.13.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.13.input_layernorm.weight
Loading weights for: base_model.model.model.layers.13.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.13.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.14.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.8.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.input_layernorm.weight
Loading weights for: base_model.model.model.layers.9.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.9.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00004-of-00009.safetensors
Loading weights for: base_model.model.model.layers.14.input_layernorm.weight
Loading weights for: base_model.model.model.layers.14.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.14.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.input_layernorm.weight
Loading weights for: base_model.model.model.layers.15.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.15.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.input_layernorm.weight
Loading weights for: base_model.model.model.layers.16.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.16.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.16.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.input_layernorm.weight
Loading weights for: base_model.model.model.layers.17.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.17.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.17.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.18.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.18.input_layernorm.weight
Loading weights for: base_model.model.model.layers.18.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.18.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.19.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00005-of-00009.safetensors
Loading weights for: base_model.model.model.layers.19.input_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.19.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.input_layernorm.weight
Loading weights for: base_model.model.model.layers.20.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.20.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.input_layernorm.weight
Loading weights for: base_model.model.model.layers.21.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.21.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.21.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.input_layernorm.weight
Loading weights for: base_model.model.model.layers.22.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.22.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.22.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.23.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.23.input_layernorm.weight
Loading weights for: base_model.model.model.layers.23.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.23.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.input_layernorm.weight
Loading weights for: base_model.model.model.layers.24.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.24.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00006-of-00009.safetensors
Loading weights for: base_model.model.model.layers.25.input_layernorm.weight
Loading weights for: base_model.model.model.layers.25.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.25.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.input_layernorm.weight
Loading weights for: base_model.model.model.layers.26.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.26.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.input_layernorm.weight
Loading weights for: base_model.model.model.layers.27.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.27.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.27.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.28.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.28.input_layernorm.weight
Loading weights for: base_model.model.model.layers.28.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.28.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.input_layernorm.weight
Loading weights for: base_model.model.model.layers.29.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.29.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.30.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00007-of-00009.safetensors
Loading weights for: base_model.model.model.layers.30.input_layernorm.weight
Loading weights for: base_model.model.model.layers.30.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.30.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.input_layernorm.weight
Loading weights for: base_model.model.model.layers.31.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.31.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.input_layernorm.weight
Loading weights for: base_model.model.model.layers.32.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.32.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.32.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.33.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.33.input_layernorm.weight
Loading weights for: base_model.model.model.layers.33.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.33.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.input_layernorm.weight
Loading weights for: base_model.model.model.layers.34.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.34.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.input_layernorm.weight
Loading weights for: base_model.model.model.layers.35.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.35.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.35.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00008-of-00009.safetensors
Loading weights for: base_model.model.model.layers.36.input_layernorm.weight
Loading weights for: base_model.model.model.layers.36.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.36.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.input_layernorm.weight
Loading weights for: base_model.model.model.layers.37.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.37.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.38.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.38.input_layernorm.weight
Loading weights for: base_model.model.model.layers.38.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.38.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.input_layernorm.weight
Loading weights for: base_model.model.model.layers.39.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.39.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.norm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00009-of-00009.safetensors
Loading weights for: base_model.model.lm_head.weight
Missing in checkpoint: vision_model.class_embedding
Missing in checkpoint: vision_model.patch_embedding.weight
Missing in checkpoint: vision_model.gated_positional_embedding.gate
Missing in checkpoint: vision_model.gated_positional_embedding.embedding
Missing in checkpoint: vision_model.gated_positional_embedding.tile_embedding.weight
Missing in checkpoint: vision_model.pre_tile_positional_embedding.gate
Missing in checkpoint: vision_model.pre_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.post_tile_positional_embedding.gate
Missing in checkpoint: vision_model.post_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.layernorm_pre.weight
Missing in checkpoint: vision_model.layernorm_pre.bias
Missing in checkpoint: vision_model.layernorm_post.weight
Missing in checkpoint: vision_model.layernorm_post.bias
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: language_model.model.embed_tokens.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.0.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.0.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.1.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.2.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.3.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.3.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.4.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.5.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.6.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.7.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.8.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.8.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.9.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.10.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.11.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.12.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.13.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.13.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.14.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.15.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.16.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.17.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.18.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.18.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.19.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.20.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.21.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.22.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.23.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.23.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.24.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.25.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.26.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.27.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.28.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.28.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.29.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.30.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.31.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.32.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.33.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.33.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.34.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.35.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.36.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.37.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.38.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.38.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.39.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.norm.weight
Missing in checkpoint: language_model.lm_head.weight
Missing in checkpoint: multi_modal_projector.weight
Missing in checkpoint: multi_modal_projector.bias
Extra in checkpoint: model.embed_tokens.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.k_proj.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.o_proj.weight
Extra in checkpoint: model.layers.0.mlp.gate_proj.weight
Extra in checkpoint: model.layers.0.mlp.up_proj.weight
Extra in checkpoint: model.layers.0.mlp.down_proj.weight
Extra in checkpoint: model.layers.0.input_layernorm.weight
Extra in checkpoint: model.layers.0.post_attention_layernorm.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.k_proj.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.o_proj.weight
Extra in checkpoint: model.layers.1.mlp.gate_proj.weight
Extra in checkpoint: model.layers.1.mlp.up_proj.weight
Extra in checkpoint: model.layers.1.mlp.down_proj.weight
Extra in checkpoint: model.layers.1.input_layernorm.weight
Extra in checkpoint: model.layers.1.post_attention_layernorm.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.k_proj.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.o_proj.weight
Extra in checkpoint: model.layers.2.mlp.gate_proj.weight
Extra in checkpoint: model.layers.2.mlp.up_proj.weight
Extra in checkpoint: model.layers.2.mlp.down_proj.weight
Extra in checkpoint: model.layers.2.input_layernorm.weight
Extra in checkpoint: model.layers.2.post_attention_layernorm.weight
Extra in checkpoint: model.layers.3.cross_attn_attn_gate
Extra in checkpoint: model.layers.3.cross_attn_mlp_gate
Extra in checkpoint: model.layers.3.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.3.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.3.input_layernorm.weight
Extra in checkpoint: model.layers.3.mlp.gate_proj.weight
Extra in checkpoint: model.layers.3.mlp.up_proj.weight
Extra in checkpoint: model.layers.3.mlp.down_proj.weight
Extra in checkpoint: model.layers.3.post_attention_layernorm.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.k_proj.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.o_proj.weight
Extra in checkpoint: model.layers.4.mlp.gate_proj.weight
Extra in checkpoint: model.layers.4.mlp.up_proj.weight
Extra in checkpoint: model.layers.4.mlp.down_proj.weight
Extra in checkpoint: model.layers.4.input_layernorm.weight
Extra in checkpoint: model.layers.4.post_attention_layernorm.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.k_proj.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.o_proj.weight
Extra in checkpoint: model.layers.5.mlp.gate_proj.weight
Extra in checkpoint: model.layers.5.mlp.up_proj.weight
Extra in checkpoint: model.layers.5.mlp.down_proj.weight
Extra in checkpoint: model.layers.5.input_layernorm.weight
Extra in checkpoint: model.layers.5.post_attention_layernorm.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.k_proj.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.o_proj.weight
Extra in checkpoint: model.layers.6.mlp.gate_proj.weight
Extra in checkpoint: model.layers.6.mlp.up_proj.weight
Extra in checkpoint: model.layers.6.mlp.down_proj.weight
Extra in checkpoint: model.layers.6.input_layernorm.weight
Extra in checkpoint: model.layers.6.post_attention_layernorm.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.k_proj.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.o_proj.weight
Extra in checkpoint: model.layers.7.mlp.gate_proj.weight
Extra in checkpoint: model.layers.7.mlp.up_proj.weight
Extra in checkpoint: model.layers.7.mlp.down_proj.weight
Extra in checkpoint: model.layers.7.input_layernorm.weight
Extra in checkpoint: model.layers.7.post_attention_layernorm.weight
Extra in checkpoint: model.layers.8.cross_attn_attn_gate
Extra in checkpoint: model.layers.8.cross_attn_mlp_gate
Extra in checkpoint: model.layers.8.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.8.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.8.input_layernorm.weight
Extra in checkpoint: model.layers.8.mlp.gate_proj.weight
Extra in checkpoint: model.layers.8.mlp.up_proj.weight
Extra in checkpoint: model.layers.8.mlp.down_proj.weight
Extra in checkpoint: model.layers.8.post_attention_layernorm.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.k_proj.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.o_proj.weight
Extra in checkpoint: model.layers.9.mlp.gate_proj.weight
Extra in checkpoint: model.layers.9.mlp.up_proj.weight
Extra in checkpoint: model.layers.9.mlp.down_proj.weight
Extra in checkpoint: model.layers.9.input_layernorm.weight
Extra in checkpoint: model.layers.9.post_attention_layernorm.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.k_proj.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.o_proj.weight
Extra in checkpoint: model.layers.10.mlp.gate_proj.weight
Extra in checkpoint: model.layers.10.mlp.up_proj.weight
Extra in checkpoint: model.layers.10.mlp.down_proj.weight
Extra in checkpoint: model.layers.10.input_layernorm.weight
Extra in checkpoint: model.layers.10.post_attention_layernorm.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.k_proj.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.o_proj.weight
Extra in checkpoint: model.layers.11.mlp.gate_proj.weight
Extra in checkpoint: model.layers.11.mlp.up_proj.weight
Extra in checkpoint: model.layers.11.mlp.down_proj.weight
Extra in checkpoint: model.layers.11.input_layernorm.weight
Extra in checkpoint: model.layers.11.post_attention_layernorm.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.k_proj.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.o_proj.weight
Extra in checkpoint: model.layers.12.mlp.gate_proj.weight
Extra in checkpoint: model.layers.12.mlp.up_proj.weight
Extra in checkpoint: model.layers.12.mlp.down_proj.weight
Extra in checkpoint: model.layers.12.input_layernorm.weight
Extra in checkpoint: model.layers.12.post_attention_layernorm.weight
Extra in checkpoint: model.layers.13.cross_attn_attn_gate
Extra in checkpoint: model.layers.13.cross_attn_mlp_gate
Extra in checkpoint: model.layers.13.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.13.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.13.input_layernorm.weight
Extra in checkpoint: model.layers.13.mlp.gate_proj.weight
Extra in checkpoint: model.layers.13.mlp.up_proj.weight
Extra in checkpoint: model.layers.13.mlp.down_proj.weight
Extra in checkpoint: model.layers.13.post_attention_layernorm.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.k_proj.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.o_proj.weight
Extra in checkpoint: model.layers.14.mlp.gate_proj.weight
Extra in checkpoint: model.layers.14.mlp.up_proj.weight
Extra in checkpoint: model.layers.14.mlp.down_proj.weight
Extra in checkpoint: model.layers.14.input_layernorm.weight
Extra in checkpoint: model.layers.14.post_attention_layernorm.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.k_proj.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.o_proj.weight
Extra in checkpoint: model.layers.15.mlp.gate_proj.weight
Extra in checkpoint: model.layers.15.mlp.up_proj.weight
Extra in checkpoint: model.layers.15.mlp.down_proj.weight
Extra in checkpoint: model.layers.15.input_layernorm.weight
Extra in checkpoint: model.layers.15.post_attention_layernorm.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.k_proj.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.o_proj.weight
Extra in checkpoint: model.layers.16.mlp.gate_proj.weight
Extra in checkpoint: model.layers.16.mlp.up_proj.weight
Extra in checkpoint: model.layers.16.mlp.down_proj.weight
Extra in checkpoint: model.layers.16.input_layernorm.weight
Extra in checkpoint: model.layers.16.post_attention_layernorm.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.k_proj.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.o_proj.weight
Extra in checkpoint: model.layers.17.mlp.gate_proj.weight
Extra in checkpoint: model.layers.17.mlp.up_proj.weight
Extra in checkpoint: model.layers.17.mlp.down_proj.weight
Extra in checkpoint: model.layers.17.input_layernorm.weight
Extra in checkpoint: model.layers.17.post_attention_layernorm.weight
Extra in checkpoint: model.layers.18.cross_attn_attn_gate
Extra in checkpoint: model.layers.18.cross_attn_mlp_gate
Extra in checkpoint: model.layers.18.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.18.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.18.input_layernorm.weight
Extra in checkpoint: model.layers.18.mlp.gate_proj.weight
Extra in checkpoint: model.layers.18.mlp.up_proj.weight
Extra in checkpoint: model.layers.18.mlp.down_proj.weight
Extra in checkpoint: model.layers.18.post_attention_layernorm.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.k_proj.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.o_proj.weight
Extra in checkpoint: model.layers.19.mlp.gate_proj.weight
Extra in checkpoint: model.layers.19.mlp.up_proj.weight
Extra in checkpoint: model.layers.19.mlp.down_proj.weight
Extra in checkpoint: model.layers.19.input_layernorm.weight
Extra in checkpoint: model.layers.19.post_attention_layernorm.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.k_proj.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.o_proj.weight
Extra in checkpoint: model.layers.20.mlp.gate_proj.weight
Extra in checkpoint: model.layers.20.mlp.up_proj.weight
Extra in checkpoint: model.layers.20.mlp.down_proj.weight
Extra in checkpoint: model.layers.20.input_layernorm.weight
Extra in checkpoint: model.layers.20.post_attention_layernorm.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.k_proj.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.o_proj.weight
Extra in checkpoint: model.layers.21.mlp.gate_proj.weight
Extra in checkpoint: model.layers.21.mlp.up_proj.weight
Extra in checkpoint: model.layers.21.mlp.down_proj.weight
Extra in checkpoint: model.layers.21.input_layernorm.weight
Extra in checkpoint: model.layers.21.post_attention_layernorm.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.k_proj.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.o_proj.weight
Extra in checkpoint: model.layers.22.mlp.gate_proj.weight
Extra in checkpoint: model.layers.22.mlp.up_proj.weight
Extra in checkpoint: model.layers.22.mlp.down_proj.weight
Extra in checkpoint: model.layers.22.input_layernorm.weight
Extra in checkpoint: model.layers.22.post_attention_layernorm.weight
Extra in checkpoint: model.layers.23.cross_attn_attn_gate
Extra in checkpoint: model.layers.23.cross_attn_mlp_gate
Extra in checkpoint: model.layers.23.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.23.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.23.input_layernorm.weight
Extra in checkpoint: model.layers.23.mlp.gate_proj.weight
Extra in checkpoint: model.layers.23.mlp.up_proj.weight
Extra in checkpoint: model.layers.23.mlp.down_proj.weight
Extra in checkpoint: model.layers.23.post_attention_layernorm.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.k_proj.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.o_proj.weight
Extra in checkpoint: model.layers.24.mlp.gate_proj.weight
Extra in checkpoint: model.layers.24.mlp.up_proj.weight
Extra in checkpoint: model.layers.24.mlp.down_proj.weight
Extra in checkpoint: model.layers.24.input_layernorm.weight
Extra in checkpoint: model.layers.24.post_attention_layernorm.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.k_proj.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.o_proj.weight
Extra in checkpoint: model.layers.25.mlp.gate_proj.weight
Extra in checkpoint: model.layers.25.mlp.up_proj.weight
Extra in checkpoint: model.layers.25.mlp.down_proj.weight
Extra in checkpoint: model.layers.25.input_layernorm.weight
Extra in checkpoint: model.layers.25.post_attention_layernorm.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.k_proj.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.o_proj.weight
Extra in checkpoint: model.layers.26.mlp.gate_proj.weight
Extra in checkpoint: model.layers.26.mlp.up_proj.weight
Extra in checkpoint: model.layers.26.mlp.down_proj.weight
Extra in checkpoint: model.layers.26.input_layernorm.weight
Extra in checkpoint: model.layers.26.post_attention_layernorm.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.k_proj.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.o_proj.weight
Extra in checkpoint: model.layers.27.mlp.gate_proj.weight
Extra in checkpoint: model.layers.27.mlp.up_proj.weight
Extra in checkpoint: model.layers.27.mlp.down_proj.weight
Extra in checkpoint: model.layers.27.input_layernorm.weight
Extra in checkpoint: model.layers.27.post_attention_layernorm.weight
Extra in checkpoint: model.layers.28.cross_attn_attn_gate
Extra in checkpoint: model.layers.28.cross_attn_mlp_gate
Extra in checkpoint: model.layers.28.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.28.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.28.input_layernorm.weight
Extra in checkpoint: model.layers.28.mlp.gate_proj.weight
Extra in checkpoint: model.layers.28.mlp.up_proj.weight
Extra in checkpoint: model.layers.28.mlp.down_proj.weight
Extra in checkpoint: model.layers.28.post_attention_layernorm.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.k_proj.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.o_proj.weight
Extra in checkpoint: model.layers.29.mlp.gate_proj.weight
Extra in checkpoint: model.layers.29.mlp.up_proj.weight
Extra in checkpoint: model.layers.29.mlp.down_proj.weight
Extra in checkpoint: model.layers.29.input_layernorm.weight
Extra in checkpoint: model.layers.29.post_attention_layernorm.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.k_proj.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.o_proj.weight
Extra in checkpoint: model.layers.30.mlp.gate_proj.weight
Extra in checkpoint: model.layers.30.mlp.up_proj.weight
Extra in checkpoint: model.layers.30.mlp.down_proj.weight
Extra in checkpoint: model.layers.30.input_layernorm.weight
Extra in checkpoint: model.layers.30.post_attention_layernorm.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.k_proj.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.o_proj.weight
Extra in checkpoint: model.layers.31.mlp.gate_proj.weight
Extra in checkpoint: model.layers.31.mlp.up_proj.weight
Extra in checkpoint: model.layers.31.mlp.down_proj.weight
Extra in checkpoint: model.layers.31.input_layernorm.weight
Extra in checkpoint: model.layers.31.post_attention_layernorm.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.k_proj.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.o_proj.weight
Extra in checkpoint: model.layers.32.mlp.gate_proj.weight
Extra in checkpoint: model.layers.32.mlp.up_proj.weight
Extra in checkpoint: model.layers.32.mlp.down_proj.weight
Extra in checkpoint: model.layers.32.input_layernorm.weight
Extra in checkpoint: model.layers.32.post_attention_layernorm.weight
Extra in checkpoint: model.layers.33.cross_attn_attn_gate
Extra in checkpoint: model.layers.33.cross_attn_mlp_gate
Extra in checkpoint: model.layers.33.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.33.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.33.input_layernorm.weight
Extra in checkpoint: model.layers.33.mlp.gate_proj.weight
Extra in checkpoint: model.layers.33.mlp.up_proj.weight
Extra in checkpoint: model.layers.33.mlp.down_proj.weight
Extra in checkpoint: model.layers.33.post_attention_layernorm.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.k_proj.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.o_proj.weight
Extra in checkpoint: model.layers.34.mlp.gate_proj.weight
Extra in checkpoint: model.layers.34.mlp.up_proj.weight
Extra in checkpoint: model.layers.34.mlp.down_proj.weight
Extra in checkpoint: model.layers.34.input_layernorm.weight
Extra in checkpoint: model.layers.34.post_attention_layernorm.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.k_proj.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.o_proj.weight
Extra in checkpoint: model.layers.35.mlp.gate_proj.weight
Extra in checkpoint: model.layers.35.mlp.up_proj.weight
Extra in checkpoint: model.layers.35.mlp.down_proj.weight
Extra in checkpoint: model.layers.35.input_layernorm.weight
Extra in checkpoint: model.layers.35.post_attention_layernorm.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.k_proj.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.o_proj.weight
Extra in checkpoint: model.layers.36.mlp.gate_proj.weight
Extra in checkpoint: model.layers.36.mlp.up_proj.weight
Extra in checkpoint: model.layers.36.mlp.down_proj.weight
Extra in checkpoint: model.layers.36.input_layernorm.weight
Extra in checkpoint: model.layers.36.post_attention_layernorm.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.k_proj.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.o_proj.weight
Extra in checkpoint: model.layers.37.mlp.gate_proj.weight
Extra in checkpoint: model.layers.37.mlp.up_proj.weight
Extra in checkpoint: model.layers.37.mlp.down_proj.weight
Extra in checkpoint: model.layers.37.input_layernorm.weight
Extra in checkpoint: model.layers.37.post_attention_layernorm.weight
Extra in checkpoint: model.layers.38.cross_attn_attn_gate
Extra in checkpoint: model.layers.38.cross_attn_mlp_gate
Extra in checkpoint: model.layers.38.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.38.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.38.input_layernorm.weight
Extra in checkpoint: model.layers.38.mlp.gate_proj.weight
Extra in checkpoint: model.layers.38.mlp.up_proj.weight
Extra in checkpoint: model.layers.38.mlp.down_proj.weight
Extra in checkpoint: model.layers.38.post_attention_layernorm.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.k_proj.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.o_proj.weight
Extra in checkpoint: model.layers.39.mlp.gate_proj.weight
Extra in checkpoint: model.layers.39.mlp.up_proj.weight
Extra in checkpoint: model.layers.39.mlp.down_proj.weight
Extra in checkpoint: model.layers.39.input_layernorm.weight
Extra in checkpoint: model.layers.39.post_attention_layernorm.weight
Extra in checkpoint: model.norm.weight
Extra in checkpoint: lm_head.weight
Some layers did not match.
LoRA model loaded successfully!
bbbbb
cuda:0
Length of the updated dataset: 560
Traceback (most recent call last):
  File "/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/eval_normal.py", line 779, in <module>
    
  File "/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/eval_normal.py", line 665, in generate_tikz_code
    
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/peft/peft_model.py", line 817, in generate
    return self.get_base_model().generate(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/generation/utils.py", line 2215, in generate
    result = self._sample(
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/generation/utils.py", line 3206, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/models/mllama/modeling_mllama.py", line 2098, in forward
    vision_outputs = self.vision_model(
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/models/mllama/modeling_mllama.py", line 1454, in forward
    output = self.transformer(
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/models/mllama/modeling_mllama.py", line 389, in forward
    layer_outputs = encoder_layer(
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/models/mllama/modeling_mllama.py", line 302, in forward
    hidden_state, attn_weights = self.self_attn(hidden_state, attention_mask=attention_mask)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/models/mllama/modeling_mllama.py", line 246, in forward
    query = self.q_proj(hidden_state)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 584, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 11.94 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.38 GiB is allocated by PyTorch, and 503.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
slurmstepd: error: *** JOB 9451103 ON cn004 CANCELLED AT 2024-11-17T14:34:20 ***
