Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:00,  5.69it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:00<00:00,  9.08it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 10.57it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  9.80it/s]
Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:21<02:50, 21.31s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:41<02:26, 20.90s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [01:03<02:06, 21.09s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [01:23<01:43, 20.72s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [01:43<01:22, 20.56s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [02:04<01:01, 20.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [02:24<00:41, 20.55s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [02:39<00:18, 18.65s/it]Loading checkpoint shards: 100%|██████████| 9/9 [02:49<00:00, 15.82s/it]Loading checkpoint shards: 100%|██████████| 9/9 [02:49<00:00, 18.78s/it]
Some weights of the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant were not used when initializing MllamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.cross_attn.q_proj.base_layer.weight', 'model.layers.13.cross_attn.q_proj.lora_A.default.weight', 'model.layers.13.cross_attn.q_proj.lora_B.default.weight', 'model.layers.13.cross_attn.v_proj.base_layer.weight', 'model.layers.13.cross_attn.v_proj.lora_A.default.weight', 'model.layers.13.cross_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.cross_attn.q_proj.base_layer.weight', 'model.layers.18.cross_attn.q_proj.lora_A.default.weight', 'model.layers.18.cross_attn.q_proj.lora_B.default.weight', 'model.layers.18.cross_attn.v_proj.base_layer.weight', 'model.layers.18.cross_attn.v_proj.lora_A.default.weight', 'model.layers.18.cross_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.cross_attn.q_proj.base_layer.weight', 'model.layers.23.cross_attn.q_proj.lora_A.default.weight', 'model.layers.23.cross_attn.q_proj.lora_B.default.weight', 'model.layers.23.cross_attn.v_proj.base_layer.weight', 'model.layers.23.cross_attn.v_proj.lora_A.default.weight', 'model.layers.23.cross_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.cross_attn.q_proj.base_layer.weight', 'model.layers.28.cross_attn.q_proj.lora_A.default.weight', 'model.layers.28.cross_attn.q_proj.lora_B.default.weight', 'model.layers.28.cross_attn.v_proj.base_layer.weight', 'model.layers.28.cross_attn.v_proj.lora_A.default.weight', 'model.layers.28.cross_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.cross_attn.q_proj.base_layer.weight', 'model.layers.3.cross_attn.q_proj.lora_A.default.weight', 'model.layers.3.cross_attn.q_proj.lora_B.default.weight', 'model.layers.3.cross_attn.v_proj.base_layer.weight', 'model.layers.3.cross_attn.v_proj.lora_A.default.weight', 'model.layers.3.cross_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.cross_attn.q_proj.base_layer.weight', 'model.layers.33.cross_attn.q_proj.lora_A.default.weight', 'model.layers.33.cross_attn.q_proj.lora_B.default.weight', 'model.layers.33.cross_attn.v_proj.base_layer.weight', 'model.layers.33.cross_attn.v_proj.lora_A.default.weight', 'model.layers.33.cross_attn.v_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.cross_attn.q_proj.base_layer.weight', 'model.layers.38.cross_attn.q_proj.lora_A.default.weight', 'model.layers.38.cross_attn.q_proj.lora_B.default.weight', 'model.layers.38.cross_attn.v_proj.base_layer.weight', 'model.layers.38.cross_attn.v_proj.lora_A.default.weight', 'model.layers.38.cross_attn.v_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.cross_attn.q_proj.base_layer.weight', 'model.layers.8.cross_attn.q_proj.lora_A.default.weight', 'model.layers.8.cross_attn.q_proj.lora_B.default.weight', 'model.layers.8.cross_attn.v_proj.base_layer.weight', 'model.layers.8.cross_attn.v_proj.lora_A.default.weight', 'model.layers.8.cross_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']
- This IS expected if you are initializing MllamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MllamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MllamaForCausalLM were not initialized from the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.cross_attn.q_proj.weight', 'model.layers.13.cross_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.cross_attn.q_proj.weight', 'model.layers.18.cross_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.cross_attn.q_proj.weight', 'model.layers.23.cross_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.cross_attn.q_proj.weight', 'model.layers.28.cross_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.cross_attn.q_proj.weight', 'model.layers.3.cross_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.cross_attn.q_proj.weight', 'model.layers.33.cross_attn.v_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.cross_attn.q_proj.weight', 'model.layers.38.cross_attn.v_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.cross_attn.q_proj.weight', 'model.layers.8.cross_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from '/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant' to 'mylesgoose/Llama-3.2-11B-Vision-Instruct'. Please ensure that the correct base model is loaded when loading this checkpoint.
  warnings.warn(
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)  # noqa: B028
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00001-of-00009.safetensors
Loading weights for: base_model.model.model.embed_tokens.weight
Loading weights for: base_model.model.model.layers.0.input_layernorm.weight
Loading weights for: base_model.model.model.layers.0.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.0.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.0.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.input_layernorm.weight
Loading weights for: base_model.model.model.layers.1.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.1.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.1.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.input_layernorm.weight
Loading weights for: base_model.model.model.layers.2.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.2.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.2.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.3.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.3.input_layernorm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00002-of-00009.safetensors
Loading weights for: base_model.model.model.layers.3.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.3.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.input_layernorm.weight
Loading weights for: base_model.model.model.layers.4.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.4.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.input_layernorm.weight
Loading weights for: base_model.model.model.layers.5.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.5.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.5.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.input_layernorm.weight
Loading weights for: base_model.model.model.layers.6.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.6.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.6.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.input_layernorm.weight
Loading weights for: base_model.model.model.layers.7.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.7.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.7.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.8.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.8.input_layernorm.weight
Loading weights for: base_model.model.model.layers.8.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.8.mlp.up_proj.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00003-of-00009.safetensors
Loading weights for: base_model.model.model.layers.10.input_layernorm.weight
Loading weights for: base_model.model.model.layers.10.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.10.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.10.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.input_layernorm.weight
Loading weights for: base_model.model.model.layers.11.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.11.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.11.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.input_layernorm.weight
Loading weights for: base_model.model.model.layers.12.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.12.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.12.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.13.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.13.input_layernorm.weight
Loading weights for: base_model.model.model.layers.13.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.13.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.14.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.8.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.input_layernorm.weight
Loading weights for: base_model.model.model.layers.9.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.9.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00004-of-00009.safetensors
Loading weights for: base_model.model.model.layers.14.input_layernorm.weight
Loading weights for: base_model.model.model.layers.14.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.14.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.input_layernorm.weight
Loading weights for: base_model.model.model.layers.15.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.15.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.input_layernorm.weight
Loading weights for: base_model.model.model.layers.16.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.16.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.16.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.input_layernorm.weight
Loading weights for: base_model.model.model.layers.17.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.17.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.17.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.18.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.18.input_layernorm.weight
Loading weights for: base_model.model.model.layers.18.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.18.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.19.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00005-of-00009.safetensors
Loading weights for: base_model.model.model.layers.19.input_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.19.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.input_layernorm.weight
Loading weights for: base_model.model.model.layers.20.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.20.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.input_layernorm.weight
Loading weights for: base_model.model.model.layers.21.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.21.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.21.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.input_layernorm.weight
Loading weights for: base_model.model.model.layers.22.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.22.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.22.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.23.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.23.input_layernorm.weight
Loading weights for: base_model.model.model.layers.23.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.23.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.input_layernorm.weight
Loading weights for: base_model.model.model.layers.24.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.24.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00006-of-00009.safetensors
Loading weights for: base_model.model.model.layers.25.input_layernorm.weight
Loading weights for: base_model.model.model.layers.25.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.25.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.input_layernorm.weight
Loading weights for: base_model.model.model.layers.26.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.26.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.input_layernorm.weight
Loading weights for: base_model.model.model.layers.27.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.27.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.27.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.28.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.28.input_layernorm.weight
Loading weights for: base_model.model.model.layers.28.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.28.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.input_layernorm.weight
Loading weights for: base_model.model.model.layers.29.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.29.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.30.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00007-of-00009.safetensors
Loading weights for: base_model.model.model.layers.30.input_layernorm.weight
Loading weights for: base_model.model.model.layers.30.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.30.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.input_layernorm.weight
Loading weights for: base_model.model.model.layers.31.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.31.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.input_layernorm.weight
Loading weights for: base_model.model.model.layers.32.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.32.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.32.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.33.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.33.input_layernorm.weight
Loading weights for: base_model.model.model.layers.33.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.33.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.input_layernorm.weight
Loading weights for: base_model.model.model.layers.34.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.34.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.input_layernorm.weight
Loading weights for: base_model.model.model.layers.35.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.35.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.35.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00008-of-00009.safetensors
Loading weights for: base_model.model.model.layers.36.input_layernorm.weight
Loading weights for: base_model.model.model.layers.36.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.36.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.input_layernorm.weight
Loading weights for: base_model.model.model.layers.37.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.37.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.38.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.38.input_layernorm.weight
Loading weights for: base_model.model.model.layers.38.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.38.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.input_layernorm.weight
Loading weights for: base_model.model.model.layers.39.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.39.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.norm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00009-of-00009.safetensors
Loading weights for: base_model.model.lm_head.weight
Missing in checkpoint: vision_model.class_embedding
Missing in checkpoint: vision_model.patch_embedding.weight
Missing in checkpoint: vision_model.gated_positional_embedding.gate
Missing in checkpoint: vision_model.gated_positional_embedding.embedding
Missing in checkpoint: vision_model.gated_positional_embedding.tile_embedding.weight
Missing in checkpoint: vision_model.pre_tile_positional_embedding.gate
Missing in checkpoint: vision_model.pre_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.post_tile_positional_embedding.gate
Missing in checkpoint: vision_model.post_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.layernorm_pre.weight
Missing in checkpoint: vision_model.layernorm_pre.bias
Missing in checkpoint: vision_model.layernorm_post.weight
Missing in checkpoint: vision_model.layernorm_post.bias
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: language_model.model.embed_tokens.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.0.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.0.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.1.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.2.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.3.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.3.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.4.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.5.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.6.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.7.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.8.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.8.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.9.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.10.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.11.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.12.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.13.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.13.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.14.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.15.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.16.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.17.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.18.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.18.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.19.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.20.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.21.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.22.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.23.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.23.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.24.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.25.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.26.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.27.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.28.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.28.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.29.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.30.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.31.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.32.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.33.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.33.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.34.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.35.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.36.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.37.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.38.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.38.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.39.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.norm.weight
Missing in checkpoint: language_model.lm_head.weight
Missing in checkpoint: multi_modal_projector.weight
Missing in checkpoint: multi_modal_projector.bias
Extra in checkpoint: model.embed_tokens.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.k_proj.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.o_proj.weight
Extra in checkpoint: model.layers.0.mlp.gate_proj.weight
Extra in checkpoint: model.layers.0.mlp.up_proj.weight
Extra in checkpoint: model.layers.0.mlp.down_proj.weight
Extra in checkpoint: model.layers.0.input_layernorm.weight
Extra in checkpoint: model.layers.0.post_attention_layernorm.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.k_proj.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.o_proj.weight
Extra in checkpoint: model.layers.1.mlp.gate_proj.weight
Extra in checkpoint: model.layers.1.mlp.up_proj.weight
Extra in checkpoint: model.layers.1.mlp.down_proj.weight
Extra in checkpoint: model.layers.1.input_layernorm.weight
Extra in checkpoint: model.layers.1.post_attention_layernorm.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.k_proj.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.o_proj.weight
Extra in checkpoint: model.layers.2.mlp.gate_proj.weight
Extra in checkpoint: model.layers.2.mlp.up_proj.weight
Extra in checkpoint: model.layers.2.mlp.down_proj.weight
Extra in checkpoint: model.layers.2.input_layernorm.weight
Extra in checkpoint: model.layers.2.post_attention_layernorm.weight
Extra in checkpoint: model.layers.3.cross_attn_attn_gate
Extra in checkpoint: model.layers.3.cross_attn_mlp_gate
Extra in checkpoint: model.layers.3.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.3.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.3.input_layernorm.weight
Extra in checkpoint: model.layers.3.mlp.gate_proj.weight
Extra in checkpoint: model.layers.3.mlp.up_proj.weight
Extra in checkpoint: model.layers.3.mlp.down_proj.weight
Extra in checkpoint: model.layers.3.post_attention_layernorm.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.k_proj.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.o_proj.weight
Extra in checkpoint: model.layers.4.mlp.gate_proj.weight
Extra in checkpoint: model.layers.4.mlp.up_proj.weight
Extra in checkpoint: model.layers.4.mlp.down_proj.weight
Extra in checkpoint: model.layers.4.input_layernorm.weight
Extra in checkpoint: model.layers.4.post_attention_layernorm.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.k_proj.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.o_proj.weight
Extra in checkpoint: model.layers.5.mlp.gate_proj.weight
Extra in checkpoint: model.layers.5.mlp.up_proj.weight
Extra in checkpoint: model.layers.5.mlp.down_proj.weight
Extra in checkpoint: model.layers.5.input_layernorm.weight
Extra in checkpoint: model.layers.5.post_attention_layernorm.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.k_proj.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.o_proj.weight
Extra in checkpoint: model.layers.6.mlp.gate_proj.weight
Extra in checkpoint: model.layers.6.mlp.up_proj.weight
Extra in checkpoint: model.layers.6.mlp.down_proj.weight
Extra in checkpoint: model.layers.6.input_layernorm.weight
Extra in checkpoint: model.layers.6.post_attention_layernorm.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.k_proj.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.o_proj.weight
Extra in checkpoint: model.layers.7.mlp.gate_proj.weight
Extra in checkpoint: model.layers.7.mlp.up_proj.weight
Extra in checkpoint: model.layers.7.mlp.down_proj.weight
Extra in checkpoint: model.layers.7.input_layernorm.weight
Extra in checkpoint: model.layers.7.post_attention_layernorm.weight
Extra in checkpoint: model.layers.8.cross_attn_attn_gate
Extra in checkpoint: model.layers.8.cross_attn_mlp_gate
Extra in checkpoint: model.layers.8.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.8.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.8.input_layernorm.weight
Extra in checkpoint: model.layers.8.mlp.gate_proj.weight
Extra in checkpoint: model.layers.8.mlp.up_proj.weight
Extra in checkpoint: model.layers.8.mlp.down_proj.weight
Extra in checkpoint: model.layers.8.post_attention_layernorm.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.k_proj.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.o_proj.weight
Extra in checkpoint: model.layers.9.mlp.gate_proj.weight
Extra in checkpoint: model.layers.9.mlp.up_proj.weight
Extra in checkpoint: model.layers.9.mlp.down_proj.weight
Extra in checkpoint: model.layers.9.input_layernorm.weight
Extra in checkpoint: model.layers.9.post_attention_layernorm.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.k_proj.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.o_proj.weight
Extra in checkpoint: model.layers.10.mlp.gate_proj.weight
Extra in checkpoint: model.layers.10.mlp.up_proj.weight
Extra in checkpoint: model.layers.10.mlp.down_proj.weight
Extra in checkpoint: model.layers.10.input_layernorm.weight
Extra in checkpoint: model.layers.10.post_attention_layernorm.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.k_proj.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.o_proj.weight
Extra in checkpoint: model.layers.11.mlp.gate_proj.weight
Extra in checkpoint: model.layers.11.mlp.up_proj.weight
Extra in checkpoint: model.layers.11.mlp.down_proj.weight
Extra in checkpoint: model.layers.11.input_layernorm.weight
Extra in checkpoint: model.layers.11.post_attention_layernorm.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.k_proj.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.o_proj.weight
Extra in checkpoint: model.layers.12.mlp.gate_proj.weight
Extra in checkpoint: model.layers.12.mlp.up_proj.weight
Extra in checkpoint: model.layers.12.mlp.down_proj.weight
Extra in checkpoint: model.layers.12.input_layernorm.weight
Extra in checkpoint: model.layers.12.post_attention_layernorm.weight
Extra in checkpoint: model.layers.13.cross_attn_attn_gate
Extra in checkpoint: model.layers.13.cross_attn_mlp_gate
Extra in checkpoint: model.layers.13.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.13.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.13.input_layernorm.weight
Extra in checkpoint: model.layers.13.mlp.gate_proj.weight
Extra in checkpoint: model.layers.13.mlp.up_proj.weight
Extra in checkpoint: model.layers.13.mlp.down_proj.weight
Extra in checkpoint: model.layers.13.post_attention_layernorm.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.k_proj.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.o_proj.weight
Extra in checkpoint: model.layers.14.mlp.gate_proj.weight
Extra in checkpoint: model.layers.14.mlp.up_proj.weight
Extra in checkpoint: model.layers.14.mlp.down_proj.weight
Extra in checkpoint: model.layers.14.input_layernorm.weight
Extra in checkpoint: model.layers.14.post_attention_layernorm.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.k_proj.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.o_proj.weight
Extra in checkpoint: model.layers.15.mlp.gate_proj.weight
Extra in checkpoint: model.layers.15.mlp.up_proj.weight
Extra in checkpoint: model.layers.15.mlp.down_proj.weight
Extra in checkpoint: model.layers.15.input_layernorm.weight
Extra in checkpoint: model.layers.15.post_attention_layernorm.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.k_proj.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.o_proj.weight
Extra in checkpoint: model.layers.16.mlp.gate_proj.weight
Extra in checkpoint: model.layers.16.mlp.up_proj.weight
Extra in checkpoint: model.layers.16.mlp.down_proj.weight
Extra in checkpoint: model.layers.16.input_layernorm.weight
Extra in checkpoint: model.layers.16.post_attention_layernorm.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.k_proj.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.o_proj.weight
Extra in checkpoint: model.layers.17.mlp.gate_proj.weight
Extra in checkpoint: model.layers.17.mlp.up_proj.weight
Extra in checkpoint: model.layers.17.mlp.down_proj.weight
Extra in checkpoint: model.layers.17.input_layernorm.weight
Extra in checkpoint: model.layers.17.post_attention_layernorm.weight
Extra in checkpoint: model.layers.18.cross_attn_attn_gate
Extra in checkpoint: model.layers.18.cross_attn_mlp_gate
Extra in checkpoint: model.layers.18.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.18.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.18.input_layernorm.weight
Extra in checkpoint: model.layers.18.mlp.gate_proj.weight
Extra in checkpoint: model.layers.18.mlp.up_proj.weight
Extra in checkpoint: model.layers.18.mlp.down_proj.weight
Extra in checkpoint: model.layers.18.post_attention_layernorm.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.k_proj.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.o_proj.weight
Extra in checkpoint: model.layers.19.mlp.gate_proj.weight
Extra in checkpoint: model.layers.19.mlp.up_proj.weight
Extra in checkpoint: model.layers.19.mlp.down_proj.weight
Extra in checkpoint: model.layers.19.input_layernorm.weight
Extra in checkpoint: model.layers.19.post_attention_layernorm.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.k_proj.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.o_proj.weight
Extra in checkpoint: model.layers.20.mlp.gate_proj.weight
Extra in checkpoint: model.layers.20.mlp.up_proj.weight
Extra in checkpoint: model.layers.20.mlp.down_proj.weight
Extra in checkpoint: model.layers.20.input_layernorm.weight
Extra in checkpoint: model.layers.20.post_attention_layernorm.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.k_proj.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.o_proj.weight
Extra in checkpoint: model.layers.21.mlp.gate_proj.weight
Extra in checkpoint: model.layers.21.mlp.up_proj.weight
Extra in checkpoint: model.layers.21.mlp.down_proj.weight
Extra in checkpoint: model.layers.21.input_layernorm.weight
Extra in checkpoint: model.layers.21.post_attention_layernorm.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.k_proj.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.o_proj.weight
Extra in checkpoint: model.layers.22.mlp.gate_proj.weight
Extra in checkpoint: model.layers.22.mlp.up_proj.weight
Extra in checkpoint: model.layers.22.mlp.down_proj.weight
Extra in checkpoint: model.layers.22.input_layernorm.weight
Extra in checkpoint: model.layers.22.post_attention_layernorm.weight
Extra in checkpoint: model.layers.23.cross_attn_attn_gate
Extra in checkpoint: model.layers.23.cross_attn_mlp_gate
Extra in checkpoint: model.layers.23.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.23.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.23.input_layernorm.weight
Extra in checkpoint: model.layers.23.mlp.gate_proj.weight
Extra in checkpoint: model.layers.23.mlp.up_proj.weight
Extra in checkpoint: model.layers.23.mlp.down_proj.weight
Extra in checkpoint: model.layers.23.post_attention_layernorm.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.k_proj.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.o_proj.weight
Extra in checkpoint: model.layers.24.mlp.gate_proj.weight
Extra in checkpoint: model.layers.24.mlp.up_proj.weight
Extra in checkpoint: model.layers.24.mlp.down_proj.weight
Extra in checkpoint: model.layers.24.input_layernorm.weight
Extra in checkpoint: model.layers.24.post_attention_layernorm.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.k_proj.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.o_proj.weight
Extra in checkpoint: model.layers.25.mlp.gate_proj.weight
Extra in checkpoint: model.layers.25.mlp.up_proj.weight
Extra in checkpoint: model.layers.25.mlp.down_proj.weight
Extra in checkpoint: model.layers.25.input_layernorm.weight
Extra in checkpoint: model.layers.25.post_attention_layernorm.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.k_proj.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.o_proj.weight
Extra in checkpoint: model.layers.26.mlp.gate_proj.weight
Extra in checkpoint: model.layers.26.mlp.up_proj.weight
Extra in checkpoint: model.layers.26.mlp.down_proj.weight
Extra in checkpoint: model.layers.26.input_layernorm.weight
Extra in checkpoint: model.layers.26.post_attention_layernorm.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.k_proj.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.o_proj.weight
Extra in checkpoint: model.layers.27.mlp.gate_proj.weight
Extra in checkpoint: model.layers.27.mlp.up_proj.weight
Extra in checkpoint: model.layers.27.mlp.down_proj.weight
Extra in checkpoint: model.layers.27.input_layernorm.weight
Extra in checkpoint: model.layers.27.post_attention_layernorm.weight
Extra in checkpoint: model.layers.28.cross_attn_attn_gate
Extra in checkpoint: model.layers.28.cross_attn_mlp_gate
Extra in checkpoint: model.layers.28.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.28.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.28.input_layernorm.weight
Extra in checkpoint: model.layers.28.mlp.gate_proj.weight
Extra in checkpoint: model.layers.28.mlp.up_proj.weight
Extra in checkpoint: model.layers.28.mlp.down_proj.weight
Extra in checkpoint: model.layers.28.post_attention_layernorm.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.k_proj.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.o_proj.weight
Extra in checkpoint: model.layers.29.mlp.gate_proj.weight
Extra in checkpoint: model.layers.29.mlp.up_proj.weight
Extra in checkpoint: model.layers.29.mlp.down_proj.weight
Extra in checkpoint: model.layers.29.input_layernorm.weight
Extra in checkpoint: model.layers.29.post_attention_layernorm.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.k_proj.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.o_proj.weight
Extra in checkpoint: model.layers.30.mlp.gate_proj.weight
Extra in checkpoint: model.layers.30.mlp.up_proj.weight
Extra in checkpoint: model.layers.30.mlp.down_proj.weight
Extra in checkpoint: model.layers.30.input_layernorm.weight
Extra in checkpoint: model.layers.30.post_attention_layernorm.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.k_proj.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.o_proj.weight
Extra in checkpoint: model.layers.31.mlp.gate_proj.weight
Extra in checkpoint: model.layers.31.mlp.up_proj.weight
Extra in checkpoint: model.layers.31.mlp.down_proj.weight
Extra in checkpoint: model.layers.31.input_layernorm.weight
Extra in checkpoint: model.layers.31.post_attention_layernorm.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.k_proj.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.o_proj.weight
Extra in checkpoint: model.layers.32.mlp.gate_proj.weight
Extra in checkpoint: model.layers.32.mlp.up_proj.weight
Extra in checkpoint: model.layers.32.mlp.down_proj.weight
Extra in checkpoint: model.layers.32.input_layernorm.weight
Extra in checkpoint: model.layers.32.post_attention_layernorm.weight
Extra in checkpoint: model.layers.33.cross_attn_attn_gate
Extra in checkpoint: model.layers.33.cross_attn_mlp_gate
Extra in checkpoint: model.layers.33.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.33.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.33.input_layernorm.weight
Extra in checkpoint: model.layers.33.mlp.gate_proj.weight
Extra in checkpoint: model.layers.33.mlp.up_proj.weight
Extra in checkpoint: model.layers.33.mlp.down_proj.weight
Extra in checkpoint: model.layers.33.post_attention_layernorm.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.k_proj.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.o_proj.weight
Extra in checkpoint: model.layers.34.mlp.gate_proj.weight
Extra in checkpoint: model.layers.34.mlp.up_proj.weight
Extra in checkpoint: model.layers.34.mlp.down_proj.weight
Extra in checkpoint: model.layers.34.input_layernorm.weight
Extra in checkpoint: model.layers.34.post_attention_layernorm.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.k_proj.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.o_proj.weight
Extra in checkpoint: model.layers.35.mlp.gate_proj.weight
Extra in checkpoint: model.layers.35.mlp.up_proj.weight
Extra in checkpoint: model.layers.35.mlp.down_proj.weight
Extra in checkpoint: model.layers.35.input_layernorm.weight
Extra in checkpoint: model.layers.35.post_attention_layernorm.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.k_proj.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.o_proj.weight
Extra in checkpoint: model.layers.36.mlp.gate_proj.weight
Extra in checkpoint: model.layers.36.mlp.up_proj.weight
Extra in checkpoint: model.layers.36.mlp.down_proj.weight
Extra in checkpoint: model.layers.36.input_layernorm.weight
Extra in checkpoint: model.layers.36.post_attention_layernorm.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.k_proj.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.o_proj.weight
Extra in checkpoint: model.layers.37.mlp.gate_proj.weight
Extra in checkpoint: model.layers.37.mlp.up_proj.weight
Extra in checkpoint: model.layers.37.mlp.down_proj.weight
Extra in checkpoint: model.layers.37.input_layernorm.weight
Extra in checkpoint: model.layers.37.post_attention_layernorm.weight
Extra in checkpoint: model.layers.38.cross_attn_attn_gate
Extra in checkpoint: model.layers.38.cross_attn_mlp_gate
Extra in checkpoint: model.layers.38.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.38.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.38.input_layernorm.weight
Extra in checkpoint: model.layers.38.mlp.gate_proj.weight
Extra in checkpoint: model.layers.38.mlp.up_proj.weight
Extra in checkpoint: model.layers.38.mlp.down_proj.weight
Extra in checkpoint: model.layers.38.post_attention_layernorm.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.k_proj.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.o_proj.weight
Extra in checkpoint: model.layers.39.mlp.gate_proj.weight
Extra in checkpoint: model.layers.39.mlp.up_proj.weight
Extra in checkpoint: model.layers.39.mlp.down_proj.weight
Extra in checkpoint: model.layers.39.input_layernorm.weight
Extra in checkpoint: model.layers.39.post_attention_layernorm.weight
Extra in checkpoint: model.norm.weight
Extra in checkpoint: lm_head.weight
Some layers did not match.
LoRA model loaded successfully!
bbbbb
cuda:0
Length of the updated dataset: 560

system

Today Date: 17 Nov 2024

You are a helpful AI that can generate tikz code from images.
user

This is a picture of a scientific figure Generate LaTeX code that draws this scientific figure using TikZ. Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. Don't forget to include \usepackage{tikz}! I understand that this is a challenging task, so do your best. Return your result in a ```latex code block.
assistant

\usepackage{tikz}

\begin{document}

\begin{tikzpicture}[scale=1.5, every node/.style={font=\small}]

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black, very thin] (-4.5, -1) grid (4.5, 2);

\draw[step=1, black
########################################################
Error: Could not locate LaTeX code markers.
Processing sample 1/560 - Caption: The image depicts a segmented line representing a discretized interval from \( x = 0 \) to \( x = 1 \). The interval is divided into \( N \) segments, each of length \( \Delta x = 1/N \). The points along the line are labeled with their corresponding positions \( x = m \Delta x \) for \( m = 0, 1, 2, \ldots, N \). Below the line, two rows of indices are shown: the global index \( j \) and the interior index \( k \). The global index \( j \) ranges from 1 to \( N+1 \), while the interior index \( k \) ranges from 1 to \( N-1 \). The correspondence between the global and interior indices is indicated, with specific points highlighted such as \( x = 0 \) (global index \( j = 1 \)), \( x = \Delta x \) (global index \( j = 2 \), interior index \( k = 1 \)), and so on, up to \( x = 1 \) (global index \( j = N+1 \)).
TEX Edit Distance for sample 1: 0.7478510141372681
CrystalBLEU Score for sample 1: 0
Sample 1 processing time: 18.82 seconds

Processing sample 2/560 - Caption: This image depicts a directed graph with six nodes and various directed edges, including both solid and dashed lines. The nodes are labeled as follows: \( V_1 (H, a) \), \( V_2 (x) \), \( V_3 (\tilde{x}) \), \( V_4 (L) \), \( V_5 (L) \), and \( V_6 (G, \tilde{c}) \). The edges are as follows:

1. A solid edge from \( V_1 \) to \( V_2 \).
2. A solid edge from \( V_1 \) to \( V_3 \).
3. A solid edge from \( V_2 \) to \( V_4 \).
4. A solid edge from \( V_3 \) to \( V_4 \).
5. A solid edge from \( V_4 \) to \( V_6 \).
6. A solid edge from \( V_5 \) to \( V_6 \).
7. A dashed edge from \( V_1 \) to \( V_4 \).
8. A dashed edge from \( V_2 \) to \( V_5 \).
9. A dashed edge from \( V_3 \) to \( V_6 \).
10. A dashed edge from \( V_4 \) to \( V_5 \).

The nodes are arranged in a roughly horizontal layout, with \( V_1 \) on the left, \( V_6 \) on the right, and the other nodes positioned between them. The edges are directed and some are curved to avoid overlapping with other edges.
TEX Edit Distance for sample 2: 0.5881145000457764
CrystalBLEU Score for sample 2: 0.058356665725612926
Sample 2 processing time: 46.98 seconds

Processing sample 3/560 - Caption: This image is a line plot with three datasets: Multi-News, SamSUM, and CNN/DM. The x-axis represents the dataset size on a logarithmic scale, ranging from \(10^3\) to \(10^5\). The y-axis represents the average value, ranging from 0.5 to 0.7. The plot includes three lines: a blue line with square markers for Multi-News, a red line with triangle markers for SamSUM, and a green line with star markers for CNN/DM. The legend is located inside the plot area, towards the right side. The grid lines are visible for better readability.
TEX Edit Distance for sample 3: 0.8242249488830566
CrystalBLEU Score for sample 3: 0.02524487617357437
Sample 3 processing time: 56.63 seconds

Processing sample 4/560 - Caption: The image shows a graphical representation of the operation on two structures, Σ(z₁) and Σ(z₂), resulting in a combined structure Σ(z₁) ⊕ Σ(z₁). On the left side, there are two separate structures, each consisting of a circle connected to a smaller circle above it, labeled Σ(z₁) and Σ(z₂) respectively. These structures are combined using the ⊕ operator. On the right side, the resulting structure is shown, which is a larger structure with three levels of circles. The bottom level has two circles, each connected to a smaller circle above it, and these two smaller circles are connected to a single circle at the top. The label Σ(z₁) ⊕ Σ(z₁) is placed below the combined structure.
TEX Edit Distance for sample 4: 0.8098483085632324
CrystalBLEU Score for sample 4: 0.005100647013610552
Sample 4 processing time: 56.60 seconds

Processing sample 5/560 - Caption: The image shows a two-panel plot comparing simulated and theoretical coalescence times under weak and strong selection. The left panel is titled "Coalescence times" and is divided into two regions: weak selection on the left and strong selection on the right. The x-axis is labeled with the selection coefficient (α), ranging from 0 to 1, while the y-axis is labeled with the expected coalescence time (E[T2]) ranging from 0 to 15. The black dots represent simulated data (N = 10^3), and the green curve represents the theoretical values. The right panel, partially visible, seems to follow a similar structure but with different y-axis values.
TEX Edit Distance for sample 5: 0.675757646560669
CrystalBLEU Score for sample 5: 0.00034318344760506634
Sample 5 processing time: 67.78 seconds

Processing sample 6/560 - Caption: This image consists of two Hasse diagrams side by side. The diagram on the left, labeled \( P \), is a diamond-shaped poset with four elements: \( a \) at the bottom, \( b \) and \( c \) in the middle, and \( d \) at the top. The elements are connected by lines indicating the partial order: \( a \leq b \), \( a \leq c \), \( b \leq d \), and \( c \leq d \).

The diagram on the right, labeled \( \text{Int} \, P \), represents the interval poset of \( P \). It has nine elements, each representing an interval in \( P \): \([a,a]\), \([a,b]\), \([a,c]\), \([b,b]\), \([b,d]\), \([c,c]\), \([c,d]\), \([d,d]\), and \([a,d]\). These intervals are connected by lines indicating the partial order among them. The structure forms a more complex lattice with multiple levels, reflecting the intervals' relationships.
TEX Edit Distance for sample 6: 0.6770069003105164
CrystalBLEU Score for sample 6: 7.70481786362922e-06
Sample 6 processing time: 22.86 seconds

Processing sample 7/560 - Caption: The figure is a line plot showing the impact of batch size on inference speedups. The x-axis represents the batch size, ranging from 1 to 16, while the y-axis represents the speedup, ranging from 1 to 5. Multiple lines, each with different colors and markers, represent different configurations or models. A legend on the top right corner identifies these configurations. A vertical dashed line at batch size 8 highlights a specific point of interest.
TEX Edit Distance for sample 7: 0.7904283404350281
CrystalBLEU Score for sample 7: 6.425721026711554e-06
Sample 7 processing time: 56.00 seconds

Processing sample 8/560 - Caption: The image illustrates a sequence of transformations of intersecting lines. 

1. The top part shows a set of horizontal and vertical lines labeled with numbers. The horizontal lines are labeled "n", "n-2", "n-4", ..., "1" from top to bottom, and the vertical lines are labeled "1".
2. An arrow points downward to the middle part, where the lines are rotated to form an "X" pattern. The lines are labeled similarly, with horizontal lines labeled "1" and "n-2", and vertical lines labeled "1" and "n-6".
3. Another arrow points downward to the bottom part, where the lines are further transformed into a symmetric "X" pattern. The lines are labeled "1" and "1", with the horizontal lines in black and the diagonal lines in red.

The image can be described in TikZ with the following elements:
- Horizontal and vertical lines with labels.
- Rotated lines forming an "X" pattern.
- Symmetric "X" pattern with different colors for the lines.
- Arrows indicating the transformation steps.
TEX Edit Distance for sample 8: 0.8341864943504333
CrystalBLEU Score for sample 8: 3.09087742457899e-06
Sample 8 processing time: 56.16 seconds

Processing sample 9/560 - Caption: This image depicts a triangular plot contained within a rectangular frame. The plot consists of two diagonal lines forming an inverted "V" shape, with tick marks and labels along both lines. The labels are positioned at regular intervals along the lines, indicating specific values. The entire plot is enclosed within a smaller rectangle, which is centered within a larger rectangular border. The larger rectangle appears to be the boundary of the entire figure, while the smaller rectangle serves as the frame for the triangular plot.
TEX Edit Distance for sample 9: 0.6769669651985168
CrystalBLEU Score for sample 9: 3.1036314900536804e-05
Sample 9 processing time: 31.83 seconds

Processing sample 10/560 - Caption: The image depicts a geometric representation of complex numbers. It shows a series of ellipses increasing in size along the positive real axis, labeled as \(|z|^2\). The ellipses are aligned along a line starting from the origin (0), which is marked by a solid black dot. The x-axis is labeled with \(|z|^2\) to indicate the squared magnitude of the complex number \(z\). The ellipses are positioned at regular intervals along this axis, with their major axes increasing linearly.
TEX Edit Distance for sample 10: 0.6987464427947998
CrystalBLEU Score for sample 10: 3.0192987548262497e-05
Sample 10 processing time: 9.13 seconds

Processing sample 11/560 - Caption: This image is a line plot depicting the relationship between sentence length and two performance metrics: "Matched Words" and "Exact Sentences." The x-axis represents sentence length, ranging from 0 to 60, while the y-axis represents the proportion, ranging from 0 to 1. Two lines are plotted: a blue line for "Matched Words" and a red line for "Exact Sentences." The blue line starts near the top of the y-axis and remains relatively high, showing a slight downward trend as sentence length increases. The red line starts high but decreases sharply as sentence length increases, approaching zero around a sentence length of 50. A legend in the bottom left corner identifies the blue and red lines. The plot has a title "Performance Metrics vs. Sentence Length" and includes grid lines for better readability.
TEX Edit Distance for sample 11: 0.5488850474357605
CrystalBLEU Score for sample 11: 0.00011032733120849805
Sample 11 processing time: 39.40 seconds

Processing sample 12/560 - Caption: The image consists of three distinct diagrams, each depicting a different colored path with labeled points and segments. 

1. The leftmost diagram is in red and shows a path with points labeled \(a\) and \(b\). The path has a horizontal dashed line segment labeled \(H\) connecting the points. The path starts from a point labeled \(R\) and ends at \(b\).

2. The middle diagram is in black and shows a similar path with points labeled \(a\) and \(b\). The path has a horizontal dashed line segment labeled \(\bar{H}\) connecting the points. The path starts from a point labeled \(\bar{R}\) and ends at \(b\).

3. The rightmost diagram is in blue and shows a path with points labeled \(t(a)\) and \(t(b)\). The path starts from a point labeled \(B'\) and ends at \(t(a)\).

Each diagram features a combination of straight and angled lines, with specific points marked by colored dots. The dashed lines in each diagram indicate a horizontal connection between the points \(a\) and \(b\).
TEX Edit Distance for sample 12: 0.461331844329834
CrystalBLEU Score for sample 12: 0.0003115689623344845
Sample 12 processing time: 22.98 seconds

Processing sample 13/560 - Caption: This image contains a series of graph transformations labeled \( F_1(n) \) through \( F_{11}(n) \). Each transformation shows a different configuration of nodes and edges, with some nodes labeled with letters (e.g., X, Y) and others with numbers (e.g., 1, 2, 3). The transformations illustrate the progression of the graph structure through various stages:

1. \( F_1(n) \) shows a triangle with nodes 1, 2, and 3 connected in a cycle, and a separate node X connected to node 1.
2. \( F_2(n) \) shows node X connected to a triangle formed by nodes 1, 2, and 3.
3. \( F_3(n) \) shows node X connected to a cycle of three nodes labeled 1, 2, and 3.
4. \( F_4(n) \) shows node X connected to a path of three nodes labeled 1, 2, and 3.
5. \( F_5(n) \) shows node X connected to a path of four nodes labeled 1, 2, 3, and 4.
6. \( F_6(n) \) shows node X connected to a path of five nodes labeled 1, 2, 3, 4, and 5.
7. \( F_7(n) \) shows node X connected to a path of six nodes labeled 1, 2, 3, 4, 5, and 6.
8. \( F_8(n) \) shows node X connected to a path of seven nodes labeled 1, 2, 3, 4, 5, 6, and 7.
9. \( F_9(n) \) shows node X connected to a path of eight nodes labeled 1, 2, 3, 4, 5, 6, 7, and 8.
10. \( F_{10}(n) \) shows node X connected to a path of nine nodes labeled 1, 2, 3, 4, 5, 6, 7, 8, and 9.
11. \( F_{11}(n) \) shows node X connected to a path of ten nodes labeled 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10.

Each graph transformation is enclosed in an oval or circle, indicating the scope of the transformation. The nodes and edges are clearly labeled to show the progression from one stage to the next.
TEX Edit Distance for sample 13: 0.6449928879737854
CrystalBLEU Score for sample 13: 0.00034782088106098474
Sample 13 processing time: 36.80 seconds

Processing sample 14/560 - Caption: The image is a bar chart displaying the number of students on the y-axis and a range of scores on the x-axis. The x-axis is labeled with score intervals, each represented by a blue bar. Two specific score intervals are highlighted with different colors: one in red and one in green. The chart includes a legend at the bottom indicating that the green bar represents "ChatGPT," the red bar represents "BingChat," and the blue bars represent "Estudiantes evaluados." The y-axis is labeled with a logarithmic scale (10^0 to 10^4), and each bar has a numerical value displayed at its top.
TEX Edit Distance for sample 14: 0.8350451588630676
CrystalBLEU Score for sample 14: 0.00022484914574251265
Sample 14 processing time: 56.13 seconds

Processing sample 15/560 - Caption: The image depicts a graph with 10 vertices arranged in a decagon. Each vertex on the decagon is connected to its adjacent vertices with black edges. Additionally, there is a central vertex connected to all vertices of the decagon with red edges. The vertices are represented by black-filled circles.

This description should help you write the TikZ code for the figure.
TEX Edit Distance for sample 15: 0.6832534074783325
CrystalBLEU Score for sample 15: 0.0004174255270791975
Sample 15 processing time: 37.57 seconds

Processing sample 16/560 - Caption: This image depicts a complex plane diagram with two semicircles centered at the origin. The horizontal axis represents the real part of the complex plane, marked with a "0" at the origin. There are two semicircles with different radii, both centered at the origin and extending upwards. The smaller semicircle is labeled with "it_n" at its highest point, and there are two dots vertically aligned along the imaginary axis, one at the origin and the other at the label "it_n".
TEX Edit Distance for sample 16: 0.6016625165939331
CrystalBLEU Score for sample 16: 0.0008537939695769841
Sample 16 processing time: 21.02 seconds

Processing sample 17/560 - Caption: This image depicts a sequence of sets \( V_1, V_2, \ldots, V_{\beta+1} \) represented by ellipses. Each set contains points labeled \( v, x_\beta, y_\beta, y_1, x_1, u \). There are directed arrows between these points indicating transitions or connections. A red path labeled \( P_1 \) and a blue dashed path labeled \( P_2 \) connect these points across the sets. The red path \( P_1 \) seems to form a continuous connection through the points \( v, x_\beta, y_\beta, y_1, x_1, u \), while the blue dashed path \( P_2 \) forms a similar connection but with a different trajectory. An arrow pointing to the right indicates progression from \( V_1 \) to \( V_{\beta+1} \).
TEX Edit Distance for sample 17: 0.642218828201294
CrystalBLEU Score for sample 17: 0.0014181598530150412
Sample 17 processing time: 43.73 seconds

Processing sample 18/560 - Caption: This image depicts a rectangular region in the \(u\)-\(T\) plane, labeled with boundaries and specific points. The vertical axis is labeled \(u\) and the horizontal axis is labeled \(T\). The rectangle is defined by the points \((1, \epsilon)\), \((T_0, \epsilon)\), \((T_0, A)\), and \((1, A)\). The boundaries of the rectangle are labeled as \(\Gamma_1\), \(\Gamma_2\), \(\Gamma_3\), and \(\Gamma_4\), with arrows indicating the direction along each boundary. The point \((1, \epsilon)\) is marked with dashed lines extending to the axes. The interior of the rectangle is labeled with \(u\).
TEX Edit Distance for sample 18: 0.48252278566360474
CrystalBLEU Score for sample 18: 0.0022105287981235644
Sample 18 processing time: 37.89 seconds

Processing sample 19/560 - Caption: The image consists of two parts. On the left, there is a diagram with vertical lines labeled "B" on both sides, and between them, there are horizontal lines labeled with numbers 3, 2, and dots representing "n+1". On the right, there is a graph with nodes labeled 1, 2, and 3. The central node labeled 3 is connected to nodes labeled 2, which are further connected to nodes labeled 1. The central node 3 is also connected to another node labeled 3, which is connected to nodes labeled 2 and 1, forming a symmetrical structure. An arrow points from the left diagram to the right graph, indicating a transformation from the left structure to the right structure.
TEX Edit Distance for sample 19: 0.677211344242096
CrystalBLEU Score for sample 19: 0.001993561244863773
Sample 19 processing time: 15.12 seconds

Processing sample 20/560 - Caption: This image depicts the architecture of a Gated Recurrent Unit (GRU) cell. The main components and their connections are as follows:

1. **Inputs and Outputs:**
   - Input \( x(t) \) enters from the bottom.
   - Previous hidden state \( h(t-1) \) enters from the left.
   - Current hidden state \( h(t) \) exits to the right.
   - Current output \( y(t) \) exits from the top.

2. **Components:**
   - Two fully connected (FC) layers, represented by blue rectangles.
   - Update gate \( z(t) \) and reset gate \( r(t) \).
   - Element-wise multiplication, represented by circles with a cross (×).
   - Element-wise addition, represented by circles with a plus (+).
   - Sigmoid activation functions, represented by the sigmoid curve symbol.

3. **Connections:**
   - \( x(t) \) and \( h(t-1) \) are inputs to both FC layers.
   - Outputs of the FC layers are used to compute \( r(t) \) and \( z(t) \).
   - \( r(t) \) is used to modulate \( h(t-1) \) before passing it to the next FC layer.
   - The result of the FC layer modulated by \( r(t) \) is combined with \( x(t) \) to compute the candidate hidden state.
   - The final hidden state \( h(t) \) is computed using a combination of the candidate hidden state and \( h(t-1) \), modulated by \( z(t) \).

This description should help in writing the TikZ code to accurately represent the GRU cell structure.
TEX Edit Distance for sample 20: 0.5999553799629211
CrystalBLEU Score for sample 20: 0.002601788300302456
Sample 20 processing time: 40.20 seconds

Processing sample 21/560 - Caption: This image depicts a 3D geometric shape resembling a crystal or diamond. The shape is composed of multiple triangular and quadrilateral faces. The edges of the shape are outlined in red, while the faces are filled with a light blue color. The overall structure is symmetrical along its vertical axis, with the top and bottom parts tapering to a point. The central section of the shape has a more complex arrangement of faces, creating a faceted appearance.
TEX Edit Distance for sample 21: 0.8196677565574646
CrystalBLEU Score for sample 21: 0.0022360582574145966
Sample 21 processing time: 56.07 seconds

Processing sample 22/560 - Caption: This image depicts a set of rays emanating from the origin in a coordinate plane. The rays are labeled with various mathematical expressions involving the function \( D \). The labels are positioned at the ends of the rays. The rays are distributed in the first and fourth quadrants, with some rays being solid lines and others being dotted lines. The axes are labeled with \( D(1) \) on the positive y-axis and \( D(2) \) on the positive x-axis. Additional labels include \( D\left(\frac{1}{2}\right) \), \( D\left(\frac{1}{22}\right) \), \( D(\tau^{-1}(2)) \), \( D\left(\frac{1}{2}(d, \lambda)\right) \), and \( D\left(\frac{11}{2}\right) \).

To create this figure using TikZ, you would need to:
1. Draw the x-axis and y-axis.
2. Draw multiple rays emanating from the origin at different angles.
3. Label the ends of the rays with the specified mathematical expressions.
4. Use solid and dotted lines for different rays as shown in the image.
TEX Edit Distance for sample 22: 0.8150118589401245
CrystalBLEU Score for sample 22: 0.002077172153463242
Sample 22 processing time: 56.07 seconds

Processing sample 23/560 - Caption: This image appears to be a grid of 4x4 spheres, each labeled with various variables and symbols. The spheres are color-coded and contain different mathematical notations. The first column contains spheres labeled with variables \(x\), \(y\), and \(z\) in red, blue, and cyan respectively. The second to fourth columns contain spheres with overlapping colored regions and additional labels such as \(a\), \(b\), \(g\), and \(f\). There are arrows pointing from the first column to the second column, indicating a transformation or mapping, with labels \(O_{0000}\) and \(O_{000}\). The symbols \( \subseteq \) are used between the columns to indicate inclusion or subset relationships. The entire grid is annotated with a number (1) on the right side.

To write the TikZ code for this figure, you would need to create a 4x4 grid of nodes, draw spheres at each node, label them appropriately, and use arrows and subset symbols to indicate the relationships between the spheres. The color coding and overlapping regions would also need to be replicated.
TEX Edit Distance for sample 23: 0.6601191163063049
CrystalBLEU Score for sample 23: 0.0021039392113056886
Sample 23 processing time: 32.77 seconds

Processing sample 24/560 - Caption: The figure illustrates a positive consumption externality in the energy-efficient housing market. The x-axis represents the quantity of energy-efficient housing, while the y-axis represents the price, costs, and benefits in yen. The graph includes the following curves:

1. **MPB (Marginal Private Benefit)**: A downward-sloping red line.
2. **MSB (Marginal Social Benefit)**: A downward-sloping blue line, above the MPB curve.
3. **MPC (Marginal Private Cost) = MSC (Marginal Social Cost)**: An upward-sloping blue line.
4. **MEB (Marginal External Benefit)**: A vertical distance between the MPB and MSB curves, marked with an orange double arrow.

The equilibrium without externality (E1) is at the intersection of the MPB and MPC curves, with a corresponding quantity Qm and price Pm. The socially optimal equilibrium (Es) is at the intersection of the MSB and MSC curves, with a corresponding quantity Qopt and price Psot.

The welfare loss, represented by a green triangle, is the area between the MPB and MSB curves from Qm to Qopt.
TEX Edit Distance for sample 24: 0.7885576486587524
CrystalBLEU Score for sample 24: 0.0018856005489927697
Sample 24 processing time: 56.28 seconds

Processing sample 25/560 - Caption: This image depicts a geometric configuration involving three vertical lines at positions \(x_{k-1}\), \(x_k\), and \(x_{k+1}\) labeled \(C_{k-1}\), \(C_k\), and \(C_{k+1}\) respectively. The line at \(x_k\) has several angles and vectors emanating from it. The vectors \(U_{\Gamma}^{(\tau)}\), \(U_{\Gamma}\), \(U_I\), and \(U_B\) are shown, with \(U_{\Gamma}^{(\tau)}\) and \(U_{\Gamma}\) drawn in blue. The angles \(\alpha_1\) and \(\beta_1\) are marked between these vectors. The top of the lines \(C_{k-1}\) and \(C_k\) are connected by a slanted line, and the top of \(C_k\) and \(C_{k+1}\) are connected similarly. The lines \(C_{k-1}\) and \(C_{k+1}\) have hatching patterns on the top sections.
TEX Edit Distance for sample 25: 0.8433152437210083
CrystalBLEU Score for sample 25: 0.0017007220553888333
Sample 25 processing time: 56.82 seconds

Processing sample 26/560 - Caption: This image depicts a decision tree diagram used to model the process of claim settlement and payment. The diagram is divided into two main sections: "Settlement" and "Payment." 

1. The "Settlement" section has a single decision node labeled "1. settlement."
2. The "Payment" section is further divided into two branches based on the outcome of the settlement:
   - If "Yes," there are two decision nodes labeled "2. payment" and "3. pct_paid."
   - If "No," there are two decision nodes labeled "2. payment" and "3. increase_paid."

The diagram includes labels for "Initial claim characteristics" and "Updates" on the left side, indicating the factors influencing the decision process. The structure is organized in a tabular format with clear horizontal and vertical lines separating the different sections and decision nodes.

This description can help in writing the TikZ code by providing a clear understanding of the hierarchical structure and labeling of the decision nodes.
TEX Edit Distance for sample 26: 0.5961518883705139
CrystalBLEU Score for sample 26: 0.002174104281165738
Sample 26 processing time: 30.64 seconds

Processing sample 27/560 - Caption: This image depicts a coordinate system with the x1-axis and x2-axis, ranging from -2 to 2 on both axes. The background is divided into two regions: the left half is shaded in red and the right half in green. The x1-axis is labeled with points a(t_min) at -1 and a(t_max) at 1. There is a semicircular arc labeled Γ centered at the origin, spanning from a(t_min) to a(t_max). The axes are labeled with x1 and x2, and there are tick marks at each integer value on both axes.
TEX Edit Distance for sample 27: 0.5959130525588989
CrystalBLEU Score for sample 27: 0.0021777726236703507
Sample 27 processing time: 25.97 seconds

Processing sample 28/560 - Caption: This image is a plot of two probability density functions (PDFs) over the same range of values for "Feature Y" on the x-axis and "Count" on the y-axis. The first PDF is represented by a blue dashed line, peaking around 0, and the second PDF is represented by a red solid line, peaking around 1. The x-axis ranges from -2 to 3, and the y-axis ranges from 0 to 0.8. The plot includes axis labels for both the x-axis ("Feature Y") and the y-axis ("Count").
TEX Edit Distance for sample 28: 0.6501487493515015
CrystalBLEU Score for sample 28: 0.0023202075304401474
Sample 28 processing time: 8.80 seconds

Processing sample 29/560 - Caption: The image depicts a horizontal number line centered at 0, extending from -2 to 2. The number line is labeled with tick marks at intervals of 0.5 units. The x-axis is denoted by \( x \) at the far right end. The tick marks are labeled with the corresponding values: -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, and 2. The arrow at the right end of the number line indicates the positive direction of the x-axis.
TEX Edit Distance for sample 29: 0.5645161271095276
CrystalBLEU Score for sample 29: 0.0026748796446615813
Sample 29 processing time: 31.90 seconds

Processing sample 30/560 - Caption: This image depicts a block diagram with five rectangular blocks and labeled arrows. Four blocks are aligned vertically on the left side, each labeled with \( w(t-2) \), \( w(t-1) \), \( w(t+1) \), and \( w(t+2) \). Each of these blocks has an arrow pointing to a central block labeled "SUM". The central "SUM" block has an arrow pointing to a fifth block on the right side, labeled \( w(t) \). This diagram represents a summation process where the inputs from the four left blocks are combined in the SUM block to produce an output in the right block.
TEX Edit Distance for sample 30: 0.5654317736625671
CrystalBLEU Score for sample 30: 0.003053958906996085
Sample 30 processing time: 17.05 seconds

Processing sample 31/560 - Caption: This scatter plot compares the probability bounds \( p \) obtained from a Program-Agnostic Neural ISM (x-axis) to those obtained from Farkas' Lemma (y-axis). The x-axis and y-axis are both on a logarithmic scale ranging from \( 10^{-3} \) to \( 10^0 \). The plot includes a dashed line representing \( y = x \) for reference. Data points are marked with red circles and red crosses, where circles indicate cases where Farkas' Lemma fails and crosses indicate cases where Farkas' Lemma succeeds. A legend in the plot explains the symbols used.
TEX Edit Distance for sample 31: 0.646028995513916
CrystalBLEU Score for sample 31: 0.0031942072810344883
Sample 31 processing time: 28.75 seconds

Processing sample 32/560 - Caption: The image shows a circular diagram with two concentric circles. The outer circle is divided into 12 equal segments, each labeled with \(B_1\) to \(B_{12}\). The inner circle is divided into 8 equal segments, each labeled with \(B_1\) to \(B_8\). The segments are separated by radial lines extending from the center of the circles to their peripheries. The labels are placed near the outer edge of each segment.
slurmstepd: error: *** JOB 9451179 ON cn267 CANCELLED AT 2024-11-17T14:56:11 ***
