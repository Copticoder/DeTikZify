Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:02<00:09,  2.29s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:04<00:06,  2.04s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:06<00:03,  1.95s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:07<00:01,  1.93s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.70s/it]
WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.
Traceback (most recent call last):
  File "/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/omar.py", line 20, in <module>
    full_model = MllamaForConditionalGeneration.from_pretrained(
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3142, in to
    raise ValueError(
ValueError: `.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
