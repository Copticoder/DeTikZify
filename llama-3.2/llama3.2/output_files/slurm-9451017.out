Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:06<00:27,  6.82s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:06<00:03,  1.83s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:07<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.43s/it]
Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:21<02:49, 21.24s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:41<02:25, 20.78s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [01:02<02:04, 20.67s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [01:22<01:43, 20.61s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [01:42<01:21, 20.36s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [02:03<01:01, 20.52s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [02:24<00:41, 20.54s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [02:38<00:18, 18.68s/it]Loading checkpoint shards: 100%|██████████| 9/9 [02:48<00:00, 15.85s/it]Loading checkpoint shards: 100%|██████████| 9/9 [02:48<00:00, 18.71s/it]
Some weights of the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant were not used when initializing MllamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.cross_attn.q_proj.base_layer.weight', 'model.layers.13.cross_attn.q_proj.lora_A.default.weight', 'model.layers.13.cross_attn.q_proj.lora_B.default.weight', 'model.layers.13.cross_attn.v_proj.base_layer.weight', 'model.layers.13.cross_attn.v_proj.lora_A.default.weight', 'model.layers.13.cross_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.cross_attn.q_proj.base_layer.weight', 'model.layers.18.cross_attn.q_proj.lora_A.default.weight', 'model.layers.18.cross_attn.q_proj.lora_B.default.weight', 'model.layers.18.cross_attn.v_proj.base_layer.weight', 'model.layers.18.cross_attn.v_proj.lora_A.default.weight', 'model.layers.18.cross_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.cross_attn.q_proj.base_layer.weight', 'model.layers.23.cross_attn.q_proj.lora_A.default.weight', 'model.layers.23.cross_attn.q_proj.lora_B.default.weight', 'model.layers.23.cross_attn.v_proj.base_layer.weight', 'model.layers.23.cross_attn.v_proj.lora_A.default.weight', 'model.layers.23.cross_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.cross_attn.q_proj.base_layer.weight', 'model.layers.28.cross_attn.q_proj.lora_A.default.weight', 'model.layers.28.cross_attn.q_proj.lora_B.default.weight', 'model.layers.28.cross_attn.v_proj.base_layer.weight', 'model.layers.28.cross_attn.v_proj.lora_A.default.weight', 'model.layers.28.cross_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.cross_attn.q_proj.base_layer.weight', 'model.layers.3.cross_attn.q_proj.lora_A.default.weight', 'model.layers.3.cross_attn.q_proj.lora_B.default.weight', 'model.layers.3.cross_attn.v_proj.base_layer.weight', 'model.layers.3.cross_attn.v_proj.lora_A.default.weight', 'model.layers.3.cross_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.cross_attn.q_proj.base_layer.weight', 'model.layers.33.cross_attn.q_proj.lora_A.default.weight', 'model.layers.33.cross_attn.q_proj.lora_B.default.weight', 'model.layers.33.cross_attn.v_proj.base_layer.weight', 'model.layers.33.cross_attn.v_proj.lora_A.default.weight', 'model.layers.33.cross_attn.v_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.cross_attn.q_proj.base_layer.weight', 'model.layers.38.cross_attn.q_proj.lora_A.default.weight', 'model.layers.38.cross_attn.q_proj.lora_B.default.weight', 'model.layers.38.cross_attn.v_proj.base_layer.weight', 'model.layers.38.cross_attn.v_proj.lora_A.default.weight', 'model.layers.38.cross_attn.v_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.cross_attn.q_proj.base_layer.weight', 'model.layers.8.cross_attn.q_proj.lora_A.default.weight', 'model.layers.8.cross_attn.q_proj.lora_B.default.weight', 'model.layers.8.cross_attn.v_proj.base_layer.weight', 'model.layers.8.cross_attn.v_proj.lora_A.default.weight', 'model.layers.8.cross_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']
- This IS expected if you are initializing MllamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MllamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MllamaForCausalLM were not initialized from the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.cross_attn.q_proj.weight', 'model.layers.13.cross_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.cross_attn.q_proj.weight', 'model.layers.18.cross_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.cross_attn.q_proj.weight', 'model.layers.23.cross_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.cross_attn.q_proj.weight', 'model.layers.28.cross_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.cross_attn.q_proj.weight', 'model.layers.3.cross_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.cross_attn.q_proj.weight', 'model.layers.33.cross_attn.v_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.cross_attn.q_proj.weight', 'model.layers.38.cross_attn.v_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.cross_attn.q_proj.weight', 'model.layers.8.cross_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from '/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant' to 'mylesgoose/Llama-3.2-11B-Vision-Instruct'. Please ensure that the correct base model is loaded when loading this checkpoint.
  warnings.warn(
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)  # noqa: B028
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00001-of-00009.safetensors
Loading weights for: base_model.model.model.embed_tokens.weight
Loading weights for: base_model.model.model.layers.0.input_layernorm.weight
Loading weights for: base_model.model.model.layers.0.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.0.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.0.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.input_layernorm.weight
Loading weights for: base_model.model.model.layers.1.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.1.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.1.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.input_layernorm.weight
Loading weights for: base_model.model.model.layers.2.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.2.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.2.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.3.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.3.input_layernorm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00002-of-00009.safetensors
Loading weights for: base_model.model.model.layers.3.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.3.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.input_layernorm.weight
Loading weights for: base_model.model.model.layers.4.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.4.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.input_layernorm.weight
Loading weights for: base_model.model.model.layers.5.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.5.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.5.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.input_layernorm.weight
Loading weights for: base_model.model.model.layers.6.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.6.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.6.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.input_layernorm.weight
Loading weights for: base_model.model.model.layers.7.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.7.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.7.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.8.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.8.input_layernorm.weight
Loading weights for: base_model.model.model.layers.8.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.8.mlp.up_proj.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00003-of-00009.safetensors
Loading weights for: base_model.model.model.layers.10.input_layernorm.weight
Loading weights for: base_model.model.model.layers.10.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.10.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.10.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.input_layernorm.weight
Loading weights for: base_model.model.model.layers.11.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.11.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.11.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.input_layernorm.weight
Loading weights for: base_model.model.model.layers.12.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.12.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.12.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.13.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.13.input_layernorm.weight
Loading weights for: base_model.model.model.layers.13.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.13.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.14.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.8.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.input_layernorm.weight
Loading weights for: base_model.model.model.layers.9.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.9.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00004-of-00009.safetensors
Loading weights for: base_model.model.model.layers.14.input_layernorm.weight
Loading weights for: base_model.model.model.layers.14.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.14.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.input_layernorm.weight
Loading weights for: base_model.model.model.layers.15.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.15.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.input_layernorm.weight
Loading weights for: base_model.model.model.layers.16.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.16.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.16.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.input_layernorm.weight
Loading weights for: base_model.model.model.layers.17.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.17.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.17.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.18.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.18.input_layernorm.weight
Loading weights for: base_model.model.model.layers.18.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.18.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.19.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00005-of-00009.safetensors
Loading weights for: base_model.model.model.layers.19.input_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.19.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.input_layernorm.weight
Loading weights for: base_model.model.model.layers.20.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.20.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.input_layernorm.weight
Loading weights for: base_model.model.model.layers.21.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.21.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.21.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.input_layernorm.weight
Loading weights for: base_model.model.model.layers.22.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.22.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.22.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.23.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.23.input_layernorm.weight
Loading weights for: base_model.model.model.layers.23.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.23.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.input_layernorm.weight
Loading weights for: base_model.model.model.layers.24.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.24.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00006-of-00009.safetensors
Loading weights for: base_model.model.model.layers.25.input_layernorm.weight
Loading weights for: base_model.model.model.layers.25.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.25.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.input_layernorm.weight
Loading weights for: base_model.model.model.layers.26.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.26.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.input_layernorm.weight
Loading weights for: base_model.model.model.layers.27.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.27.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.27.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.28.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.28.input_layernorm.weight
Loading weights for: base_model.model.model.layers.28.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.28.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.input_layernorm.weight
Loading weights for: base_model.model.model.layers.29.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.29.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.30.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00007-of-00009.safetensors
Loading weights for: base_model.model.model.layers.30.input_layernorm.weight
Loading weights for: base_model.model.model.layers.30.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.30.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.input_layernorm.weight
Loading weights for: base_model.model.model.layers.31.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.31.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.input_layernorm.weight
Loading weights for: base_model.model.model.layers.32.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.32.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.32.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.33.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.33.input_layernorm.weight
Loading weights for: base_model.model.model.layers.33.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.33.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.input_layernorm.weight
Loading weights for: base_model.model.model.layers.34.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.34.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.input_layernorm.weight
Loading weights for: base_model.model.model.layers.35.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.35.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.35.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00008-of-00009.safetensors
Loading weights for: base_model.model.model.layers.36.input_layernorm.weight
Loading weights for: base_model.model.model.layers.36.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.36.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.input_layernorm.weight
Loading weights for: base_model.model.model.layers.37.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.37.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.38.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.38.input_layernorm.weight
Loading weights for: base_model.model.model.layers.38.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.38.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.input_layernorm.weight
Loading weights for: base_model.model.model.layers.39.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.39.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.norm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00009-of-00009.safetensors
Loading weights for: base_model.model.lm_head.weight
Missing in checkpoint: vision_model.class_embedding
Missing in checkpoint: vision_model.patch_embedding.weight
Missing in checkpoint: vision_model.gated_positional_embedding.gate
Missing in checkpoint: vision_model.gated_positional_embedding.embedding
Missing in checkpoint: vision_model.gated_positional_embedding.tile_embedding.weight
Missing in checkpoint: vision_model.pre_tile_positional_embedding.gate
Missing in checkpoint: vision_model.pre_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.post_tile_positional_embedding.gate
Missing in checkpoint: vision_model.post_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.layernorm_pre.weight
Missing in checkpoint: vision_model.layernorm_pre.bias
Missing in checkpoint: vision_model.layernorm_post.weight
Missing in checkpoint: vision_model.layernorm_post.bias
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: language_model.model.embed_tokens.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.0.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.0.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.1.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.2.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.3.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.3.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.4.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.5.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.6.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.7.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.8.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.8.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.9.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.10.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.11.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.12.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.13.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.13.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.14.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.15.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.16.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.17.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.18.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.18.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.19.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.20.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.21.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.22.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.23.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.23.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.24.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.25.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.26.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.27.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.28.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.28.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.29.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.30.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.31.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.32.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.33.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.33.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.34.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.35.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.36.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.37.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.38.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.38.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.39.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.norm.weight
Missing in checkpoint: language_model.lm_head.weight
Missing in checkpoint: multi_modal_projector.weight
Missing in checkpoint: multi_modal_projector.bias
Extra in checkpoint: model.embed_tokens.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.k_proj.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.o_proj.weight
Extra in checkpoint: model.layers.0.mlp.gate_proj.weight
Extra in checkpoint: model.layers.0.mlp.up_proj.weight
Extra in checkpoint: model.layers.0.mlp.down_proj.weight
Extra in checkpoint: model.layers.0.input_layernorm.weight
Extra in checkpoint: model.layers.0.post_attention_layernorm.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.k_proj.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.o_proj.weight
Extra in checkpoint: model.layers.1.mlp.gate_proj.weight
Extra in checkpoint: model.layers.1.mlp.up_proj.weight
Extra in checkpoint: model.layers.1.mlp.down_proj.weight
Extra in checkpoint: model.layers.1.input_layernorm.weight
Extra in checkpoint: model.layers.1.post_attention_layernorm.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.k_proj.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.o_proj.weight
Extra in checkpoint: model.layers.2.mlp.gate_proj.weight
Extra in checkpoint: model.layers.2.mlp.up_proj.weight
Extra in checkpoint: model.layers.2.mlp.down_proj.weight
Extra in checkpoint: model.layers.2.input_layernorm.weight
Extra in checkpoint: model.layers.2.post_attention_layernorm.weight
Extra in checkpoint: model.layers.3.cross_attn_attn_gate
Extra in checkpoint: model.layers.3.cross_attn_mlp_gate
Extra in checkpoint: model.layers.3.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.3.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.3.input_layernorm.weight
Extra in checkpoint: model.layers.3.mlp.gate_proj.weight
Extra in checkpoint: model.layers.3.mlp.up_proj.weight
Extra in checkpoint: model.layers.3.mlp.down_proj.weight
Extra in checkpoint: model.layers.3.post_attention_layernorm.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.k_proj.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.o_proj.weight
Extra in checkpoint: model.layers.4.mlp.gate_proj.weight
Extra in checkpoint: model.layers.4.mlp.up_proj.weight
Extra in checkpoint: model.layers.4.mlp.down_proj.weight
Extra in checkpoint: model.layers.4.input_layernorm.weight
Extra in checkpoint: model.layers.4.post_attention_layernorm.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.k_proj.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.o_proj.weight
Extra in checkpoint: model.layers.5.mlp.gate_proj.weight
Extra in checkpoint: model.layers.5.mlp.up_proj.weight
Extra in checkpoint: model.layers.5.mlp.down_proj.weight
Extra in checkpoint: model.layers.5.input_layernorm.weight
Extra in checkpoint: model.layers.5.post_attention_layernorm.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.k_proj.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.o_proj.weight
Extra in checkpoint: model.layers.6.mlp.gate_proj.weight
Extra in checkpoint: model.layers.6.mlp.up_proj.weight
Extra in checkpoint: model.layers.6.mlp.down_proj.weight
Extra in checkpoint: model.layers.6.input_layernorm.weight
Extra in checkpoint: model.layers.6.post_attention_layernorm.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.k_proj.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.o_proj.weight
Extra in checkpoint: model.layers.7.mlp.gate_proj.weight
Extra in checkpoint: model.layers.7.mlp.up_proj.weight
Extra in checkpoint: model.layers.7.mlp.down_proj.weight
Extra in checkpoint: model.layers.7.input_layernorm.weight
Extra in checkpoint: model.layers.7.post_attention_layernorm.weight
Extra in checkpoint: model.layers.8.cross_attn_attn_gate
Extra in checkpoint: model.layers.8.cross_attn_mlp_gate
Extra in checkpoint: model.layers.8.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.8.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.8.input_layernorm.weight
Extra in checkpoint: model.layers.8.mlp.gate_proj.weight
Extra in checkpoint: model.layers.8.mlp.up_proj.weight
Extra in checkpoint: model.layers.8.mlp.down_proj.weight
Extra in checkpoint: model.layers.8.post_attention_layernorm.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.k_proj.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.o_proj.weight
Extra in checkpoint: model.layers.9.mlp.gate_proj.weight
Extra in checkpoint: model.layers.9.mlp.up_proj.weight
Extra in checkpoint: model.layers.9.mlp.down_proj.weight
Extra in checkpoint: model.layers.9.input_layernorm.weight
Extra in checkpoint: model.layers.9.post_attention_layernorm.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.k_proj.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.o_proj.weight
Extra in checkpoint: model.layers.10.mlp.gate_proj.weight
Extra in checkpoint: model.layers.10.mlp.up_proj.weight
Extra in checkpoint: model.layers.10.mlp.down_proj.weight
Extra in checkpoint: model.layers.10.input_layernorm.weight
Extra in checkpoint: model.layers.10.post_attention_layernorm.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.k_proj.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.o_proj.weight
Extra in checkpoint: model.layers.11.mlp.gate_proj.weight
Extra in checkpoint: model.layers.11.mlp.up_proj.weight
Extra in checkpoint: model.layers.11.mlp.down_proj.weight
Extra in checkpoint: model.layers.11.input_layernorm.weight
Extra in checkpoint: model.layers.11.post_attention_layernorm.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.k_proj.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.o_proj.weight
Extra in checkpoint: model.layers.12.mlp.gate_proj.weight
Extra in checkpoint: model.layers.12.mlp.up_proj.weight
Extra in checkpoint: model.layers.12.mlp.down_proj.weight
Extra in checkpoint: model.layers.12.input_layernorm.weight
Extra in checkpoint: model.layers.12.post_attention_layernorm.weight
Extra in checkpoint: model.layers.13.cross_attn_attn_gate
Extra in checkpoint: model.layers.13.cross_attn_mlp_gate
Extra in checkpoint: model.layers.13.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.13.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.13.input_layernorm.weight
Extra in checkpoint: model.layers.13.mlp.gate_proj.weight
Extra in checkpoint: model.layers.13.mlp.up_proj.weight
Extra in checkpoint: model.layers.13.mlp.down_proj.weight
Extra in checkpoint: model.layers.13.post_attention_layernorm.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.k_proj.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.o_proj.weight
Extra in checkpoint: model.layers.14.mlp.gate_proj.weight
Extra in checkpoint: model.layers.14.mlp.up_proj.weight
Extra in checkpoint: model.layers.14.mlp.down_proj.weight
Extra in checkpoint: model.layers.14.input_layernorm.weight
Extra in checkpoint: model.layers.14.post_attention_layernorm.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.k_proj.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.o_proj.weight
Extra in checkpoint: model.layers.15.mlp.gate_proj.weight
Extra in checkpoint: model.layers.15.mlp.up_proj.weight
Extra in checkpoint: model.layers.15.mlp.down_proj.weight
Extra in checkpoint: model.layers.15.input_layernorm.weight
Extra in checkpoint: model.layers.15.post_attention_layernorm.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.k_proj.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.o_proj.weight
Extra in checkpoint: model.layers.16.mlp.gate_proj.weight
Extra in checkpoint: model.layers.16.mlp.up_proj.weight
Extra in checkpoint: model.layers.16.mlp.down_proj.weight
Extra in checkpoint: model.layers.16.input_layernorm.weight
Extra in checkpoint: model.layers.16.post_attention_layernorm.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.k_proj.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.o_proj.weight
Extra in checkpoint: model.layers.17.mlp.gate_proj.weight
Extra in checkpoint: model.layers.17.mlp.up_proj.weight
Extra in checkpoint: model.layers.17.mlp.down_proj.weight
Extra in checkpoint: model.layers.17.input_layernorm.weight
Extra in checkpoint: model.layers.17.post_attention_layernorm.weight
Extra in checkpoint: model.layers.18.cross_attn_attn_gate
Extra in checkpoint: model.layers.18.cross_attn_mlp_gate
Extra in checkpoint: model.layers.18.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.18.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.18.input_layernorm.weight
Extra in checkpoint: model.layers.18.mlp.gate_proj.weight
Extra in checkpoint: model.layers.18.mlp.up_proj.weight
Extra in checkpoint: model.layers.18.mlp.down_proj.weight
Extra in checkpoint: model.layers.18.post_attention_layernorm.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.k_proj.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.o_proj.weight
Extra in checkpoint: model.layers.19.mlp.gate_proj.weight
Extra in checkpoint: model.layers.19.mlp.up_proj.weight
Extra in checkpoint: model.layers.19.mlp.down_proj.weight
Extra in checkpoint: model.layers.19.input_layernorm.weight
Extra in checkpoint: model.layers.19.post_attention_layernorm.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.k_proj.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.o_proj.weight
Extra in checkpoint: model.layers.20.mlp.gate_proj.weight
Extra in checkpoint: model.layers.20.mlp.up_proj.weight
Extra in checkpoint: model.layers.20.mlp.down_proj.weight
Extra in checkpoint: model.layers.20.input_layernorm.weight
Extra in checkpoint: model.layers.20.post_attention_layernorm.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.k_proj.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.o_proj.weight
Extra in checkpoint: model.layers.21.mlp.gate_proj.weight
Extra in checkpoint: model.layers.21.mlp.up_proj.weight
Extra in checkpoint: model.layers.21.mlp.down_proj.weight
Extra in checkpoint: model.layers.21.input_layernorm.weight
Extra in checkpoint: model.layers.21.post_attention_layernorm.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.k_proj.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.o_proj.weight
Extra in checkpoint: model.layers.22.mlp.gate_proj.weight
Extra in checkpoint: model.layers.22.mlp.up_proj.weight
Extra in checkpoint: model.layers.22.mlp.down_proj.weight
Extra in checkpoint: model.layers.22.input_layernorm.weight
Extra in checkpoint: model.layers.22.post_attention_layernorm.weight
Extra in checkpoint: model.layers.23.cross_attn_attn_gate
Extra in checkpoint: model.layers.23.cross_attn_mlp_gate
Extra in checkpoint: model.layers.23.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.23.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.23.input_layernorm.weight
Extra in checkpoint: model.layers.23.mlp.gate_proj.weight
Extra in checkpoint: model.layers.23.mlp.up_proj.weight
Extra in checkpoint: model.layers.23.mlp.down_proj.weight
Extra in checkpoint: model.layers.23.post_attention_layernorm.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.k_proj.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.o_proj.weight
Extra in checkpoint: model.layers.24.mlp.gate_proj.weight
Extra in checkpoint: model.layers.24.mlp.up_proj.weight
Extra in checkpoint: model.layers.24.mlp.down_proj.weight
Extra in checkpoint: model.layers.24.input_layernorm.weight
Extra in checkpoint: model.layers.24.post_attention_layernorm.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.k_proj.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.o_proj.weight
Extra in checkpoint: model.layers.25.mlp.gate_proj.weight
Extra in checkpoint: model.layers.25.mlp.up_proj.weight
Extra in checkpoint: model.layers.25.mlp.down_proj.weight
Extra in checkpoint: model.layers.25.input_layernorm.weight
Extra in checkpoint: model.layers.25.post_attention_layernorm.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.k_proj.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.o_proj.weight
Extra in checkpoint: model.layers.26.mlp.gate_proj.weight
Extra in checkpoint: model.layers.26.mlp.up_proj.weight
Extra in checkpoint: model.layers.26.mlp.down_proj.weight
Extra in checkpoint: model.layers.26.input_layernorm.weight
Extra in checkpoint: model.layers.26.post_attention_layernorm.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.k_proj.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.o_proj.weight
Extra in checkpoint: model.layers.27.mlp.gate_proj.weight
Extra in checkpoint: model.layers.27.mlp.up_proj.weight
Extra in checkpoint: model.layers.27.mlp.down_proj.weight
Extra in checkpoint: model.layers.27.input_layernorm.weight
Extra in checkpoint: model.layers.27.post_attention_layernorm.weight
Extra in checkpoint: model.layers.28.cross_attn_attn_gate
Extra in checkpoint: model.layers.28.cross_attn_mlp_gate
Extra in checkpoint: model.layers.28.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.28.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.28.input_layernorm.weight
Extra in checkpoint: model.layers.28.mlp.gate_proj.weight
Extra in checkpoint: model.layers.28.mlp.up_proj.weight
Extra in checkpoint: model.layers.28.mlp.down_proj.weight
Extra in checkpoint: model.layers.28.post_attention_layernorm.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.k_proj.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.o_proj.weight
Extra in checkpoint: model.layers.29.mlp.gate_proj.weight
Extra in checkpoint: model.layers.29.mlp.up_proj.weight
Extra in checkpoint: model.layers.29.mlp.down_proj.weight
Extra in checkpoint: model.layers.29.input_layernorm.weight
Extra in checkpoint: model.layers.29.post_attention_layernorm.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.k_proj.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.o_proj.weight
Extra in checkpoint: model.layers.30.mlp.gate_proj.weight
Extra in checkpoint: model.layers.30.mlp.up_proj.weight
Extra in checkpoint: model.layers.30.mlp.down_proj.weight
Extra in checkpoint: model.layers.30.input_layernorm.weight
Extra in checkpoint: model.layers.30.post_attention_layernorm.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.k_proj.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.o_proj.weight
Extra in checkpoint: model.layers.31.mlp.gate_proj.weight
Extra in checkpoint: model.layers.31.mlp.up_proj.weight
Extra in checkpoint: model.layers.31.mlp.down_proj.weight
Extra in checkpoint: model.layers.31.input_layernorm.weight
Extra in checkpoint: model.layers.31.post_attention_layernorm.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.k_proj.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.o_proj.weight
Extra in checkpoint: model.layers.32.mlp.gate_proj.weight
Extra in checkpoint: model.layers.32.mlp.up_proj.weight
Extra in checkpoint: model.layers.32.mlp.down_proj.weight
Extra in checkpoint: model.layers.32.input_layernorm.weight
Extra in checkpoint: model.layers.32.post_attention_layernorm.weight
Extra in checkpoint: model.layers.33.cross_attn_attn_gate
Extra in checkpoint: model.layers.33.cross_attn_mlp_gate
Extra in checkpoint: model.layers.33.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.33.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.33.input_layernorm.weight
Extra in checkpoint: model.layers.33.mlp.gate_proj.weight
Extra in checkpoint: model.layers.33.mlp.up_proj.weight
Extra in checkpoint: model.layers.33.mlp.down_proj.weight
Extra in checkpoint: model.layers.33.post_attention_layernorm.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.k_proj.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.o_proj.weight
Extra in checkpoint: model.layers.34.mlp.gate_proj.weight
Extra in checkpoint: model.layers.34.mlp.up_proj.weight
Extra in checkpoint: model.layers.34.mlp.down_proj.weight
Extra in checkpoint: model.layers.34.input_layernorm.weight
Extra in checkpoint: model.layers.34.post_attention_layernorm.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.k_proj.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.o_proj.weight
Extra in checkpoint: model.layers.35.mlp.gate_proj.weight
Extra in checkpoint: model.layers.35.mlp.up_proj.weight
Extra in checkpoint: model.layers.35.mlp.down_proj.weight
Extra in checkpoint: model.layers.35.input_layernorm.weight
Extra in checkpoint: model.layers.35.post_attention_layernorm.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.k_proj.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.o_proj.weight
Extra in checkpoint: model.layers.36.mlp.gate_proj.weight
Extra in checkpoint: model.layers.36.mlp.up_proj.weight
Extra in checkpoint: model.layers.36.mlp.down_proj.weight
Extra in checkpoint: model.layers.36.input_layernorm.weight
Extra in checkpoint: model.layers.36.post_attention_layernorm.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.k_proj.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.o_proj.weight
Extra in checkpoint: model.layers.37.mlp.gate_proj.weight
Extra in checkpoint: model.layers.37.mlp.up_proj.weight
Extra in checkpoint: model.layers.37.mlp.down_proj.weight
Extra in checkpoint: model.layers.37.input_layernorm.weight
Extra in checkpoint: model.layers.37.post_attention_layernorm.weight
Extra in checkpoint: model.layers.38.cross_attn_attn_gate
Extra in checkpoint: model.layers.38.cross_attn_mlp_gate
Extra in checkpoint: model.layers.38.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.38.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.38.input_layernorm.weight
Extra in checkpoint: model.layers.38.mlp.gate_proj.weight
Extra in checkpoint: model.layers.38.mlp.up_proj.weight
Extra in checkpoint: model.layers.38.mlp.down_proj.weight
Extra in checkpoint: model.layers.38.post_attention_layernorm.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.k_proj.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.o_proj.weight
Extra in checkpoint: model.layers.39.mlp.gate_proj.weight
Extra in checkpoint: model.layers.39.mlp.up_proj.weight
Extra in checkpoint: model.layers.39.mlp.down_proj.weight
Extra in checkpoint: model.layers.39.input_layernorm.weight
Extra in checkpoint: model.layers.39.post_attention_layernorm.weight
Extra in checkpoint: model.norm.weight
Extra in checkpoint: lm_head.weight
Some layers did not match.
LoRA model loaded successfully!
bbbbb
cuda:0
Length of the updated dataset: 560

system

Today Date: 17 Nov 2024

You are a helpful AI that can generate tikz code from images.
user

This is a picture of a scientific figure Generate LaTeX code that draws this scientific figure using TikZ. Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. Don't forget to include \usepackage{tikz}! I understand that this is a challenging task, so do your best. Return your result in a ```latex code block.
assistant

\documentclass[border=10pt]{standalone}
\usepackage{tikz}
\begin{document}

\begin{tikzpicture}[scale=1,font=\small]

\draw[step=0.5cm,gray,very thin] (-0.5,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,-0.5) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid (4,3);

\draw[step=0.5cm,gray,very thin] (0,0) grid
########################################################
Error: Could not locate LaTeX code markers.
Processing sample 1/560 - Caption: The image depicts a segmented line representing a discretized interval from \( x = 0 \) to \( x = 1 \). The interval is divided into \( N \) segments, each of length \( \Delta x = 1/N \). The points along the line are labeled with their corresponding positions \( x = m \Delta x \) for \( m = 0, 1, 2, \ldots, N \). Below the line, two rows of indices are shown: the global index \( j \) and the interior index \( k \). The global index \( j \) ranges from 1 to \( N+1 \), while the interior index \( k \) ranges from 1 to \( N-1 \). The correspondence between the global and interior indices is indicated, with specific points highlighted such as \( x = 0 \) (global index \( j = 1 \)), \( x = \Delta x \) (global index \( j = 2 \), interior index \( k = 1 \)), and so on, up to \( x = 1 \) (global index \( j = N+1 \)).
TEX Edit Distance for sample 1: 0.8007503747940063
CrystalBLEU Score for sample 1: 0
Sample 1 processing time: 55.81 seconds

Processing sample 2/560 - Caption: This image depicts a directed graph with six nodes and various directed edges, including both solid and dashed lines. The nodes are labeled as follows: \( V_1 (H, a) \), \( V_2 (x) \), \( V_3 (\tilde{x}) \), \( V_4 (L) \), \( V_5 (L) \), and \( V_6 (G, \tilde{c}) \). The edges are as follows:

1. A solid edge from \( V_1 \) to \( V_2 \).
2. A solid edge from \( V_1 \) to \( V_3 \).
3. A solid edge from \( V_2 \) to \( V_4 \).
4. A solid edge from \( V_3 \) to \( V_4 \).
5. A solid edge from \( V_4 \) to \( V_6 \).
6. A solid edge from \( V_5 \) to \( V_6 \).
7. A dashed edge from \( V_1 \) to \( V_4 \).
8. A dashed edge from \( V_2 \) to \( V_5 \).
9. A dashed edge from \( V_3 \) to \( V_6 \).
10. A dashed edge from \( V_4 \) to \( V_5 \).

The nodes are arranged in a roughly horizontal layout, with \( V_1 \) on the left, \( V_6 \) on the right, and the other nodes positioned between them. The edges are directed and some are curved to avoid overlapping with other edges.
TEX Edit Distance for sample 2: 0.5619602203369141
CrystalBLEU Score for sample 2: 0.016551856708081174
Sample 2 processing time: 28.17 seconds

Processing sample 3/560 - Caption: This image is a line plot with three datasets: Multi-News, SamSUM, and CNN/DM. The x-axis represents the dataset size on a logarithmic scale, ranging from \(10^3\) to \(10^5\). The y-axis represents the average value, ranging from 0.5 to 0.7. The plot includes three lines: a blue line with square markers for Multi-News, a red line with triangle markers for SamSUM, and a green line with star markers for CNN/DM. The legend is located inside the plot area, towards the right side. The grid lines are visible for better readability.
TEX Edit Distance for sample 3: 0.8242249488830566
CrystalBLEU Score for sample 3: 0.0030537034935598883
Sample 3 processing time: 55.84 seconds

Processing sample 4/560 - Caption: The image shows a graphical representation of the operation on two structures, Σ(z₁) and Σ(z₂), resulting in a combined structure Σ(z₁) ⊕ Σ(z₁). On the left side, there are two separate structures, each consisting of a circle connected to a smaller circle above it, labeled Σ(z₁) and Σ(z₂) respectively. These structures are combined using the ⊕ operator. On the right side, the resulting structure is shown, which is a larger structure with three levels of circles. The bottom level has two circles, each connected to a smaller circle above it, and these two smaller circles are connected to a single circle at the top. The label Σ(z₁) ⊕ Σ(z₁) is placed below the combined structure.
TEX Edit Distance for sample 4: 0.6596148610115051
CrystalBLEU Score for sample 4: 0.0005999239901600418
Sample 4 processing time: 17.70 seconds

Processing sample 5/560 - Caption: The image shows a two-panel plot comparing simulated and theoretical coalescence times under weak and strong selection. The left panel is titled "Coalescence times" and is divided into two regions: weak selection on the left and strong selection on the right. The x-axis is labeled with the selection coefficient (α), ranging from 0 to 1, while the y-axis is labeled with the expected coalescence time (E[T2]) ranging from 0 to 15. The black dots represent simulated data (N = 10^3), and the green curve represents the theoretical values. The right panel, partially visible, seems to follow a similar structure but with different y-axis values.
TEX Edit Distance for sample 5: 0.6454665064811707
CrystalBLEU Score for sample 5: 1.3897754371508595e-05
Sample 5 processing time: 44.96 seconds

Processing sample 6/560 - Caption: This image consists of two Hasse diagrams side by side. The diagram on the left, labeled \( P \), is a diamond-shaped poset with four elements: \( a \) at the bottom, \( b \) and \( c \) in the middle, and \( d \) at the top. The elements are connected by lines indicating the partial order: \( a \leq b \), \( a \leq c \), \( b \leq d \), and \( c \leq d \).

The diagram on the right, labeled \( \text{Int} \, P \), represents the interval poset of \( P \). It has nine elements, each representing an interval in \( P \): \([a,a]\), \([a,b]\), \([a,c]\), \([b,b]\), \([b,d]\), \([c,c]\), \([c,d]\), \([d,d]\), and \([a,d]\). These intervals are connected by lines indicating the partial order among them. The structure forms a more complex lattice with multiple levels, reflecting the intervals' relationships.
TEX Edit Distance for sample 6: 0.7455529570579529
CrystalBLEU Score for sample 6: 2.912911343742091e-07
Sample 6 processing time: 30.89 seconds

Processing sample 7/560 - Caption: The figure is a line plot showing the impact of batch size on inference speedups. The x-axis represents the batch size, ranging from 1 to 16, while the y-axis represents the speedup, ranging from 1 to 5. Multiple lines, each with different colors and markers, represent different configurations or models. A legend on the top right corner identifies these configurations. A vertical dashed line at batch size 8 highlights a specific point of interest.
TEX Edit Distance for sample 7: 0.7904283404350281
CrystalBLEU Score for sample 7: 2.3577462643888274e-07
Sample 7 processing time: 55.85 seconds

Processing sample 8/560 - Caption: The image illustrates a sequence of transformations of intersecting lines. 

1. The top part shows a set of horizontal and vertical lines labeled with numbers. The horizontal lines are labeled "n", "n-2", "n-4", ..., "1" from top to bottom, and the vertical lines are labeled "1".
2. An arrow points downward to the middle part, where the lines are rotated to form an "X" pattern. The lines are labeled similarly, with horizontal lines labeled "1" and "n-2", and vertical lines labeled "1" and "n-6".
3. Another arrow points downward to the bottom part, where the lines are further transformed into a symmetric "X" pattern. The lines are labeled "1" and "1", with the horizontal lines in black and the diagonal lines in red.

The image can be described in TikZ with the following elements:
- Horizontal and vertical lines with labels.
- Rotated lines forming an "X" pattern.
- Symmetric "X" pattern with different colors for the lines.
- Arrows indicating the transformation steps.
TEX Edit Distance for sample 8: 0.8341864943504333
CrystalBLEU Score for sample 8: 9.07543003405425e-08
Sample 8 processing time: 56.02 seconds

Processing sample 9/560 - Caption: This image depicts a triangular plot contained within a rectangular frame. The plot consists of two diagonal lines forming an inverted "V" shape, with tick marks and labels along both lines. The labels are positioned at regular intervals along the lines, indicating specific values. The entire plot is enclosed within a smaller rectangle, which is centered within a larger rectangular border. The larger rectangle appears to be the boundary of the entire figure, while the smaller rectangle serves as the frame for the triangular plot.
TEX Edit Distance for sample 9: 0.6842021346092224
CrystalBLEU Score for sample 9: 1.3172960112072808e-05
Sample 9 processing time: 38.13 seconds

Processing sample 10/560 - Caption: The image depicts a geometric representation of complex numbers. It shows a series of ellipses increasing in size along the positive real axis, labeled as \(|z|^2\). The ellipses are aligned along a line starting from the origin (0), which is marked by a solid black dot. The x-axis is labeled with \(|z|^2\) to indicate the squared magnitude of the complex number \(z\). The ellipses are positioned at regular intervals along this axis, with their major axes increasing linearly.
TEX Edit Distance for sample 10: 0.7072562575340271
CrystalBLEU Score for sample 10: 1.1856642685082685e-05
Sample 10 processing time: 8.91 seconds

Processing sample 11/560 - Caption: This image is a line plot depicting the relationship between sentence length and two performance metrics: "Matched Words" and "Exact Sentences." The x-axis represents sentence length, ranging from 0 to 60, while the y-axis represents the proportion, ranging from 0 to 1. Two lines are plotted: a blue line for "Matched Words" and a red line for "Exact Sentences." The blue line starts near the top of the y-axis and remains relatively high, showing a slight downward trend as sentence length increases. The red line starts high but decreases sharply as sentence length increases, approaching zero around a sentence length of 50. A legend in the bottom left corner identifies the blue and red lines. The plot has a title "Performance Metrics vs. Sentence Length" and includes grid lines for better readability.
TEX Edit Distance for sample 11: 0.6259247660636902
CrystalBLEU Score for sample 11: 4.6227226160837036e-05
Sample 11 processing time: 40.65 seconds

Processing sample 12/560 - Caption: The image consists of three distinct diagrams, each depicting a different colored path with labeled points and segments. 

1. The leftmost diagram is in red and shows a path with points labeled \(a\) and \(b\). The path has a horizontal dashed line segment labeled \(H\) connecting the points. The path starts from a point labeled \(R\) and ends at \(b\).

2. The middle diagram is in black and shows a similar path with points labeled \(a\) and \(b\). The path has a horizontal dashed line segment labeled \(\bar{H}\) connecting the points. The path starts from a point labeled \(\bar{R}\) and ends at \(b\).

3. The rightmost diagram is in blue and shows a path with points labeled \(t(a)\) and \(t(b)\). The path starts from a point labeled \(B'\) and ends at \(t(a)\).

Each diagram features a combination of straight and angled lines, with specific points marked by colored dots. The dashed lines in each diagram indicate a horizontal connection between the points \(a\) and \(b\).
TEX Edit Distance for sample 12: 0.5949740409851074
CrystalBLEU Score for sample 12: 0.00013660244267095048
Sample 12 processing time: 39.19 seconds

Processing sample 13/560 - Caption: This image contains a series of graph transformations labeled \( F_1(n) \) through \( F_{11}(n) \). Each transformation shows a different configuration of nodes and edges, with some nodes labeled with letters (e.g., X, Y) and others with numbers (e.g., 1, 2, 3). The transformations illustrate the progression of the graph structure through various stages:

1. \( F_1(n) \) shows a triangle with nodes 1, 2, and 3 connected in a cycle, and a separate node X connected to node 1.
2. \( F_2(n) \) shows node X connected to a triangle formed by nodes 1, 2, and 3.
3. \( F_3(n) \) shows node X connected to a cycle of three nodes labeled 1, 2, and 3.
4. \( F_4(n) \) shows node X connected to a path of three nodes labeled 1, 2, and 3.
5. \( F_5(n) \) shows node X connected to a path of four nodes labeled 1, 2, 3, and 4.
6. \( F_6(n) \) shows node X connected to a path of five nodes labeled 1, 2, 3, 4, and 5.
7. \( F_7(n) \) shows node X connected to a path of six nodes labeled 1, 2, 3, 4, 5, and 6.
8. \( F_8(n) \) shows node X connected to a path of seven nodes labeled 1, 2, 3, 4, 5, 6, and 7.
9. \( F_9(n) \) shows node X connected to a path of eight nodes labeled 1, 2, 3, 4, 5, 6, 7, and 8.
10. \( F_{10}(n) \) shows node X connected to a path of nine nodes labeled 1, 2, 3, 4, 5, 6, 7, 8, and 9.
11. \( F_{11}(n) \) shows node X connected to a path of ten nodes labeled 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10.

Each graph transformation is enclosed in an oval or circle, indicating the scope of the transformation. The nodes and edges are clearly labeled to show the progression from one stage to the next.
TEX Edit Distance for sample 13: 0.6722259521484375
CrystalBLEU Score for sample 13: 9.79036822660015e-05
Sample 13 processing time: 42.98 seconds

Processing sample 14/560 - Caption: The image is a bar chart displaying the number of students on the y-axis and a range of scores on the x-axis. The x-axis is labeled with score intervals, each represented by a blue bar. Two specific score intervals are highlighted with different colors: one in red and one in green. The chart includes a legend at the bottom indicating that the green bar represents "ChatGPT," the red bar represents "BingChat," and the blue bars represent "Estudiantes evaluados." The y-axis is labeled with a logarithmic scale (10^0 to 10^4), and each bar has a numerical value displayed at its top.
TEX Edit Distance for sample 14: 0.8350451588630676
CrystalBLEU Score for sample 14: 5.903056552794638e-05
Sample 14 processing time: 55.77 seconds

Processing sample 15/560 - Caption: The image depicts a graph with 10 vertices arranged in a decagon. Each vertex on the decagon is connected to its adjacent vertices with black edges. Additionally, there is a central vertex connected to all vertices of the decagon with red edges. The vertices are represented by black-filled circles.

This description should help you write the TikZ code for the figure.
TEX Edit Distance for sample 15: 0.6571428775787354
CrystalBLEU Score for sample 15: 8.687112907316951e-05
Sample 15 processing time: 44.26 seconds

Processing sample 16/560 - Caption: This image depicts a complex plane diagram with two semicircles centered at the origin. The horizontal axis represents the real part of the complex plane, marked with a "0" at the origin. There are two semicircles with different radii, both centered at the origin and extending upwards. The smaller semicircle is labeled with "it_n" at its highest point, and there are two dots vertically aligned along the imaginary axis, one at the origin and the other at the label "it_n".
TEX Edit Distance for sample 16: 0.5937031507492065
CrystalBLEU Score for sample 16: 0.00018116159428035935
Sample 16 processing time: 18.01 seconds

Processing sample 17/560 - Caption: This image depicts a sequence of sets \( V_1, V_2, \ldots, V_{\beta+1} \) represented by ellipses. Each set contains points labeled \( v, x_\beta, y_\beta, y_1, x_1, u \). There are directed arrows between these points indicating transitions or connections. A red path labeled \( P_1 \) and a blue dashed path labeled \( P_2 \) connect these points across the sets. The red path \( P_1 \) seems to form a continuous connection through the points \( v, x_\beta, y_\beta, y_1, x_1, u \), while the blue dashed path \( P_2 \) forms a similar connection but with a different trajectory. An arrow pointing to the right indicates progression from \( V_1 \) to \( V_{\beta+1} \).
TEX Edit Distance for sample 17: 0.6554401516914368
CrystalBLEU Score for sample 17: 0.00033584606507848285
Sample 17 processing time: 24.86 seconds

Processing sample 18/560 - Caption: This image depicts a rectangular region in the \(u\)-\(T\) plane, labeled with boundaries and specific points. The vertical axis is labeled \(u\) and the horizontal axis is labeled \(T\). The rectangle is defined by the points \((1, \epsilon)\), \((T_0, \epsilon)\), \((T_0, A)\), and \((1, A)\). The boundaries of the rectangle are labeled as \(\Gamma_1\), \(\Gamma_2\), \(\Gamma_3\), and \(\Gamma_4\), with arrows indicating the direction along each boundary. The point \((1, \epsilon)\) is marked with dashed lines extending to the axes. The interior of the rectangle is labeled with \(u\).
TEX Edit Distance for sample 18: 0.4970414340496063
CrystalBLEU Score for sample 18: 0.0005201809805956129
Sample 18 processing time: 18.81 seconds

Processing sample 19/560 - Caption: The image consists of two parts. On the left, there is a diagram with vertical lines labeled "B" on both sides, and between them, there are horizontal lines labeled with numbers 3, 2, and dots representing "n+1". On the right, there is a graph with nodes labeled 1, 2, and 3. The central node labeled 3 is connected to nodes labeled 2, which are further connected to nodes labeled 1. The central node 3 is also connected to another node labeled 3, which is connected to nodes labeled 2 and 1, forming a symmetrical structure. An arrow points from the left diagram to the right graph, indicating a transformation from the left structure to the right structure.
TEX Edit Distance for sample 19: 0.6204153299331665
CrystalBLEU Score for sample 19: 0.0004885939146173952
Sample 19 processing time: 36.13 seconds

Processing sample 20/560 - Caption: This image depicts the architecture of a Gated Recurrent Unit (GRU) cell. The main components and their connections are as follows:

1. **Inputs and Outputs:**
   - Input \( x(t) \) enters from the bottom.
   - Previous hidden state \( h(t-1) \) enters from the left.
   - Current hidden state \( h(t) \) exits to the right.
   - Current output \( y(t) \) exits from the top.

2. **Components:**
   - Two fully connected (FC) layers, represented by blue rectangles.
   - Update gate \( z(t) \) and reset gate \( r(t) \).
   - Element-wise multiplication, represented by circles with a cross (×).
   - Element-wise addition, represented by circles with a plus (+).
   - Sigmoid activation functions, represented by the sigmoid curve symbol.

3. **Connections:**
   - \( x(t) \) and \( h(t-1) \) are inputs to both FC layers.
   - Outputs of the FC layers are used to compute \( r(t) \) and \( z(t) \).
   - \( r(t) \) is used to modulate \( h(t-1) \) before passing it to the next FC layer.
   - The result of the FC layer modulated by \( r(t) \) is combined with \( x(t) \) to compute the candidate hidden state.
   - The final hidden state \( h(t) \) is computed using a combination of the candidate hidden state and \( h(t-1) \), modulated by \( z(t) \).

This description should help in writing the TikZ code to accurately represent the GRU cell structure.
TEX Edit Distance for sample 20: 0.6369734406471252
CrystalBLEU Score for sample 20: 0.0006930879675516067
Sample 20 processing time: 34.37 seconds

Processing sample 21/560 - Caption: This image depicts a 3D geometric shape resembling a crystal or diamond. The shape is composed of multiple triangular and quadrilateral faces. The edges of the shape are outlined in red, while the faces are filled with a light blue color. The overall structure is symmetrical along its vertical axis, with the top and bottom parts tapering to a point. The central section of the shape has a more complex arrangement of faces, creating a faceted appearance.
TEX Edit Distance for sample 21: 0.6115403175354004
CrystalBLEU Score for sample 21: 0.0008249866641689557
Sample 21 processing time: 38.76 seconds

Processing sample 22/560 - Caption: This image depicts a set of rays emanating from the origin in a coordinate plane. The rays are labeled with various mathematical expressions involving the function \( D \). The labels are positioned at the ends of the rays. The rays are distributed in the first and fourth quadrants, with some rays being solid lines and others being dotted lines. The axes are labeled with \( D(1) \) on the positive y-axis and \( D(2) \) on the positive x-axis. Additional labels include \( D\left(\frac{1}{2}\right) \), \( D\left(\frac{1}{22}\right) \), \( D(\tau^{-1}(2)) \), \( D\left(\frac{1}{2}(d, \lambda)\right) \), and \( D\left(\frac{11}{2}\right) \).

To create this figure using TikZ, you would need to:
1. Draw the x-axis and y-axis.
2. Draw multiple rays emanating from the origin at different angles.
3. Label the ends of the rays with the specified mathematical expressions.
4. Use solid and dotted lines for different rays as shown in the image.
TEX Edit Distance for sample 22: 0.5218681693077087
CrystalBLEU Score for sample 22: 0.0009465987159954411
Sample 22 processing time: 16.05 seconds

Processing sample 23/560 - Caption: This image appears to be a grid of 4x4 spheres, each labeled with various variables and symbols. The spheres are color-coded and contain different mathematical notations. The first column contains spheres labeled with variables \(x\), \(y\), and \(z\) in red, blue, and cyan respectively. The second to fourth columns contain spheres with overlapping colored regions and additional labels such as \(a\), \(b\), \(g\), and \(f\). There are arrows pointing from the first column to the second column, indicating a transformation or mapping, with labels \(O_{0000}\) and \(O_{000}\). The symbols \( \subseteq \) are used between the columns to indicate inclusion or subset relationships. The entire grid is annotated with a number (1) on the right side.

To write the TikZ code for this figure, you would need to create a 4x4 grid of nodes, draw spheres at each node, label them appropriately, and use arrows and subset symbols to indicate the relationships between the spheres. The color coding and overlapping regions would also need to be replicated.
TEX Edit Distance for sample 23: 0.8147314786911011
CrystalBLEU Score for sample 23: 0.0006696851366774808
Sample 23 processing time: 55.91 seconds

Processing sample 24/560 - Caption: The figure illustrates a positive consumption externality in the energy-efficient housing market. The x-axis represents the quantity of energy-efficient housing, while the y-axis represents the price, costs, and benefits in yen. The graph includes the following curves:

1. **MPB (Marginal Private Benefit)**: A downward-sloping red line.
2. **MSB (Marginal Social Benefit)**: A downward-sloping blue line, above the MPB curve.
3. **MPC (Marginal Private Cost) = MSC (Marginal Social Cost)**: An upward-sloping blue line.
4. **MEB (Marginal External Benefit)**: A vertical distance between the MPB and MSB curves, marked with an orange double arrow.

The equilibrium without externality (E1) is at the intersection of the MPB and MPC curves, with a corresponding quantity Qm and price Pm. The socially optimal equilibrium (Es) is at the intersection of the MSB and MSC curves, with a corresponding quantity Qopt and price Psot.

The welfare loss, represented by a green triangle, is the area between the MPB and MSB curves from Qm to Qopt.
TEX Edit Distance for sample 24: 0.5623220205307007
CrystalBLEU Score for sample 24: 0.000904184074917227
Sample 24 processing time: 43.68 seconds

Processing sample 25/560 - Caption: This image depicts a geometric configuration involving three vertical lines at positions \(x_{k-1}\), \(x_k\), and \(x_{k+1}\) labeled \(C_{k-1}\), \(C_k\), and \(C_{k+1}\) respectively. The line at \(x_k\) has several angles and vectors emanating from it. The vectors \(U_{\Gamma}^{(\tau)}\), \(U_{\Gamma}\), \(U_I\), and \(U_B\) are shown, with \(U_{\Gamma}^{(\tau)}\) and \(U_{\Gamma}\) drawn in blue. The angles \(\alpha_1\) and \(\beta_1\) are marked between these vectors. The top of the lines \(C_{k-1}\) and \(C_k\) are connected by a slanted line, and the top of \(C_k\) and \(C_{k+1}\) are connected similarly. The lines \(C_{k-1}\) and \(C_{k+1}\) have hatching patterns on the top sections.
TEX Edit Distance for sample 25: 0.5457442998886108
CrystalBLEU Score for sample 25: 0.0010866303096082476
Sample 25 processing time: 18.55 seconds

Processing sample 26/560 - Caption: This image depicts a decision tree diagram used to model the process of claim settlement and payment. The diagram is divided into two main sections: "Settlement" and "Payment." 

1. The "Settlement" section has a single decision node labeled "1. settlement."
2. The "Payment" section is further divided into two branches based on the outcome of the settlement:
   - If "Yes," there are two decision nodes labeled "2. payment" and "3. pct_paid."
   - If "No," there are two decision nodes labeled "2. payment" and "3. increase_paid."

The diagram includes labels for "Initial claim characteristics" and "Updates" on the left side, indicating the factors influencing the decision process. The structure is organized in a tabular format with clear horizontal and vertical lines separating the different sections and decision nodes.

This description can help in writing the TikZ code by providing a clear understanding of the hierarchical structure and labeling of the decision nodes.
TEX Edit Distance for sample 26: 0.6218050718307495
CrystalBLEU Score for sample 26: 0.001389214869444367
Sample 26 processing time: 24.82 seconds

Processing sample 27/560 - Caption: This image depicts a coordinate system with the x1-axis and x2-axis, ranging from -2 to 2 on both axes. The background is divided into two regions: the left half is shaded in red and the right half in green. The x1-axis is labeled with points a(t_min) at -1 and a(t_max) at 1. There is a semicircular arc labeled Γ centered at the origin, spanning from a(t_min) to a(t_max). The axes are labeled with x1 and x2, and there are tick marks at each integer value on both axes.
TEX Edit Distance for sample 27: 0.8211050629615784
CrystalBLEU Score for sample 27: 0.0012884402450714157
Sample 27 processing time: 55.71 seconds

Processing sample 28/560 - Caption: This image is a plot of two probability density functions (PDFs) over the same range of values for "Feature Y" on the x-axis and "Count" on the y-axis. The first PDF is represented by a blue dashed line, peaking around 0, and the second PDF is represented by a red solid line, peaking around 1. The x-axis ranges from -2 to 3, and the y-axis ranges from 0 to 0.8. The plot includes axis labels for both the x-axis ("Feature Y") and the y-axis ("Count").
TEX Edit Distance for sample 28: 0.7921146750450134
CrystalBLEU Score for sample 28: 0.0012489648785302748
Sample 28 processing time: 55.68 seconds

Processing sample 29/560 - Caption: The image depicts a horizontal number line centered at 0, extending from -2 to 2. The number line is labeled with tick marks at intervals of 0.5 units. The x-axis is denoted by \( x \) at the far right end. The tick marks are labeled with the corresponding values: -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, and 2. The arrow at the right end of the number line indicates the positive direction of the x-axis.
TEX Edit Distance for sample 29: 0.6365532279014587
CrystalBLEU Score for sample 29: 0.001538437712288933
Sample 29 processing time: 20.26 seconds

Processing sample 30/560 - Caption: This image depicts a block diagram with five rectangular blocks and labeled arrows. Four blocks are aligned vertically on the left side, each labeled with \( w(t-2) \), \( w(t-1) \), \( w(t+1) \), and \( w(t+2) \). Each of these blocks has an arrow pointing to a central block labeled "SUM". The central "SUM" block has an arrow pointing to a fifth block on the right side, labeled \( w(t) \). This diagram represents a summation process where the inputs from the four left blocks are combined in the SUM block to produce an output in the right block.
TEX Edit Distance for sample 30: 0.6271344423294067
CrystalBLEU Score for sample 30: 0.0018800662980364064
Sample 30 processing time: 15.62 seconds

Processing sample 31/560 - Caption: This scatter plot compares the probability bounds \( p \) obtained from a Program-Agnostic Neural ISM (x-axis) to those obtained from Farkas' Lemma (y-axis). The x-axis and y-axis are both on a logarithmic scale ranging from \( 10^{-3} \) to \( 10^0 \). The plot includes a dashed line representing \( y = x \) for reference. Data points are marked with red circles and red crosses, where circles indicate cases where Farkas' Lemma fails and crosses indicate cases where Farkas' Lemma succeeds. A legend in the plot explains the symbols used.
TEX Edit Distance for sample 31: 0.735724687576294
CrystalBLEU Score for sample 31: 0.0026225801829082053
Sample 31 processing time: 54.28 seconds

Processing sample 32/560 - Caption: The image shows a circular diagram with two concentric circles. The outer circle is divided into 12 equal segments, each labeled with \(B_1\) to \(B_{12}\). The inner circle is divided into 8 equal segments, each labeled with \(B_1\) to \(B_8\). The segments are separated by radial lines extending from the center of the circles to their peripheries. The labels are placed near the outer edge of each segment.
TEX Edit Distance for sample 32: 0.6368340253829956
CrystalBLEU Score for sample 32: 0.0027093053992854887
Sample 32 processing time: 9.05 seconds

Processing sample 33/560 - Caption: This image depicts a geometric configuration involving three vertical lines at positions \(x_{k-1}\), \(x_k\), and \(x_{k+1}\) labeled \(C_{k-1}\), \(C_k\), and \(C_{k+1}\) respectively. The line at \(x_k\) has several angles and vectors emanating from it. The vectors \(U_{\Gamma}^{(\tau)}\), \(U_{\Gamma}\), \(U_I\), and \(U_B\) are shown, with \(U_{\Gamma}^{(\tau)}\) and \(U_{\Gamma}\) drawn in blue. The angles \(\alpha_1\) and \(\beta_1\) are marked between these vectors. The top of the lines \(C_{k-1}\) and \(C_k\) are connected by a slanted line, and the top of \(C_k\) and \(C_{k+1}\) are connected similarly. The lines \(C_{k-1}\) and \(C_{k+1}\) have hatching patterns on the top sections.
TEX Edit Distance for sample 33: 0.5661175847053528
CrystalBLEU Score for sample 33: 0.003365976419688108
Sample 33 processing time: 30.90 seconds

Processing sample 34/560 - Caption: The image shows three small graphs with labeled vertices and edges. 

1. The first graph on the left has two vertices: \( y_3 \) (filled circle) at the top and \( z_1 \) (square) at the bottom. There is a vertical edge connecting \( y_3 \) and \( z_1 \). The coordinates \( (2,3) \) are written next to \( y_3 \).

2. The second graph in the middle has three vertices: \( y_5 \) (empty circle) at the top, \( y_4 \) (filled circle) in the middle, and \( y_1 \) (filled circle) at the bottom. There are vertical edges connecting \( y_5 \) to \( y_4 \) and \( y_4 \) to \( y_1 \). The coordinates \( (1,0) \) are written next to \( y_5 \), and \( (3,1) \) are written next to \( y_1 \).

3. The third graph on the right has two vertices: \( y_6 \) (empty circle) at the top and \( y_2 \) (filled circle) at the bottom. There is a vertical edge connecting \( y_6 \) and \( y_2 \). The coordinates \( (0,3) \) are written next to \( y_6 \).

All vertices and edges are aligned vertically, and the labels are positioned near the vertices.
TEX Edit Distance for sample 34: 0.6409343481063843
CrystalBLEU Score for sample 34: 0.0036817546431111656
Sample 34 processing time: 19.42 seconds

Processing sample 35/560 - Caption: The image depicts a mathematical diagram with several elements:

1. **Background**: A rectangular grid with a light green background.
2. **Axes**: The center of the grid is marked with a black dot labeled "0".
3. **Curves**: 
   - A black curve passing through the center.
   - Two additional curves, one labeled \(\psi^+\) (in purple) and another labeled \(\psi^-\) (in blue), intersecting the black curve.
4. **Arrows**: 
   - Red arrows pointing upwards along vertical lines.
   - Purple and blue arrows along the curves \(\psi^+\) and \(\psi^-\), respectively, indicating direction.
5. **Labels**: 
   - The label \(\Psi_z(W_z) = [-1,1]^2\) on the left side.
   - The label \(\Psi_z(X \cap W_z)\) on the right side.
6. **Vertical Lines**: Red vertical lines spaced evenly across the grid.

This description should help in writing the TikZ code to replicate the figure.
TEX Edit Distance for sample 35: 0.834838330745697
CrystalBLEU Score for sample 35: 0.002682125683380968
Sample 35 processing time: 55.72 seconds

Processing sample 36/560 - Caption: Caption: "The image depicts a logarithmic spiral created using a series of interconnected lines. The spiral starts from the center and expands outward, with the lines forming a mesh-like pattern. The color of the lines is a gradient of blue, becoming lighter as they move outward from the center."

This description should help you understand the structure and color gradient of the spiral for writing the TikZ code.
TEX Edit Distance for sample 36: 0.807107150554657
CrystalBLEU Score for sample 36: 0.0025500941604191443
Sample 36 processing time: 55.47 seconds

Processing sample 37/560 - Caption: This image depicts a hierarchical tree structure with multiple levels of nodes and connections. At the top, there is a single node labeled \( \nu_c \) within an oval labeled \( \Omega_n \). This top node connects to a middle layer of nodes contained within a larger oval, labeled \( \Omega_g \cup \Omega_{nb} \). Each node in this middle layer is connected to several nodes in the bottom layer, which are grouped into three separate ovals labeled \( \Omega_p \). The nodes and connections are represented by black dots and lines, respectively. The structure illustrates a clear hierarchical relationship among the nodes, with the top node branching out to multiple nodes in the middle layer, which in turn branch out to multiple nodes in the bottom layer.
TEX Edit Distance for sample 37: 0.6614806056022644
CrystalBLEU Score for sample 37: 0.0029947374928290314
Sample 37 processing time: 30.35 seconds

Processing sample 38/560 - Caption: This bar chart compares the accuracy of five different methods: MLP, WL-Kernel, FEATHER, SLaq-LSD, and SLaq-VGNE. The y-axis represents accuracy, ranging from 0.00 to 1.00. Each bar is labeled with its corresponding accuracy value: MLP (0.8054), WL-Kernel (0.7053), FEATHER (0.8488), SLaq-LSD (0.7790), and SLaq-VGNE (0.5590). The bars are colored blue, and the x-axis labels are rotated for better readability.
TEX Edit Distance for sample 38: 0.4462788701057434
CrystalBLEU Score for sample 38: 0.003722002885955938
Sample 38 processing time: 40.56 seconds

Processing sample 39/560 - Caption: The image depicts a diagram consisting of two vertical columns of dots, each containing six dots. These columns are connected by various curved lines. The left column has three pairs of dots connected by semicircular arcs on the left side. The right column has three pairs of dots connected by semicircular arcs on the right side. Additionally, there are curved lines connecting dots between the two columns. One of these lines crosses over another. At the top right, there is a rectangular box labeled "2" connected to the second dot from the top in the right column. Below the diagram, the letter "D" is centered.
TEX Edit Distance for sample 39: 0.5759574770927429
CrystalBLEU Score for sample 39: 0.0037988796027952536
Sample 39 processing time: 24.80 seconds

Processing sample 40/560 - Caption: The image displays a single red circle centered on a white background. There is a text "The alt" in blue located at the top left corner of the image. 

To create this in TikZ, you would need to:
1. Draw a red circle at the center of the canvas.
2. Place blue text "The alt" at the top left corner of the canvas.

Here is a sample caption for the image:
"A centered red circle with radius 1 unit on a white background. Blue text 'The alt' is positioned at the top left corner of the canvas."

This caption should help you write the corresponding TikZ code.
TEX Edit Distance for sample 40: 0.3991215229034424
CrystalBLEU Score for sample 40: 0.003850736926529356
Sample 40 processing time: 7.76 seconds

Processing sample 41/560 - Caption: This image depicts a sequence of connected subgraphs, each represented by a rectangle labeled \( K_{n,n}^{(i)} - e \), where \( i \) ranges from 1 to \( n \). Each rectangle has a node at the top and bottom left corners, connected by an edge labeled "1". The rectangles are connected in a sequence by edges labeled "1" at the top nodes and by edges labeled "u" at the bottom nodes. The bottom nodes of the rectangles are connected to a central node at the bottom, which connects to the bottom left node of each rectangle. The sequence continues with ellipses indicating the continuation up to the \( n \)-th rectangle.
TEX Edit Distance for sample 41: 0.5513975024223328
CrystalBLEU Score for sample 41: 0.004144325669836022
Sample 41 processing time: 22.62 seconds

Processing sample 42/560 - Caption: This image depicts three distinct graphs \( G \), \( H \), and \( K \) aligned horizontally along a common baseline. The graph \( G \) consists of a diamond shape at the top connected by a vertical path of three edges to the baseline. The edges of the diamond are colored alternately in blue and red. The vertical path connecting the diamond to the baseline consists of two edges labeled \( e_0 \) and \( e_1 \), with \( e_0 \) being the topmost edge. The graph \( H \) is a simple vertical path of three edges, with the top and bottom edges colored red and the middle edge colored blue. The graph \( K \) is a diamond shape similar to the one in \( G \), with alternating blue and red edges, but it is directly on the baseline. All vertices are represented by black dots.
TEX Edit Distance for sample 42: 0.6402022242546082
CrystalBLEU Score for sample 42: 0.00463375384026274
Sample 42 processing time: 30.14 seconds

Processing sample 43/560 - Caption: The image depicts a mathematical operation involving three hexagonal graphs. The first hexagon on the left is labeled with zeros at each vertex and contains dashed lines forming smaller triangles within it. The vertex at the top is labeled as Δ_{1}(1). The middle hexagon is labeled similarly with zeros at each vertex, but it contains solid lines forming a more complex internal structure with additional labels M_{1}(2), M_{1}(4), M_{1}(5), and M_{1}(6) at specific vertices. The operation between the two hexagons is indicated by a multiplication sign (×). The resulting hexagon on the right, labeled with zeros at each vertex, shows a combination of the internal structures from the first two hexagons, forming a more intricate pattern of lines. An equality sign (=) connects the middle and right hexagons, indicating the result of the operation.
TEX Edit Distance for sample 43: 0.8437506556510925
CrystalBLEU Score for sample 43: 0.003948146068766684
Sample 43 processing time: 55.54 seconds

Processing sample 44/560 - Caption: This image depicts a series of geometric transformations and movements. It shows a sequence of squares and circles along a horizontal axis. The squares are rotated 45 degrees and positioned above the circles, with arrows indicating upward movement. The circles are placed along the horizontal axis with dashed circles indicating intermediate positions. Additionally, there are curved arrows below the circles indicating rotational movement. The horizontal axis has tick marks and an arrow pointing to the right, suggesting a positive direction.

To create this image using TikZ, you would need to:
1. Draw the horizontal axis with tick marks and an arrow.
2. Position the circles along the axis, including dashed circles for intermediate positions.
3. Draw the squares rotated at 45 degrees above the circles, with arrows indicating upward movement.
4. Add curved arrows below the circles to indicate rotational movement.
TEX Edit Distance for sample 44: 0.5505056977272034
CrystalBLEU Score for sample 44: 0.004470113921440959
Sample 44 processing time: 43.77 seconds

Processing sample 45/560 - Caption: This image depicts a series of nested and overlapping shapes, each labeled with different terms. Starting from the innermost shape, there are concentric ellipses labeled "stable" and "stable-c" in red. Surrounding these ellipses is a rectangle labeled "PO" and "optimal-c" in blue, and "optimal-cc" in blue. Outside of this rectangle, there is another larger rectangle with diagonal hatching, and outside of this, another rectangle labeled "SWC" on the left and "SWN" on the right, with "LS" at the bottom center. The outermost shape is an ellipse labeled "stable-cc" in red, and the largest rectangle is labeled "CTC" at the top. The labels are color-coded and positioned appropriately to indicate the regions they describe.
TEX Edit Distance for sample 45: 0.807334303855896
CrystalBLEU Score for sample 45: 0.0041960907860662
Sample 45 processing time: 55.47 seconds

Processing sample 46/560 - Caption: This image depicts a flowchart of a neural network training process using automatic differentiation. The flowchart consists of the following components:

1. Two input nodes labeled \( x \) and \( t \) on the left.
2. Two arrows pointing from \( x \) and \( t \) to two separate boxes labeled \( A_u(\theta_u) \) and \( A_{\gamma}(\theta_{\gamma}) \), respectively.
3. The outputs of \( A_u(\theta_u) \) and \( A_{\gamma}(\theta_{\gamma}) \) are connected to a single box containing the equation \( \mathcal{L} = \mathcal{L}_N + \mathcal{L}_B + \mathcal{L}_L + \mathcal{L}_M \).
4. The box containing the loss function \( \mathcal{L} \) has arrows pointing back to the boxes \( A_u(\theta_u) \) and \( A_{\gamma}(\theta_{\gamma}) \) labeled with \( \nabla_{\theta_u} \mathcal{L} \) and \( \nabla_{\theta_{\gamma}} \mathcal{L} \), respectively.
5. The arrows indicating the gradients are labeled "automatic differentiation".

This description should help in writing the TikZ code to create a similar diagram.
TEX Edit Distance for sample 46: 0.5677441358566284
CrystalBLEU Score for sample 46: 0.004517826983697914
Sample 46 processing time: 23.20 seconds

Processing sample 47/560 - Caption: The image shows three different bipartite graphs with 8 vertices each. Each graph is drawn with black vertices and edges, and red edges to highlight specific connections. The vertices are labeled from 1 to 8.

1. The first graph (left) has vertices arranged in two parallel vertical lines, with edges connecting corresponding vertices across the lines (1-2, 3-4, 5-6, 7-8).
2. The second graph (middle) has vertices arranged in two parallel vertical lines, with edges connecting vertices in a crisscross pattern (1-5, 2-6, 3-7, 4-8).
3. The third graph (right) has vertices arranged in a square grid pattern, with edges connecting vertices in a square and diagonal pattern (1-3, 2-4, 3-5, 4-6, 5-7, 6-8, 7-1, 8-2).

This description should help in writing the TikZ code to recreate these graphs.
TEX Edit Distance for sample 47: 0.6555442810058594
CrystalBLEU Score for sample 47: 0.004815695305567765
Sample 47 processing time: 52.32 seconds

Processing sample 48/560 - Caption: This image depicts a schematic diagram of a computational domain divided into three types of cells: Normal Cell, Intermediate Cell, and Structure Cell. The cells are arranged horizontally and labeled as \(x_{i-2}\), \(x_{i-1}\), and \(x_i\) respectively. The height of the cells is denoted by \(h\). 

- The Normal Cell is on the left, shaded in light blue, and has a force \(F_{i-\frac{1}{2}}\) acting to the right.
- The Intermediate Cell is in the middle, shaded in a slightly darker blue, and has forces \(F_{i-\frac{1}{2}}\) and \(F_{i+\frac{1}{2}}\) acting to the right.
- The Structure Cell is on the right, shaded in the darkest blue, and has forces \(F_{i+\frac{1}{2}}\), \(F_{i+\frac{3}{2}}\), and \(F_{i+\frac{5}{2}}\) acting to the right.

Each force is represented by an arrow pointing to the right. The vertical dashed lines indicate the boundaries between the cells. The top and bottom boundaries of the cells are marked with dashed lines extending horizontally. The labels \(z_{i-2}\), \(z_{i-1}\), and \(z_i\) are positioned along the right side of the cells to indicate vertical positions.
TEX Edit Distance for sample 48: 0.5920448899269104
CrystalBLEU Score for sample 48: 0.005734961769362138
Sample 48 processing time: 43.77 seconds

Processing sample 49/560 - Caption: This image is a plot showing the cumulative distribution function (CDF) of position error (err) in meters on a logarithmic scale for both random and directional estimations. The x-axis is labeled "err [m]" and ranges from \(10^{-2}\) to \(10^{-1}\). The y-axis is labeled "Pr(Position error < err)" and ranges from 0 to 1. There are four curves in the plot:

1. A red dashed line representing "random (Est.)".
2. A red solid line representing "random (PEB)".
3. A black dash-dotted line representing "directional (Est.)".
4. A black solid line representing "directional (PEB)".

A legend is present in the upper right corner of the plot, identifying each line style and color with its corresponding label.
TEX Edit Distance for sample 49: 0.9139338731765747
CrystalBLEU Score for sample 49: 0.004987681109481684
Sample 49 processing time: 55.90 seconds

Processing sample 50/560 - Caption: The image depicts a mathematical diagram with a circle centered at the origin and intersecting the x-axis at two points. The x-axis is marked with several labeled points: \(a_1\), \(a_2\), \(b_1\), \(b_2\), \(c_1\), \(c_2\), \(d_1\), and \(d_2\). The points \(a_1\), \(a_2\), \(c_1\), and \(c_2\) are clustered near the origin, while \(b_1\), \(b_2\), \(d_1\), and \(d_2\) are further to the right. The y-axis is also present, intersecting the x-axis at the origin. There is an arrow indicating a counterclockwise direction around the circle. The points on the x-axis are represented as filled circles, and the circle and axes are drawn with solid lines.
TEX Edit Distance for sample 50: 0.5721971392631531
CrystalBLEU Score for sample 50: 0.00532573355595878
Sample 50 processing time: 47.99 seconds

Processing sample 51/560 - Caption: The image depicts a graph with three distinct vertical layers of nodes, each layer containing nodes of a different color. The top layer consists of orange nodes, the middle layer consists of blue nodes, and the bottom layer consists of red nodes. Nodes are connected by black edges, forming a network with connections primarily between adjacent layers. The connections include both vertical and diagonal edges, creating a structured yet interconnected graph.

This description can help in writing the TikZ code by specifying:
1. The positions and colors of the nodes.
2. The connections (edges) between the nodes.
3. The overall layout of the graph.

Here is a sample TikZ code snippet to create a similar graph:

```latex
\documentclass{standalone}
\usepackage{tikz}

\begin{document}
\begin{tikzpicture}

% Define node styles
\tikzstyle{orange node}=[circle, fill=orange, minimum size=6pt, inner sep=0pt]
\tikzstyle{blue node}=[circle, fill=blue, minimum size=6pt, inner sep=0pt]
\tikzstyle{red node}=[circle, fill=red, minimum size=6pt, inner sep=0pt]

% Draw nodes
\node[orange node] (o1) at (0, 6) {};
\node[orange node] (o2) at (0, 5) {};
\node[orange node] (o3) at (0, 4) {};
\node[orange node] (o4) at (0, 3) {};
\node[orange node] (o5) at (1, 4) {};

\node[blue node] (b1) at (0, 2) {};
\node[blue node] (b2) at (0, 1) {};
\node[blue node] (b3) at (1, 2) {};
\node[blue node] (b4) at (1, 1) {};

\node[red node] (r1) at (0, 0) {};
\node[red node] (r2) at (1, 0) {};

% Draw edges
\draw (o1) -- (o2);
\draw (o2) -- (o3);
\draw (o3) -- (o4);
\draw (o4) -- (b1);
\draw (b1) -- (b2);
\draw (b2) -- (r1);
\draw (b4) -- (r2);

\draw (o3) -- (b3);
\draw (o4) -- (b4);
\draw (b1) -- (b3);
\draw (b2) -- (b4);

\draw (o5) -- (b3);
\draw (o5) -- (b4);

\end{tikzpicture}
\end{document}
```

This code will generate a graph similar to the one in the image, with nodes and edges placed accordingly.
TEX Edit Distance for sample 51: 0.6697733998298645
CrystalBLEU Score for sample 51: 0.0053370923881629175
Sample 51 processing time: 14.08 seconds

Processing sample 52/560 - Caption: The image depicts a circle with four angles labeled θ1, θ2, θ3, and θ4 at the center, denoted as Z. Four arrows labeled t1, t2, t3, and t4 radiate outward from the circle in different directions. The arrows t1 and t3 are aligned horizontally, pointing to the right and left respectively, while t2 and t4 are aligned vertically, pointing upwards and downwards respectively. The angles θ1, θ2, θ3, and θ4 are positioned around the center Z, indicating the directions of the arrows.
TEX Edit Distance for sample 52: 0.585644006729126
CrystalBLEU Score for sample 52: 0.005553512333887859
Sample 52 processing time: 16.05 seconds

Processing sample 53/560 - Caption: This image depicts a decision flowchart with three decision nodes and three outcomes. The flowchart is structured as follows:

1. The first decision node is a rectangle with the question: "Has the number of observed unfavorable outcomes exceeded the pre-determined threshold?" 
   - If the answer is "no," the flowchart leads to the outcome "STOP" in red text.
   - If the answer is "yes," it leads to the next decision node.

2. The second decision node is a rectangle with the question: "Is there an acceptable probability that it matches the target product profile?"
   - If the answer is "no," it leads to the outcome "STOP" in red text.
   - If the answer is "yes," it leads to the next decision node.
   - If the evidence is inadequate, it leads to the outcome "Continue" in yellow text.

3. The third decision node is a dashed rectangle with the question: "Does it rank well?"
   - If the answer is "no," it loops back to the second decision node.
   - If the answer is "yes," it leads to the outcome "GO" in green text.

Arrows indicate the flow of decisions, with "yes" and "no" labels guiding the direction.
TEX Edit Distance for sample 53: 0.5018119215965271
CrystalBLEU Score for sample 53: 0.00661845691207427
Sample 53 processing time: 48.41 seconds

Processing sample 54/560 - Caption: This image is a horizontal bar chart depicting the correlation coefficients (ρ) between a dependent variable (ε) and several independent variables (δ). The independent variables listed on the y-axis are "hours per week," "education-num," "Adm-Clerical (Occupation)," "Exec-managerial (Occupation)," "Craft-repair (Occupation)," and "Priv-house-serv (Occupation)." The corresponding correlation coefficients are displayed at the end of each bar: 0.57, 0.54, 0.52, 0.35, 0.31, and -0.29 respectively. The bars are color-coded in blue, with positive correlations extending to the right and the negative correlation extending to the left. The x-axis ranges from -1 to 1, indicating the possible values of the correlation coefficients.
TEX Edit Distance for sample 54: 0.719133734703064
CrystalBLEU Score for sample 54: 0.006826899675792205
Sample 54 processing time: 24.11 seconds

Processing sample 55/560 - Caption: The image depicts a graph with 12 vertices and 14 edges. The vertices are represented as black dots, and the edges are represented as black lines connecting the vertices. The graph has a central polygonal structure with additional vertices and edges extending outward, forming a complex, interconnected shape. The vertices and edges vary in length and orientation, creating an irregular, non-symmetric pattern. This graph could be used to represent a network or a complex geometric structure.
TEX Edit Distance for sample 55: 0.5820298790931702
CrystalBLEU Score for sample 55: 0.0069223870070841366
Sample 55 processing time: 31.29 seconds

Processing sample 56/560 - Caption: The image shows two identical matrices labeled \( M \). Each matrix is a 2x4 matrix with the following elements:

\[
M = \begin{bmatrix}
1 & 2 & 9 & 3 \\
1 & -3 & -6 & 3
\end{bmatrix}
\]

The matrices are displayed one below the other with some vertical spacing between them. The elements of the matrices are typeset in a standard mathematical font.
TEX Edit Distance for sample 56: 0.4960545301437378
CrystalBLEU Score for sample 56: 0.00723365431862444
Sample 56 processing time: 17.42 seconds

Processing sample 57/560 - Caption: This image depicts a block diagram of an attention mechanism in a sequence-to-sequence model. The diagram includes the following components:

1. **Inputs**: 
   - \( A_1, I_1, \ldots, A_n, I_n \) are input pairs, where \( A_i \) and \( I_i \) are processed by a "TE" (presumably a Token Embedding) and "T" (presumably a Transformer) block respectively.
   - The outputs of these blocks are directed into the "attention" block, resulting in \( V_1, K_1, \ldots, V_n, K_n \).

2. **Attention Block**: 
   - The attention block processes the inputs and outputs the attention vectors.

3. **Decoder**:
   - The attention output is fed into a "TD" (presumably a Transformer Decoder) block.
   - The decoder also takes an initial input sequence \( \text{SOS}, a_1^Q, \ldots, a_{m-1}^Q \) and produces the output sequence \( a_1^Q, \ldots, a_m^Q \).

4. **Additional Inputs**:
   - An additional input \( I_Q \) is processed by a "T" block and fed into the attention block.

The diagram uses arrows to indicate the flow of data between these components, and the blocks are labeled with their respective functions.

This caption should help in writing the TikZ code by identifying the key components and their connections.
TEX Edit Distance for sample 57: 0.6421876549720764
CrystalBLEU Score for sample 57: 0.008691365664612816
Sample 57 processing time: 58.09 seconds

Processing sample 58/560 - Caption: The image shows two hexagonal grids, one smaller and one larger, with labeled vertices. The smaller hexagon on the left is filled with a light green color and labeled "σp+1" at the bottom. Its vertices are labeled with the numbers 1 and 2. The larger hexagon on the right, labeled "σk'+p+1" at the bottom, is composed of multiple smaller hexagons. The central hexagon is also filled with light green and labeled with the number 2, while the surrounding hexagons are filled with light yellow and labeled with numbers in the form of "2k'-1", "2k'", "2k'+1", etc. An arrow labeled "k" points from the smaller hexagon to the larger hexagon, indicating a transformation or expansion process.
TEX Edit Distance for sample 58: 0.8155726194381714
CrystalBLEU Score for sample 58: 0.00600747702583519
Sample 58 processing time: 55.87 seconds

Processing sample 59/560 - Caption: The image consists of three squares labeled "A", "B", and "C" from left to right. Each square has a border and contains a letter centered within it. The squares labeled "A" and "C" have a yellow background, while the square labeled "B" has a white background. Each square has a vertical line on both the left and right sides, close to the borders. The squares are evenly spaced horizontally.
TEX Edit Distance for sample 59: 0.5203263163566589
CrystalBLEU Score for sample 59: 0.006112621478690836
Sample 59 processing time: 7.47 seconds

Processing sample 60/560 - Caption: The image shows two diamond-shaped graphs side by side. The graph on the left consists of five vertices labeled \(u_1\), \(u_2\), \(u_3\), \(v\), and \(w\). The vertices \(u_1\), \(u_2\), and \(u_3\) form a triangle with \(u_2\) at the top, and \(v\) and \(w\) are connected to \(u_2\) forming a diamond shape. The graph on the right consists of six vertices labeled \(u_1\), \(u_2\), \(u_3\), \(u_4\), \(v\), and \(w\). The vertices \(u_1\), \(u_2\), \(u_3\), and \(u_4\) form a quadrilateral with \(u_2\) and \(u_3\) in the middle, and \(v\) and \(w\) are connected to \(u_2\) and \(u_3\) forming a more complex diamond shape. Both graphs have thick edges.
TEX Edit Distance for sample 60: 0.6304923295974731
CrystalBLEU Score for sample 60: 0.005776985316155958
Sample 60 processing time: 30.29 seconds

Processing sample 61/560 - Caption: This image depicts a layered directed graph, commonly used to represent a network or a flowchart. The graph consists of multiple layers of nodes connected by directed edges. The structure is as follows:

1. **Nodes**:
   - There is a single starting node labeled "0" on the left.
   - Multiple layers of nodes follow, each layer containing nodes labeled with a combination of "X" and an index (e.g., X_1, X_2, etc.).
   - The final layer contains nodes labeled with "B" and an index (e.g., B_1, B_2, etc.).

2. **Edges**:
   - Directed edges connect nodes from one layer to the next.
   - The starting node "0" has directed edges to the first node in each subsequent layer.
   - Each node in the intermediate layers has directed edges to the next node in the same layer and possibly to nodes in other layers.

3. **Labels**:
   - Nodes are labeled with indices.
   - Edges are labeled with numbers indicating the order or weight of the connection.

4. **Layout**:
   - The graph is organized in a grid-like structure with nodes aligned vertically and horizontally.
   - The edges are mostly straight lines, with some curved or dashed lines indicating different types of connections.

This structure can be implemented in TikZ using nodes for the vertices and arrows for the directed edges, with appropriate labels for each node and edge.
TEX Edit Distance for sample 61: 0.7944754362106323
CrystalBLEU Score for sample 61: 0.005540989096432347
Sample 61 processing time: 55.58 seconds

Processing sample 62/560 - Caption: This image shows five arrows originating from a common point (the origin) and extending outward in different directions. Each arrow is a different color: blue, red, magenta, green, and yellow, arranged in increasing angles from the horizontal axis. The blue arrow is horizontal, the red arrow is at an angle above the blue, followed by the magenta, green, and yellow arrows, each at progressively larger angles from the horizontal axis. The arrows are evenly spaced in terms of their angles.
TEX Edit Distance for sample 62: 0.5609926581382751
CrystalBLEU Score for sample 62: 0.005739163165852424
Sample 62 processing time: 24.55 seconds

Processing sample 63/560 - Caption: The image consists of two distinct shapes on a white background. On the left side, there is a solid red circle with a black outline. On the right side, there is a red sector of a circle (a pie slice) with a black outline. The sector appears to be a quarter circle.

This description can help you write the TikZ code to recreate the image.
TEX Edit Distance for sample 63: 0.5017271041870117
CrystalBLEU Score for sample 63: 0.005809954257048399
Sample 63 processing time: 19.84 seconds

Processing sample 64/560 - Caption: This image depicts a factor graph with two variable nodes and two factor nodes. The variable nodes, labeled \(X_i\) and \(X_j\), are represented as circles, while the factor nodes, labeled \(C_1\) and \(C_2\), are represented as squares. The variable nodes are connected to the factor nodes by edges, forming a diamond-like shape. The factor nodes are shaded in gray, and the variable nodes are not shaded. The edges connecting the nodes are solid lines.

This description should help in writing the TikZ code to recreate this figure.
TEX Edit Distance for sample 64: 0.4626060128211975
CrystalBLEU Score for sample 64: 0.006075370209921203
Sample 64 processing time: 19.76 seconds

Processing sample 65/560 - Caption: The image shows a semicircle with a black outline positioned above a red line. The semicircle is oriented with its flat edge on top and its curved edge on the bottom. The red line extends from the center of the semicircle's flat edge and continues downward at an angle to the left. The background is a light yellow color.

This description can help in writing the TikZ code by specifying the semicircle's position and orientation, the starting point and angle of the red line, and the background color.
TEX Edit Distance for sample 65: 0.5275937914848328
CrystalBLEU Score for sample 65: 0.006153186614674914
Sample 65 processing time: 11.28 seconds

Processing sample 66/560 - Caption: This image depicts a geometric representation of two vectors, \( \hat{A}_d \) and \( \hat{A}_u \), in a coordinate system with axes labeled \( \sigma_1 \) and \( \sigma_3 \). The vectors form two shaded regions: a blue region above the \( \hat{A}_d \) vector and an orange region below it. The angle between \( \hat{A}_d \) and \( \hat{A}_u \) is labeled \( \theta_d \), and the angle between the vector \( \hat{A}_d \times \hat{A}_u \) and the horizontal axis is labeled \( 2\theta_c \). The intersection of the vectors is labeled \( c_{1}\sigma_{2} \sim \hat{A}_d \times \hat{A}_u \). The points \( c_R \), \( X \), and the conditions \( |\Delta C| = 1 \) and \( |\Delta S| = 1 \) are also marked. The vectors and angles are clearly indicated, and the shaded regions are distinguished by different colors.
TEX Edit Distance for sample 66: 0.8095571398735046
CrystalBLEU Score for sample 66: 0.005873035745794038
Sample 66 processing time: 55.57 seconds

Processing sample 67/560 - Caption: This image depicts a grid network with horizontal and vertical lines intersecting at nodes. The nodes are represented by black dots. There are two horizontal cuts labeled "Cut A" and "Cut B" at the bottom and top of the grid, respectively. A rectangular box is centered around one of the nodes, labeled \( n_i(t) \). Four nodes adjacent to the central node are labeled with the Greek letter \( \alpha \). The grid lines are thicker at the edges near the cuts and thinner elsewhere.

To create this image using TikZ, you would need to:
1. Draw a grid of lines.
2. Highlight certain nodes with black dots.
3. Label specific nodes with \( \alpha \).
4. Draw a rectangular box around a central node and label it \( n_i(t) \).
5. Add labels "Cut A" and "Cut B" at the appropriate positions on the grid.
TEX Edit Distance for sample 67: 0.8201475739479065
CrystalBLEU Score for sample 67: 0.005496669144139722
Sample 67 processing time: 55.43 seconds

Processing sample 68/560 - Caption: This image is a line plot depicting the relationship between the standard deviation (σ) on the x-axis and the average number of steps until convergence (scaled by 10^4) on the y-axis. The plot includes three lines representing different conditions: "resent" (blue), "resent+apprec" (orange), and "apprec" (green). The legend is placed inside the plot area, in the upper left quadrant. The x-axis ranges from 0 to 100, and the y-axis ranges from 0 to 3.5 (scaled by 10^4).
TEX Edit Distance for sample 68: 0.8326359987258911
CrystalBLEU Score for sample 68: 0.0053699209123265115
Sample 68 processing time: 55.79 seconds

Processing sample 69/560 - Caption: This image depicts a star polygon (pentagram) inscribed in a circle. The vertices of the pentagram are labeled with the numbers 1, 2, 3, 4, and 5. The intersections of the lines forming the pentagram are labeled with the numbers 14, 13, 24, 35, and 25. The points on the circle that are not vertices of the pentagram are labeled with the numbers 12, 23, 34, 45, and 15. All labels are enclosed in small circles. The entire figure is symmetric and centered around the number 25, which is at the center of the pentagram.
TEX Edit Distance for sample 69: 0.625
CrystalBLEU Score for sample 69: 0.0055253832105630445
Sample 69 processing time: 31.90 seconds

Processing sample 70/560 - Caption: This image is a bubble chart that represents the relationship between different property keyword categories and the properties optimized. The x-axis is labeled "Property optimised" with categories: Time, Size, Energy, Other, and Memory. The y-axis is labeled "Property keyword category" with categories: Time, Energy, Quality, Memory, and Other. Each bubble's size represents the magnitude of the relationship, and the color indicates different categories, with larger red bubbles indicating a stronger relationship and smaller yellow bubbles indicating a weaker relationship. The grid lines and points are shown to help align the bubbles accurately.
TEX Edit Distance for sample 70: 0.8871462345123291
CrystalBLEU Score for sample 70: 0.005161313771280069
Sample 70 processing time: 55.68 seconds

Processing sample 71/560 - Caption: This image depicts a mathematical diagram with a number line representing the x-axis. The x-axis is divided into segments with labels at specific points: \( x_{-3/2} \), \( L_{-2} \), \( x_{-1/2} \), \( L_{-1} \), \( x_{1/2} \), \( I_0 \), \( x_{3/2} \), \( I_1 \), and \( x_2 \). Above the x-axis, there are several labeled points: \( Q_{-2} \), \( Q_{-1} \), \( Q_R \), \( Q_L \), \( Q_0 \), and \( Q_1 \). The diagram includes two partial differential equations on either side of a vertical dashed line, with terms involving \( \partial Q \), \( S_1 \), \( S_2 \), \( \epsilon \), \( F_1(U) \), \( F_2(U) \), and \( V \). At the top, there is a condition \( \Psi(Q_R, Q_L) = 0 \).
TEX Edit Distance for sample 71: 0.8069939613342285
CrystalBLEU Score for sample 71: 0.004923114751835928
Sample 71 processing time: 55.52 seconds

Processing sample 72/560 - Caption: The image shows a diagram with a centered text box at coordinates (0,2) containing the text "centered text at (0/2)". Below the text box, there is a vertical dashed line extending downwards. At the bottom of the dashed line, there are two horizontal lines extending left and right, each ending with a letter: "L" on the left and "R" on the right. The horizontal lines are solid and the vertical dashed line intersects them at their midpoint.
TEX Edit Distance for sample 72: 0.4154634475708008
CrystalBLEU Score for sample 72: 0.005064856356178185
Sample 72 processing time: 13.32 seconds

Processing sample 73/560 - Caption: This image depicts a Venn diagram with five overlapping regions, each labeled with a Greek letter. The diagram consists of four ellipses, each representing a different set. The labels are as follows:

- The leftmost ellipse is labeled with "λ".
- The topmost ellipse is labeled with "ν₃".
- The middle ellipse is labeled with "ν₁".
- The bottom ellipse is labeled with "ν₂".
- The rightmost ellipse is labeled with "μ".
- The outermost region, which encompasses all the ellipses, is labeled with "ν₄".

The ellipses intersect in such a way that they create multiple overlapping regions, each representing the intersection of different sets. The labels are placed within the respective ellipses and intersections to indicate the different subsets formed by the overlaps.
TEX Edit Distance for sample 73: 0.5462366938591003
CrystalBLEU Score for sample 73: 0.005768851394366011
Sample 73 processing time: 36.90 seconds

Processing sample 74/560 - Caption: This image depicts a labeled tree diagram with nodes and edges. The central node is labeled \( \rho' \) and branches out in multiple directions. The diagram includes the following key features:

1. **Horizontal Branch**:
   - Extends left and right from \( \rho' \).
   - Left nodes: \( \epsilon \), \( \sigma \), and \( \text{id}_N \).
   - Right nodes: \( \epsilon \rho' \), \( \sigma \alpha_2 \), \( \epsilon \alpha_2 \), and \( \alpha_2 \).

2. **Vertical Branch**:
   - Extends upwards and downwards from \( \rho' \).
   - Upward nodes: \( \epsilon \alpha_1' \), \( \sigma \alpha_1' \), and \( \alpha_1' \).
   - Downward nodes: \( \epsilon \alpha_4' \), \( \sigma \alpha_4' \), and \( \alpha_4' \).

3. **Diagonal Branches**:
   - One branch extends diagonally upwards to the right from \( \rho' \) with nodes \( \epsilon \alpha_2' \), \( \sigma \alpha_2' \), and \( \alpha_2' \).
   - Another branch extends diagonally downwards to the right from \( \rho' \) with nodes \( \epsilon \alpha_3 \), \( \sigma \alpha_3 \), and \( \alpha_3 \).

4. **Node Labels**:
   - Each node is labeled with a specific notation such as \( \epsilon \), \( \sigma \), \( \alpha \), and combinations thereof.

This structure can be used to write the TikZ code by defining the coordinates for each node and drawing the connecting lines accordingly.
TEX Edit Distance for sample 74: 0.5397517681121826
CrystalBLEU Score for sample 74: 0.006317935508133661
Sample 74 processing time: 44.77 seconds

Processing sample 75/560 - Caption: The image depicts a plot with three shaded regions, each labeled with numbers 1, 2, and 3. The regions are bounded by red lines, and the shading intensity increases from region 1 to region 3. A horizontal blue line intersects the plot at y = 1.5. The x-axis ranges from 0 to 8, and the y-axis ranges from 0 to 3. The red lines appear to be linear boundaries that converge towards the origin, creating the distinct regions. The plot is likely representing a feasible region or solution space for a set of inequalities.
TEX Edit Distance for sample 75: 0.6229939460754395
CrystalBLEU Score for sample 75: 0.006544410647310897
Sample 75 processing time: 30.29 seconds

Processing sample 76/560 - Caption: The image depicts a 3D plot of a series of concentric ellipsoids centered at the origin. The ellipsoids are shaded in varying shades of red, with the innermost ellipsoid being the darkest and the outermost being the lightest. There are three blue arrows representing the x, y, and z axes, originating from the center of the ellipsoids and extending outward in positive and negative directions. The arrows are labeled with arrowheads, indicating the direction of the axes. The overall visualization suggests a symmetrical distribution around the origin.
TEX Edit Distance for sample 76: 0.6538299918174744
CrystalBLEU Score for sample 76: 0.006791072521483706
Sample 76 processing time: 20.18 seconds

Processing sample 77/560 - Caption: The image depicts a geometric structure resembling a truncated icosahedron, with a blue fill color and red edges. The structure consists of a combination of hexagons and pentagons. The outer boundary is formed by a series of red edges, while the inner polygons are outlined in a lighter shade. The overall shape is spherical, with the polygons fitting together in a symmetrical pattern.

This description should help in writing the TikZ code by focusing on the following elements:
1. **Nodes and Coordinates**: Define the coordinates for the vertices of the polygons.
2. **Polygons**: Use `\filldraw` or `\draw` commands to create the hexagons and pentagons.
3. **Coloring**: Apply blue fill for the polygons and red for the edges.
4. **Symmetry**: Ensure the polygons are arranged symmetrically to form the spherical shape.

Here is a basic structure of the TikZ code to get started:

```latex
\documentclass{standalone}
\usepackage{tikz}
\begin{document}
\begin{tikzpicture}

% Define the coordinates for the vertices
% Example coordinates, adjust as needed
\coordinate (A) at (0,0);
\coordinate (B) at (1,0);
\coordinate (C) at (1.5,0.87);
\coordinate (D) at (1,1.73);
\coordinate (E) at (0,1.73);
\coordinate (F) at (-0.5,0.87);

% Draw the polygons
\filldraw[fill=blue, draw=red] (A) -- (B) -- (C) -- (D) -- (E) -- (F) -- cycle;

% Repeat for other polygons
% Add more coordinates and draw commands to complete the structure

\end{tikzpicture}
\end{document}
```

This code provides a starting point. You will need to add more coordinates and `\filldraw` commands to complete the entire structure as shown in the image.
TEX Edit Distance for sample 77: 0.6740208268165588
CrystalBLEU Score for sample 77: 0.006179016222061867
Sample 77 processing time: 46.46 seconds

Processing sample 78/560 - Caption: The image depicts a graph composed of three main parts: a square on the left, a hexagon on the right, and a linear chain of five vertices connecting them. Each vertex is represented by a black dot, and edges are represented by straight lines connecting the vertices. The square and hexagon are regular polygons, and the linear chain connects the rightmost vertex of the square to the leftmost vertex of the hexagon.
TEX Edit Distance for sample 78: 0.47601422667503357
CrystalBLEU Score for sample 78: 0.006505427693434245
Sample 78 processing time: 34.80 seconds

Processing sample 79/560 - Caption: The image depicts a cylindrical coordinate system with a cylindrical object oriented in 3D space. The cylinder is tilted and labeled with points and vectors. The key elements are:

1. The cylinder is tilted and has a shaded surface.
2. The cylinder has three orthogonal vectors originating from its surface labeled as **p**, **q**, and **r**.
3. The cylinder is intersected by a vector **U** at an angle **θ** from the vertical axis (z-axis).
4. An angle **φ** is marked between the vector **U** and the horizontal projection of the cylinder.
5. The global coordinate system is shown with **x**, **y**, and **z** axes.
6. A vector **g** is pointing downward, indicating the direction of gravity.

This description should help in writing the TikZ code to recreate the figure.
TEX Edit Distance for sample 79: 0.5061004757881165
CrystalBLEU Score for sample 79: 0.006836103257373386
Sample 79 processing time: 31.10 seconds

Processing sample 80/560 - Caption: The image shows a single straight line with three parallel hash marks crossing it perpendicularly at the center. The line is drawn at an angle, not horizontal or vertical. The hash marks are evenly spaced and shorter than the main line. This figure can be used to represent a specific type of electrical component or a graphical notation in a diagram.

To create this in TikZ, you would draw a line and then add three shorter perpendicular lines at the center of the main line.
TEX Edit Distance for sample 80: 0.5636667609214783
CrystalBLEU Score for sample 80: 0.007148054148250799
Sample 80 processing time: 25.87 seconds

Processing sample 81/560 - Caption: This image depicts a complex directed graph with multiple nodes and edges. The nodes are labeled with combinations of letters and numbers, such as "1a", "2b", "3c", etc. The edges are directed, indicated by arrows, and are labeled with letters such as "a", "b", "c". The graph includes various loops, cycles, and connections between nodes, forming a network of paths. Some edges are thicker, suggesting a higher weight or importance. The overall structure appears to be intricate, with nodes connected in a non-linear, web-like fashion.

To write the TikZ code for this image, you would need to:
1. Define the nodes with their respective labels.
2. Draw directed edges between the nodes with appropriate labels.
3. Ensure some edges are thicker to indicate higher weight or importance.
4. Arrange the nodes in a way that matches the layout of the graph.

Here is a basic template to get you started with the TikZ code:

```latex
\documentclass{standalone}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}

\begin{document}
\begin{tikzpicture}[->, >=Stealth, node distance=2cm, thick]

% Define nodes
\node (1a) {1a};
\node (2b) [right of=1a] {2b};
\node (3c) [below of=1a] {3c};
% Add more nodes as needed

% Define edges
\draw (1a) -- (2b) node[midway, above] {a};
\draw (2b) -- (3c) node[midway, right] {b};
\draw (3c) -- (1a) node[midway, left] {c};
% Add more edges as needed

% Thicker edges
\draw[very thick] (1a) -- (3c) node[midway, left] {a};

\end{tikzpicture}
\end{document}
```

You will need to expand this template by adding all the nodes and edges as per the graph in the image. Adjust the positions and distances to match the layout accurately.
TEX Edit Distance for sample 81: 0.6850156784057617
CrystalBLEU Score for sample 81: 0.004540625407375726
Sample 81 processing time: 57.28 seconds

Processing sample 82/560 - Caption: The image consists of two subfigures labeled (a) and (b). 

Subfigure (a) depicts a circular diagram divided into four regions by lines radiating from the center. The regions are labeled as follows: the top region is labeled "B", the left region is labeled "A", the bottom region is labeled "B", and the right region is labeled "C".

Subfigure (b) shows a series of concentric circles with three points labeled on the outermost circle: "A'" on the left, "C'" on the right, and "B'" in the center. There is a vertical blue line passing through the center of the circles, connecting points "A'" and "C'".

To create this figure using TikZ, you will need to use commands for drawing circles, lines, and labeling specific points.
TEX Edit Distance for sample 82: 0.5662972927093506
CrystalBLEU Score for sample 82: 0.004626915663865589
Sample 82 processing time: 21.79 seconds

Processing sample 83/560 - Caption: The image depicts a sequence of nodes connected by directed arrows, representing a flow from one node to another. Each node is labeled with a variable (e.g., \( y_i \), \( y_{i-1} \), \( x_j \), \( y_{i-1} \)) and is enclosed in a dashed red circle. The arrows between the nodes are labeled with mathematical symbols (e.g., \( \sigma_1 \), \( \sigma_1^{-1} \), \( \sigma_0^{-1} \), \( \sigma_1 \), \( x_j^{-1} \), \( \sigma_0^{-1} \)). Below each node, there is a label indicating the column position (e.g., "2nd column", "1st column", "3rd column"). The nodes and arrows are arranged horizontally, with arrows pointing from left to right.

This description can help in writing the TikZ code by identifying the nodes, their labels, the arrows connecting them, and the corresponding labels for the arrows and columns.
TEX Edit Distance for sample 83: 0.5224553346633911
CrystalBLEU Score for sample 83: 0.004820587772007734
Sample 83 processing time: 32.91 seconds

Processing sample 84/560 - Caption: This image represents a grid with labeled points and curves intersecting at a central point. The grid is composed of squares with side lengths labeled as \(a_i\) and \(b_i\) (where \(i\) ranges from 1 to 4). The horizontal lines are labeled \(a_1, a_2, a_3, a_4\) from top to bottom, and the vertical lines are labeled \(b_1, b_2, b_3, b_4\) from left to right. The central intersection point is labeled \(X_{a2}\). Two curves, labeled \(\mathcal{L}_1\) and \(\mathcal{L}_2\), intersect at \(X_{a2}\). The curves extend through the grid, bending as they pass through the intersections of the grid lines.

To generate this figure using TikZ, you would need to:
1. Create a grid with labeled intersections.
2. Draw the curves \(\mathcal{L}_1\) and \(\mathcal{L}_2\) intersecting at \(X_{a2}\).
3. Label the grid lines and the intersection point.

Here is a possible TikZ code structure to start with:

```latex
\documentclass{standalone}
\usepackage{tikz}
\begin{document}
\begin{tikzpicture}

% Draw the grid
\foreach \x in {1,2,3,4} {
    \foreach \y in {1,2,3,4} {
        \draw (\x, \y) -- (\x+1, \y);
        \draw (\x, \y) -- (\x, \y+1);
    }
}

% Label the grid lines
\node at (1.5, 4.5) {$a_1$};
\node at (2.5, 4.5) {$a_2$};
\node at (3.5, 4.5) {$a_3$};
\node at (4.5, 4.5) {$a_4$};

\node at (0.5, 3.5) {$b_1$};
\node at (0.5, 2.5) {$b_2$};
\node at (0.5, 1.5) {$b_3$};
\node at (0.5, 0.5) {$b_4$};

% Draw the curves
\draw[thick] (1,4) .. controls (2,3) and (3,2) .. (4,1);
\draw[thick] (1,1) .. controls (2,2) and (3,3) .. (4,4);

% Label the intersection point and curves
\node at (2.5, 2.5) {$X_{a2}$};
\node at (1.5, 3.5) {$\mathcal{L}_1$};
\node at (3.5, 1.5) {$\mathcal{L}_2$};

\end{tikzpicture}
\end{document}
```

This code provides a basic structure to create the grid and curves, and you can adjust the positions and labels as needed to match the image more closely.
TEX Edit Distance for sample 84: 0.8135690689086914
CrystalBLEU Score for sample 84: 0.004718804402388499
Sample 84 processing time: 55.47 seconds

Processing sample 85/560 - Caption: This image is a scatter plot with a red line connecting the first two data points. The x-axis is labeled "Zeit" (Time) and ranges from 0 to 180 in increments of 15. The y-axis is labeled "Extinktion" (Extinction) and ranges from 1.272 to 1.29 in increments of 0.002. The data points are represented by black dots. The red line starts at the point (15, 1.282) and ends at the point (45, 1.278). The plot has a grid with both vertical and horizontal lines.
TEX Edit Distance for sample 85: 0.7994503378868103
CrystalBLEU Score for sample 85: 0.004668333700929729
Sample 85 processing time: 55.45 seconds

Processing sample 86/560 - Caption: This image is a flow diagram illustrating the process of solving a MOMDP (Multi-Objective Markov Decision Process) using an algorithm. The diagram is divided into two phases: the "planning or learning phase" and the "execution phase." 

1. On the left side, there is a label "MOMDP + utility function."
2. An arrow points from this label to an oval labeled "algorithm," indicating that the algorithm processes the MOMDP and utility function.
3. The oval is enclosed in a dashed box representing the "planning or learning phase."
4. A dotted vertical line separates the "planning or learning phase" from the "execution phase."
5. An arrow extends from the oval to the right side, pointing to a label "single solution," indicating the output of the algorithm.
6. The "execution phase" label is placed below the arrow pointing to the "single solution."

This description should help you write the corresponding TikZ code for this flow diagram.
TEX Edit Distance for sample 86: 0.6589375138282776
CrystalBLEU Score for sample 86: 0.004717410430798921
Sample 86 processing time: 8.87 seconds

Processing sample 87/560 - Caption: The image depicts a directed graph with six nodes labeled A, B, C, D, E, and F arranged in a circular layout. Each pair of adjacent nodes is connected by two directed edges forming a bidirectional connection. The edges between the nodes are curved, creating an elliptical shape around each pair of connected nodes. The graph forms a closed loop, with the following pairs of nodes connected: (A, B), (B, C), (C, D), (D, E), (E, F), and (F, A).
TEX Edit Distance for sample 87: 0.623722493648529
CrystalBLEU Score for sample 87: 0.004893584046014397
Sample 87 processing time: 26.89 seconds

Processing sample 88/560 - Caption: This image depicts a bipartite graph with two sets of nodes, represented by two ellipses labeled \(J(L)\) and \(M(L)\). The nodes within these ellipses are connected to nodes outside the ellipses. The nodes are connected by edges, some of which are colored blue and red. The nodes \(j(a)\), \(j(b)\), \(m(a)\), and \(m(b)\) are positioned outside the ellipses, with \(j(a)\) and \(j(b)\) on the left and \(m(a)\) and \(m(b)\) on the right. The top and bottom nodes are colored red, while the left and right nodes are colored blue. The connections between the nodes form a network, with the ellipses representing subsets of the nodes. The central region between the ellipses is labeled \(R\).
TEX Edit Distance for sample 88: 0.6396898627281189
CrystalBLEU Score for sample 88: 0.004958250431066687
Sample 88 processing time: 22.81 seconds

Processing sample 89/560 - Caption: The image displays two horizontally aligned rectangles with rounded corners. Both rectangles have a red border and no fill color. The rectangles are separated by a small gap. The dimensions of the rectangles are identical.
TEX Edit Distance for sample 89: 0.6372623443603516
CrystalBLEU Score for sample 89: 0.0049660432422893586
Sample 89 processing time: 5.08 seconds

Processing sample 90/560 - Caption: The image consists of three horizontal bar charts, each representing different approaches to workload distribution over a series of days. 

1. **Demand patterns**: The first chart shows the demand patterns for four workers (Worker 1, Worker 2, Worker 3, Worker 4) over six days. Each worker's workload is represented by a distinct color (cyan, purple, orange, pink). Overloading and underloading are indicated by striped patterns in red and green, respectively. Performance decrease is marked by a black border around the bars.

2. **Classic approach**: The second chart illustrates the classic approach to workload distribution over the same period. The same color scheme and patterns are used to represent the workers and their workload statuses.

3. **Behavior-aware approach**: The third chart displays the behavior-aware approach to workload distribution, again over the same period and using the same color scheme and patterns.

Each chart has a legend at the top that explains the colors and patterns used. The x-axis represents the days, and the y-axis represents the workload intensity. The right side of each chart has a color-coded key indicating the risk levels associated with each approach.

To write the TikZ code for this figure, you would need to create three separate bar charts with the specified colors and patterns, ensuring to include the legend and the risk indicator keys.
TEX Edit Distance for sample 90: 0.6196715235710144
CrystalBLEU Score for sample 90: 0.004942753288038235
Sample 90 processing time: 30.86 seconds

Processing sample 91/560 - Caption: This image represents a grid with labeled points and curves intersecting at a central point. The grid is composed of squares with side lengths labeled as \(a_i\) and \(b_i\) (where \(i\) ranges from 1 to 4). The horizontal lines are labeled \(a_1, a_2, a_3, a_4\) from top to bottom, and the vertical lines are labeled \(b_1, b_2, b_3, b_4\) from left to right. The central intersection point is labeled \(X_{a2}\). Two curves, labeled \(\mathcal{L}_1\) and \(\mathcal{L}_2\), intersect at \(X_{a2}\). The curves extend through the grid, bending as they pass through the intersections of the grid lines.

To generate this figure using TikZ, you would need to:
1. Create a grid with labeled intersections.
2. Draw the curves \(\mathcal{L}_1\) and \(\mathcal{L}_2\) intersecting at \(X_{a2}\).
3. Label the grid lines and the intersection point.

Here is a possible TikZ code structure to start with:

```latex
\documentclass{standalone}
\usepackage{tikz}
\begin{document}
\begin{tikzpicture}

% Draw the grid
\foreach \x in {1,2,3,4} {
    \foreach \y in {1,2,3,4} {
        \draw (\x, \y) -- (\x+1, \y);
        \draw (\x, \y) -- (\x, \y+1);
    }
}

% Label the grid lines
\node at (1.5, 4.5) {$a_1$};
\node at (2.5, 4.5) {$a_2$};
\node at (3.5, 4.5) {$a_3$};
\node at (4.5, 4.5) {$a_4$};

\node at (0.5, 3.5) {$b_1$};
\node at (0.5, 2.5) {$b_2$};
\node at (0.5, 1.5) {$b_3$};
\node at (0.5, 0.5) {$b_4$};

% Draw the curves
\draw[thick] (1,4) .. controls (2,3) and (3,2) .. (4,1);
\draw[thick] (1,1) .. controls (2,2) and (3,3) .. (4,4);

% Label the intersection point and curves
\node at (2.5, 2.5) {$X_{a2}$};
\node at (1.5, 3.5) {$\mathcal{L}_1$};
\node at (3.5, 1.5) {$\mathcal{L}_2$};

\end{tikzpicture}
\end{document}
```

This code provides a basic structure to create the grid and curves, and you can adjust the positions and labels as needed to match the image more closely.
TEX Edit Distance for sample 91: 0.6540915966033936
CrystalBLEU Score for sample 91: 0.005212370731788176
Sample 91 processing time: 40.84 seconds

Processing sample 92/560 - Caption: This image is a line plot comparing the number of nonlinear iterations (y-axis) against the degrees of freedom (x-axis) on a logarithmic scale. The x-axis ranges from \(10^4\) to \(10^6\), and the y-axis ranges from 10 to 25. There are two lines: one in red representing "PPGD \(P_1\)" and one in blue representing "PPGD \(P_2\)". The legend is located in the upper right corner of the plot. The red line has square markers, and the blue line has circular markers. Both lines show a general upward trend as the degrees of freedom increase.
TEX Edit Distance for sample 92: 0.8155043721199036
CrystalBLEU Score for sample 92: 0.005145673817674053
Sample 92 processing time: 55.45 seconds

Processing sample 93/560 - Caption: The image depicts a bipartite graph with multiple layers of nodes connected by edges. The graph consists of three main columns of nodes:

1. The first column contains nodes labeled \( s_1^{(1)}, s_2^{(1)}, \ldots, s_W^{(1)} \).
2. The second column contains nodes labeled \( s_1^{(2)}, s_2^{(2)}, \ldots, s_W^{(2)} \).
3. The third column contains nodes labeled \( s_{T-1}^{(1)}, s_{T-1}^{(2)}, \ldots, s_{T-1}^{(W)} \).

The nodes in the first column are connected to the nodes in the second column with solid lines, indicating direct connections. The nodes in the second column are connected to the nodes in the third column with dashed lines, indicating a different type of connection. The third column nodes are connected to a single node labeled \( \emptyset \) with solid lines.

The connections between the nodes in the first and second columns are dense, suggesting a fully connected bipartite graph, while the connections between the nodes in the second and third columns are sparse, indicated by dashed lines.

This description should help in writing the TikZ code to create this figure.
TEX Edit Distance for sample 93: 0.5679559111595154
CrystalBLEU Score for sample 93: 0.00538732904803421
Sample 93 processing time: 40.87 seconds

Processing sample 94/560 - Caption: This image is a plot with a vertical axis labeled \(v\) and a horizontal axis labeled \(p\). The vertical axis has several marked points: \(\epsilon\) at the bottom, \(1/\epsilon^2\) in the middle, and \(v_{\text{max}}\) at the top. The horizontal axis has marked points: \(\epsilon^{20}\) near the origin and 1 at the far right. 

There are three horizontal dashed lines: one at \(\epsilon\), one at \(1/\epsilon^2\), and one at \(v_{\text{max}}\). The region between the horizontal lines at \(\epsilon\) and \(1/\epsilon^2\) is shaded. 

A vertical dashed line is drawn at \(p = \epsilon^{20}\), intersecting the horizontal dashed line at \(\epsilon\). The intersection is labeled \((1 + \epsilon)^k = v_i\).

There are two sets of points labeled \(X_{v_i}\) and \(X_{v_i}^+\) on the horizontal line at \(1/\epsilon^2\). The first set \(X_{v_i}\) includes points \(Y_2, Y_3, Y_4, Y_5, Y_6\), and the second set \(X_{v_i}^+\) includes points \(Y_4, Y_5, Y_6, Y_7, Y_8, Y_9\). The points \(X_{v_i}\) and \(X_{v_i}^+\) are enclosed in curly braces.

This description should help in writing the TikZ code to replicate the figure.
TEX Edit Distance for sample 94: 0.6285526156425476
CrystalBLEU Score for sample 94: 0.0055202155516595654
Sample 94 processing time: 15.62 seconds

Processing sample 95/560 - Caption: The image shows two separate line plots, each with blue line segments and points. Both plots share the same x-axis labels, ranging from \(v_0 - 2\sqrt{\epsilon}\) to \(v_0 + 2\sqrt{\epsilon}\), and y-axis labels, ranging from \(2v_0 - 4\sqrt{\epsilon}\) to \(2v_0 + 4\sqrt{\epsilon}\). The left plot contains a single line segment connecting two points, while the right plot contains three line segments connecting four points. The points and lines are plotted in blue.
TEX Edit Distance for sample 95: 0.8271486759185791
CrystalBLEU Score for sample 95: 0.005324337994927688
Sample 95 processing time: 55.42 seconds

Processing sample 96/560 - Caption: The image is a state diagram for a remote controller. It consists of seven states: "Init," "read_LC_on," "check_LC_on_count," "read_LC_off," "check_LC_off_count," "check_data," and "read_data." The transitions between these states are labeled with conditions such as "pos_edge," "data=0," "LC_on counter out of range," "LC_off counter out of range," "data_counter = 31," and "data_counter /= 31." The initial state is "Init," and the diagram includes loops and transitions between states, indicating the flow of the state machine. The states are represented by circles, and the transitions are represented by arrows connecting these circles.
TEX Edit Distance for sample 96: 0.7791343927383423
CrystalBLEU Score for sample 96: 0.005190032525994617
Sample 96 processing time: 56.89 seconds

Processing sample 97/560 - Caption: The image depicts a graph with three distinct vertical layers of nodes, each layer containing nodes of a different color. The top layer consists of orange nodes, the middle layer consists of blue nodes, and the bottom layer consists of red nodes. Nodes are connected by black edges, forming a network with connections primarily between adjacent layers. The connections include both vertical and diagonal edges, creating a structured yet interconnected graph.

This description can help in writing the TikZ code by specifying:
1. The positions and colors of the nodes.
2. The connections (edges) between the nodes.
3. The overall layout of the graph.

Here is a sample TikZ code snippet to create a similar graph:

```latex
\documentclass{standalone}
\usepackage{tikz}

\begin{document}
\begin{tikzpicture}

% Define node styles
\tikzstyle{orange node}=[circle, fill=orange, minimum size=6pt, inner sep=0pt]
\tikzstyle{blue node}=[circle, fill=blue, minimum size=6pt, inner sep=0pt]
\tikzstyle{red node}=[circle, fill=red, minimum size=6pt, inner sep=0pt]

% Draw nodes
\node[orange node] (o1) at (0, 6) {};
\node[orange node] (o2) at (0, 5) {};
\node[orange node] (o3) at (0, 4) {};
\node[orange node] (o4) at (0, 3) {};
\node[orange node] (o5) at (1, 4) {};

\node[blue node] (b1) at (0, 2) {};
\node[blue node] (b2) at (0, 1) {};
\node[blue node] (b3) at (1, 2) {};
\node[blue node] (b4) at (1, 1) {};

\node[red node] (r1) at (0, 0) {};
\node[red node] (r2) at (1, 0) {};

% Draw edges
\draw (o1) -- (o2);
\draw (o2) -- (o3);
\draw (o3) -- (o4);
\draw (o4) -- (b1);
\draw (b1) -- (b2);
\draw (b2) -- (r1);
\draw (b4) -- (r2);

\draw (o3) -- (b3);
\draw (o4) -- (b4);
\draw (b1) -- (b3);
\draw (b2) -- (b4);

\draw (o5) -- (b3);
\draw (o5) -- (b4);

\end{tikzpicture}
\end{document}
```

This code will generate a graph similar to the one in the image, with nodes and edges placed accordingly.
slurmstepd: error: *** JOB 9451017 ON cn267 CANCELLED AT 2024-11-17T14:09:48 ***
