Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:01,  3.74it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:00<00:00,  5.39it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:00<00:00,  6.29it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:00<00:00,  6.78it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  7.58it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  6.60it/s]
Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:23<03:06, 23.37s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:46<02:41, 23.13s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [01:09<02:17, 22.99s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [01:33<01:56, 23.40s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [01:55<01:32, 23.10s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [02:18<01:09, 23.12s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [02:41<00:45, 22.99s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [03:01<00:21, 21.93s/it]Loading checkpoint shards: 100%|██████████| 9/9 [03:16<00:00, 19.91s/it]Loading checkpoint shards: 100%|██████████| 9/9 [03:16<00:00, 21.86s/it]
Some weights of the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant were not used when initializing MllamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.cross_attn.q_proj.base_layer.weight', 'model.layers.13.cross_attn.q_proj.lora_A.default.weight', 'model.layers.13.cross_attn.q_proj.lora_B.default.weight', 'model.layers.13.cross_attn.v_proj.base_layer.weight', 'model.layers.13.cross_attn.v_proj.lora_A.default.weight', 'model.layers.13.cross_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.cross_attn.q_proj.base_layer.weight', 'model.layers.18.cross_attn.q_proj.lora_A.default.weight', 'model.layers.18.cross_attn.q_proj.lora_B.default.weight', 'model.layers.18.cross_attn.v_proj.base_layer.weight', 'model.layers.18.cross_attn.v_proj.lora_A.default.weight', 'model.layers.18.cross_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.cross_attn.q_proj.base_layer.weight', 'model.layers.23.cross_attn.q_proj.lora_A.default.weight', 'model.layers.23.cross_attn.q_proj.lora_B.default.weight', 'model.layers.23.cross_attn.v_proj.base_layer.weight', 'model.layers.23.cross_attn.v_proj.lora_A.default.weight', 'model.layers.23.cross_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.cross_attn.q_proj.base_layer.weight', 'model.layers.28.cross_attn.q_proj.lora_A.default.weight', 'model.layers.28.cross_attn.q_proj.lora_B.default.weight', 'model.layers.28.cross_attn.v_proj.base_layer.weight', 'model.layers.28.cross_attn.v_proj.lora_A.default.weight', 'model.layers.28.cross_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.cross_attn.q_proj.base_layer.weight', 'model.layers.3.cross_attn.q_proj.lora_A.default.weight', 'model.layers.3.cross_attn.q_proj.lora_B.default.weight', 'model.layers.3.cross_attn.v_proj.base_layer.weight', 'model.layers.3.cross_attn.v_proj.lora_A.default.weight', 'model.layers.3.cross_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.cross_attn.q_proj.base_layer.weight', 'model.layers.33.cross_attn.q_proj.lora_A.default.weight', 'model.layers.33.cross_attn.q_proj.lora_B.default.weight', 'model.layers.33.cross_attn.v_proj.base_layer.weight', 'model.layers.33.cross_attn.v_proj.lora_A.default.weight', 'model.layers.33.cross_attn.v_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.cross_attn.q_proj.base_layer.weight', 'model.layers.38.cross_attn.q_proj.lora_A.default.weight', 'model.layers.38.cross_attn.q_proj.lora_B.default.weight', 'model.layers.38.cross_attn.v_proj.base_layer.weight', 'model.layers.38.cross_attn.v_proj.lora_A.default.weight', 'model.layers.38.cross_attn.v_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.cross_attn.q_proj.base_layer.weight', 'model.layers.8.cross_attn.q_proj.lora_A.default.weight', 'model.layers.8.cross_attn.q_proj.lora_B.default.weight', 'model.layers.8.cross_attn.v_proj.base_layer.weight', 'model.layers.8.cross_attn.v_proj.lora_A.default.weight', 'model.layers.8.cross_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']
- This IS expected if you are initializing MllamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MllamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MllamaForCausalLM were not initialized from the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.cross_attn.q_proj.weight', 'model.layers.13.cross_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.cross_attn.q_proj.weight', 'model.layers.18.cross_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.cross_attn.q_proj.weight', 'model.layers.23.cross_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.cross_attn.q_proj.weight', 'model.layers.28.cross_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.cross_attn.q_proj.weight', 'model.layers.3.cross_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.cross_attn.q_proj.weight', 'model.layers.33.cross_attn.v_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.cross_attn.q_proj.weight', 'model.layers.38.cross_attn.v_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.cross_attn.q_proj.weight', 'model.layers.8.cross_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from '/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant' to 'mylesgoose/Llama-3.2-11B-Vision-Instruct'. Please ensure that the correct base model is loaded when loading this checkpoint.
  warnings.warn(
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)  # noqa: B028
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00001-of-00009.safetensors
Loading weights for: base_model.model.model.embed_tokens.weight
Loading weights for: base_model.model.model.layers.0.input_layernorm.weight
Loading weights for: base_model.model.model.layers.0.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.0.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.0.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.input_layernorm.weight
Loading weights for: base_model.model.model.layers.1.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.1.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.1.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.input_layernorm.weight
Loading weights for: base_model.model.model.layers.2.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.2.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.2.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.3.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.3.input_layernorm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00002-of-00009.safetensors
Loading weights for: base_model.model.model.layers.3.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.3.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.input_layernorm.weight
Loading weights for: base_model.model.model.layers.4.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.4.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.input_layernorm.weight
Loading weights for: base_model.model.model.layers.5.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.5.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.5.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.input_layernorm.weight
Loading weights for: base_model.model.model.layers.6.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.6.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.6.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.input_layernorm.weight
Loading weights for: base_model.model.model.layers.7.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.7.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.7.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.8.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.8.input_layernorm.weight
Loading weights for: base_model.model.model.layers.8.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.8.mlp.up_proj.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00003-of-00009.safetensors
Loading weights for: base_model.model.model.layers.10.input_layernorm.weight
Loading weights for: base_model.model.model.layers.10.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.10.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.10.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.input_layernorm.weight
Loading weights for: base_model.model.model.layers.11.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.11.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.11.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.input_layernorm.weight
Loading weights for: base_model.model.model.layers.12.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.12.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.12.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.13.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.13.input_layernorm.weight
Loading weights for: base_model.model.model.layers.13.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.13.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.14.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.8.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.input_layernorm.weight
Loading weights for: base_model.model.model.layers.9.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.9.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00004-of-00009.safetensors
Loading weights for: base_model.model.model.layers.14.input_layernorm.weight
Loading weights for: base_model.model.model.layers.14.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.14.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.input_layernorm.weight
Loading weights for: base_model.model.model.layers.15.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.15.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.input_layernorm.weight
Loading weights for: base_model.model.model.layers.16.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.16.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.16.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.input_layernorm.weight
Loading weights for: base_model.model.model.layers.17.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.17.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.17.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.18.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.18.input_layernorm.weight
Loading weights for: base_model.model.model.layers.18.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.18.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.19.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00005-of-00009.safetensors
Loading weights for: base_model.model.model.layers.19.input_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.19.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.input_layernorm.weight
Loading weights for: base_model.model.model.layers.20.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.20.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.input_layernorm.weight
Loading weights for: base_model.model.model.layers.21.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.21.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.21.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.input_layernorm.weight
Loading weights for: base_model.model.model.layers.22.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.22.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.22.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.23.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.23.input_layernorm.weight
Loading weights for: base_model.model.model.layers.23.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.23.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.input_layernorm.weight
Loading weights for: base_model.model.model.layers.24.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.24.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00006-of-00009.safetensors
Loading weights for: base_model.model.model.layers.25.input_layernorm.weight
Loading weights for: base_model.model.model.layers.25.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.25.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.input_layernorm.weight
Loading weights for: base_model.model.model.layers.26.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.26.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.input_layernorm.weight
Loading weights for: base_model.model.model.layers.27.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.27.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.27.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.28.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.28.input_layernorm.weight
Loading weights for: base_model.model.model.layers.28.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.28.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.input_layernorm.weight
Loading weights for: base_model.model.model.layers.29.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.29.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.30.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00007-of-00009.safetensors
Loading weights for: base_model.model.model.layers.30.input_layernorm.weight
Loading weights for: base_model.model.model.layers.30.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.30.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.input_layernorm.weight
Loading weights for: base_model.model.model.layers.31.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.31.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.input_layernorm.weight
Loading weights for: base_model.model.model.layers.32.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.32.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.32.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.33.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.33.input_layernorm.weight
Loading weights for: base_model.model.model.layers.33.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.33.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.input_layernorm.weight
Loading weights for: base_model.model.model.layers.34.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.34.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.input_layernorm.weight
Loading weights for: base_model.model.model.layers.35.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.35.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.35.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00008-of-00009.safetensors
Loading weights for: base_model.model.model.layers.36.input_layernorm.weight
Loading weights for: base_model.model.model.layers.36.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.36.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.input_layernorm.weight
Loading weights for: base_model.model.model.layers.37.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.37.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.38.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.38.input_layernorm.weight
Loading weights for: base_model.model.model.layers.38.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.38.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.input_layernorm.weight
Loading weights for: base_model.model.model.layers.39.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.39.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.norm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_noquant/model-00009-of-00009.safetensors
Loading weights for: base_model.model.lm_head.weight
Missing in checkpoint: vision_model.class_embedding
Missing in checkpoint: vision_model.patch_embedding.weight
Missing in checkpoint: vision_model.gated_positional_embedding.gate
Missing in checkpoint: vision_model.gated_positional_embedding.embedding
Missing in checkpoint: vision_model.gated_positional_embedding.tile_embedding.weight
Missing in checkpoint: vision_model.pre_tile_positional_embedding.gate
Missing in checkpoint: vision_model.pre_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.post_tile_positional_embedding.gate
Missing in checkpoint: vision_model.post_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.layernorm_pre.weight
Missing in checkpoint: vision_model.layernorm_pre.bias
Missing in checkpoint: vision_model.layernorm_post.weight
Missing in checkpoint: vision_model.layernorm_post.bias
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: language_model.model.embed_tokens.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.0.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.0.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.0.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.0.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.1.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.1.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.1.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.1.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.2.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.2.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.2.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.2.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.3.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.3.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.3.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.3.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.3.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.4.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.4.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.4.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.4.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.5.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.5.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.5.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.5.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.6.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.6.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.6.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.6.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.7.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.7.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.7.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.7.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.8.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.8.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.8.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.8.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.8.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.9.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.9.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.9.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.9.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.10.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.10.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.10.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.10.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.11.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.11.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.11.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.11.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.12.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.12.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.12.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.12.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.13.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.13.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.13.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.13.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.13.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.14.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.14.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.14.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.14.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.15.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.15.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.15.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.15.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.16.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.16.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.16.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.16.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.17.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.17.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.17.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.17.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.18.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.18.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.18.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.18.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.18.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.19.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.19.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.19.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.19.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.20.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.20.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.20.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.20.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.21.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.21.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.21.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.21.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.22.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.22.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.22.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.22.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.23.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.23.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.23.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.23.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.23.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.24.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.24.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.24.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.24.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.25.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.25.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.25.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.25.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.26.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.26.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.26.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.26.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.27.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.27.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.27.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.27.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.28.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.28.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.28.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.28.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.28.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.29.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.29.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.29.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.29.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.30.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.30.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.30.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.30.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.31.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.31.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.31.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.31.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.32.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.32.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.32.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.32.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.33.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.33.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.33.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.33.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.33.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.34.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.34.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.34.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.34.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.35.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.35.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.35.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.35.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.36.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.36.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.36.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.36.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.37.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.37.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.37.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.37.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn_attn_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn_mlp_gate
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.q_norm.weight
Missing in checkpoint: language_model.model.layers.38.cross_attn.k_norm.weight
Missing in checkpoint: language_model.model.layers.38.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.38.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.38.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.38.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.k_proj.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.base_layer.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: language_model.model.layers.39.self_attn.o_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.gate_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.up_proj.weight
Missing in checkpoint: language_model.model.layers.39.mlp.down_proj.weight
Missing in checkpoint: language_model.model.layers.39.input_layernorm.weight
Missing in checkpoint: language_model.model.layers.39.post_attention_layernorm.weight
Missing in checkpoint: language_model.model.norm.weight
Missing in checkpoint: language_model.lm_head.weight
Missing in checkpoint: multi_modal_projector.weight
Missing in checkpoint: multi_modal_projector.bias
Extra in checkpoint: model.embed_tokens.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.k_proj.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.0.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.0.self_attn.o_proj.weight
Extra in checkpoint: model.layers.0.mlp.gate_proj.weight
Extra in checkpoint: model.layers.0.mlp.up_proj.weight
Extra in checkpoint: model.layers.0.mlp.down_proj.weight
Extra in checkpoint: model.layers.0.input_layernorm.weight
Extra in checkpoint: model.layers.0.post_attention_layernorm.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.k_proj.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.1.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.1.self_attn.o_proj.weight
Extra in checkpoint: model.layers.1.mlp.gate_proj.weight
Extra in checkpoint: model.layers.1.mlp.up_proj.weight
Extra in checkpoint: model.layers.1.mlp.down_proj.weight
Extra in checkpoint: model.layers.1.input_layernorm.weight
Extra in checkpoint: model.layers.1.post_attention_layernorm.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.k_proj.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.2.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.2.self_attn.o_proj.weight
Extra in checkpoint: model.layers.2.mlp.gate_proj.weight
Extra in checkpoint: model.layers.2.mlp.up_proj.weight
Extra in checkpoint: model.layers.2.mlp.down_proj.weight
Extra in checkpoint: model.layers.2.input_layernorm.weight
Extra in checkpoint: model.layers.2.post_attention_layernorm.weight
Extra in checkpoint: model.layers.3.cross_attn_attn_gate
Extra in checkpoint: model.layers.3.cross_attn_mlp_gate
Extra in checkpoint: model.layers.3.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.3.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.3.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.3.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.3.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.3.input_layernorm.weight
Extra in checkpoint: model.layers.3.mlp.gate_proj.weight
Extra in checkpoint: model.layers.3.mlp.up_proj.weight
Extra in checkpoint: model.layers.3.mlp.down_proj.weight
Extra in checkpoint: model.layers.3.post_attention_layernorm.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.k_proj.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.4.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.4.self_attn.o_proj.weight
Extra in checkpoint: model.layers.4.mlp.gate_proj.weight
Extra in checkpoint: model.layers.4.mlp.up_proj.weight
Extra in checkpoint: model.layers.4.mlp.down_proj.weight
Extra in checkpoint: model.layers.4.input_layernorm.weight
Extra in checkpoint: model.layers.4.post_attention_layernorm.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.k_proj.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.5.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.5.self_attn.o_proj.weight
Extra in checkpoint: model.layers.5.mlp.gate_proj.weight
Extra in checkpoint: model.layers.5.mlp.up_proj.weight
Extra in checkpoint: model.layers.5.mlp.down_proj.weight
Extra in checkpoint: model.layers.5.input_layernorm.weight
Extra in checkpoint: model.layers.5.post_attention_layernorm.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.k_proj.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.6.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.6.self_attn.o_proj.weight
Extra in checkpoint: model.layers.6.mlp.gate_proj.weight
Extra in checkpoint: model.layers.6.mlp.up_proj.weight
Extra in checkpoint: model.layers.6.mlp.down_proj.weight
Extra in checkpoint: model.layers.6.input_layernorm.weight
Extra in checkpoint: model.layers.6.post_attention_layernorm.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.k_proj.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.7.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.7.self_attn.o_proj.weight
Extra in checkpoint: model.layers.7.mlp.gate_proj.weight
Extra in checkpoint: model.layers.7.mlp.up_proj.weight
Extra in checkpoint: model.layers.7.mlp.down_proj.weight
Extra in checkpoint: model.layers.7.input_layernorm.weight
Extra in checkpoint: model.layers.7.post_attention_layernorm.weight
Extra in checkpoint: model.layers.8.cross_attn_attn_gate
Extra in checkpoint: model.layers.8.cross_attn_mlp_gate
Extra in checkpoint: model.layers.8.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.8.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.8.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.8.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.8.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.8.input_layernorm.weight
Extra in checkpoint: model.layers.8.mlp.gate_proj.weight
Extra in checkpoint: model.layers.8.mlp.up_proj.weight
Extra in checkpoint: model.layers.8.mlp.down_proj.weight
Extra in checkpoint: model.layers.8.post_attention_layernorm.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.k_proj.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.9.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.9.self_attn.o_proj.weight
Extra in checkpoint: model.layers.9.mlp.gate_proj.weight
Extra in checkpoint: model.layers.9.mlp.up_proj.weight
Extra in checkpoint: model.layers.9.mlp.down_proj.weight
Extra in checkpoint: model.layers.9.input_layernorm.weight
Extra in checkpoint: model.layers.9.post_attention_layernorm.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.k_proj.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.10.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.10.self_attn.o_proj.weight
Extra in checkpoint: model.layers.10.mlp.gate_proj.weight
Extra in checkpoint: model.layers.10.mlp.up_proj.weight
Extra in checkpoint: model.layers.10.mlp.down_proj.weight
Extra in checkpoint: model.layers.10.input_layernorm.weight
Extra in checkpoint: model.layers.10.post_attention_layernorm.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.k_proj.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.11.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.11.self_attn.o_proj.weight
Extra in checkpoint: model.layers.11.mlp.gate_proj.weight
Extra in checkpoint: model.layers.11.mlp.up_proj.weight
Extra in checkpoint: model.layers.11.mlp.down_proj.weight
Extra in checkpoint: model.layers.11.input_layernorm.weight
Extra in checkpoint: model.layers.11.post_attention_layernorm.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.k_proj.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.12.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.12.self_attn.o_proj.weight
Extra in checkpoint: model.layers.12.mlp.gate_proj.weight
Extra in checkpoint: model.layers.12.mlp.up_proj.weight
Extra in checkpoint: model.layers.12.mlp.down_proj.weight
Extra in checkpoint: model.layers.12.input_layernorm.weight
Extra in checkpoint: model.layers.12.post_attention_layernorm.weight
Extra in checkpoint: model.layers.13.cross_attn_attn_gate
Extra in checkpoint: model.layers.13.cross_attn_mlp_gate
Extra in checkpoint: model.layers.13.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.13.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.13.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.13.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.13.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.13.input_layernorm.weight
Extra in checkpoint: model.layers.13.mlp.gate_proj.weight
Extra in checkpoint: model.layers.13.mlp.up_proj.weight
Extra in checkpoint: model.layers.13.mlp.down_proj.weight
Extra in checkpoint: model.layers.13.post_attention_layernorm.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.k_proj.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.14.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.14.self_attn.o_proj.weight
Extra in checkpoint: model.layers.14.mlp.gate_proj.weight
Extra in checkpoint: model.layers.14.mlp.up_proj.weight
Extra in checkpoint: model.layers.14.mlp.down_proj.weight
Extra in checkpoint: model.layers.14.input_layernorm.weight
Extra in checkpoint: model.layers.14.post_attention_layernorm.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.k_proj.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.15.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.15.self_attn.o_proj.weight
Extra in checkpoint: model.layers.15.mlp.gate_proj.weight
Extra in checkpoint: model.layers.15.mlp.up_proj.weight
Extra in checkpoint: model.layers.15.mlp.down_proj.weight
Extra in checkpoint: model.layers.15.input_layernorm.weight
Extra in checkpoint: model.layers.15.post_attention_layernorm.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.k_proj.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.16.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.16.self_attn.o_proj.weight
Extra in checkpoint: model.layers.16.mlp.gate_proj.weight
Extra in checkpoint: model.layers.16.mlp.up_proj.weight
Extra in checkpoint: model.layers.16.mlp.down_proj.weight
Extra in checkpoint: model.layers.16.input_layernorm.weight
Extra in checkpoint: model.layers.16.post_attention_layernorm.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.k_proj.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.17.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.17.self_attn.o_proj.weight
Extra in checkpoint: model.layers.17.mlp.gate_proj.weight
Extra in checkpoint: model.layers.17.mlp.up_proj.weight
Extra in checkpoint: model.layers.17.mlp.down_proj.weight
Extra in checkpoint: model.layers.17.input_layernorm.weight
Extra in checkpoint: model.layers.17.post_attention_layernorm.weight
Extra in checkpoint: model.layers.18.cross_attn_attn_gate
Extra in checkpoint: model.layers.18.cross_attn_mlp_gate
Extra in checkpoint: model.layers.18.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.18.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.18.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.18.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.18.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.18.input_layernorm.weight
Extra in checkpoint: model.layers.18.mlp.gate_proj.weight
Extra in checkpoint: model.layers.18.mlp.up_proj.weight
Extra in checkpoint: model.layers.18.mlp.down_proj.weight
Extra in checkpoint: model.layers.18.post_attention_layernorm.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.k_proj.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.19.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.19.self_attn.o_proj.weight
Extra in checkpoint: model.layers.19.mlp.gate_proj.weight
Extra in checkpoint: model.layers.19.mlp.up_proj.weight
Extra in checkpoint: model.layers.19.mlp.down_proj.weight
Extra in checkpoint: model.layers.19.input_layernorm.weight
Extra in checkpoint: model.layers.19.post_attention_layernorm.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.k_proj.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.20.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.20.self_attn.o_proj.weight
Extra in checkpoint: model.layers.20.mlp.gate_proj.weight
Extra in checkpoint: model.layers.20.mlp.up_proj.weight
Extra in checkpoint: model.layers.20.mlp.down_proj.weight
Extra in checkpoint: model.layers.20.input_layernorm.weight
Extra in checkpoint: model.layers.20.post_attention_layernorm.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.k_proj.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.21.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.21.self_attn.o_proj.weight
Extra in checkpoint: model.layers.21.mlp.gate_proj.weight
Extra in checkpoint: model.layers.21.mlp.up_proj.weight
Extra in checkpoint: model.layers.21.mlp.down_proj.weight
Extra in checkpoint: model.layers.21.input_layernorm.weight
Extra in checkpoint: model.layers.21.post_attention_layernorm.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.k_proj.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.22.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.22.self_attn.o_proj.weight
Extra in checkpoint: model.layers.22.mlp.gate_proj.weight
Extra in checkpoint: model.layers.22.mlp.up_proj.weight
Extra in checkpoint: model.layers.22.mlp.down_proj.weight
Extra in checkpoint: model.layers.22.input_layernorm.weight
Extra in checkpoint: model.layers.22.post_attention_layernorm.weight
Extra in checkpoint: model.layers.23.cross_attn_attn_gate
Extra in checkpoint: model.layers.23.cross_attn_mlp_gate
Extra in checkpoint: model.layers.23.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.23.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.23.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.23.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.23.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.23.input_layernorm.weight
Extra in checkpoint: model.layers.23.mlp.gate_proj.weight
Extra in checkpoint: model.layers.23.mlp.up_proj.weight
Extra in checkpoint: model.layers.23.mlp.down_proj.weight
Extra in checkpoint: model.layers.23.post_attention_layernorm.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.k_proj.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.24.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.24.self_attn.o_proj.weight
Extra in checkpoint: model.layers.24.mlp.gate_proj.weight
Extra in checkpoint: model.layers.24.mlp.up_proj.weight
Extra in checkpoint: model.layers.24.mlp.down_proj.weight
Extra in checkpoint: model.layers.24.input_layernorm.weight
Extra in checkpoint: model.layers.24.post_attention_layernorm.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.k_proj.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.25.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.25.self_attn.o_proj.weight
Extra in checkpoint: model.layers.25.mlp.gate_proj.weight
Extra in checkpoint: model.layers.25.mlp.up_proj.weight
Extra in checkpoint: model.layers.25.mlp.down_proj.weight
Extra in checkpoint: model.layers.25.input_layernorm.weight
Extra in checkpoint: model.layers.25.post_attention_layernorm.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.k_proj.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.26.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.26.self_attn.o_proj.weight
Extra in checkpoint: model.layers.26.mlp.gate_proj.weight
Extra in checkpoint: model.layers.26.mlp.up_proj.weight
Extra in checkpoint: model.layers.26.mlp.down_proj.weight
Extra in checkpoint: model.layers.26.input_layernorm.weight
Extra in checkpoint: model.layers.26.post_attention_layernorm.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.k_proj.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.27.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.27.self_attn.o_proj.weight
Extra in checkpoint: model.layers.27.mlp.gate_proj.weight
Extra in checkpoint: model.layers.27.mlp.up_proj.weight
Extra in checkpoint: model.layers.27.mlp.down_proj.weight
Extra in checkpoint: model.layers.27.input_layernorm.weight
Extra in checkpoint: model.layers.27.post_attention_layernorm.weight
Extra in checkpoint: model.layers.28.cross_attn_attn_gate
Extra in checkpoint: model.layers.28.cross_attn_mlp_gate
Extra in checkpoint: model.layers.28.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.28.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.28.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.28.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.28.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.28.input_layernorm.weight
Extra in checkpoint: model.layers.28.mlp.gate_proj.weight
Extra in checkpoint: model.layers.28.mlp.up_proj.weight
Extra in checkpoint: model.layers.28.mlp.down_proj.weight
Extra in checkpoint: model.layers.28.post_attention_layernorm.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.k_proj.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.29.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.29.self_attn.o_proj.weight
Extra in checkpoint: model.layers.29.mlp.gate_proj.weight
Extra in checkpoint: model.layers.29.mlp.up_proj.weight
Extra in checkpoint: model.layers.29.mlp.down_proj.weight
Extra in checkpoint: model.layers.29.input_layernorm.weight
Extra in checkpoint: model.layers.29.post_attention_layernorm.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.k_proj.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.30.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.30.self_attn.o_proj.weight
Extra in checkpoint: model.layers.30.mlp.gate_proj.weight
Extra in checkpoint: model.layers.30.mlp.up_proj.weight
Extra in checkpoint: model.layers.30.mlp.down_proj.weight
Extra in checkpoint: model.layers.30.input_layernorm.weight
Extra in checkpoint: model.layers.30.post_attention_layernorm.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.k_proj.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.31.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.31.self_attn.o_proj.weight
Extra in checkpoint: model.layers.31.mlp.gate_proj.weight
Extra in checkpoint: model.layers.31.mlp.up_proj.weight
Extra in checkpoint: model.layers.31.mlp.down_proj.weight
Extra in checkpoint: model.layers.31.input_layernorm.weight
Extra in checkpoint: model.layers.31.post_attention_layernorm.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.k_proj.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.32.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.32.self_attn.o_proj.weight
Extra in checkpoint: model.layers.32.mlp.gate_proj.weight
Extra in checkpoint: model.layers.32.mlp.up_proj.weight
Extra in checkpoint: model.layers.32.mlp.down_proj.weight
Extra in checkpoint: model.layers.32.input_layernorm.weight
Extra in checkpoint: model.layers.32.post_attention_layernorm.weight
Extra in checkpoint: model.layers.33.cross_attn_attn_gate
Extra in checkpoint: model.layers.33.cross_attn_mlp_gate
Extra in checkpoint: model.layers.33.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.33.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.33.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.33.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.33.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.33.input_layernorm.weight
Extra in checkpoint: model.layers.33.mlp.gate_proj.weight
Extra in checkpoint: model.layers.33.mlp.up_proj.weight
Extra in checkpoint: model.layers.33.mlp.down_proj.weight
Extra in checkpoint: model.layers.33.post_attention_layernorm.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.k_proj.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.34.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.34.self_attn.o_proj.weight
Extra in checkpoint: model.layers.34.mlp.gate_proj.weight
Extra in checkpoint: model.layers.34.mlp.up_proj.weight
Extra in checkpoint: model.layers.34.mlp.down_proj.weight
Extra in checkpoint: model.layers.34.input_layernorm.weight
Extra in checkpoint: model.layers.34.post_attention_layernorm.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.k_proj.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.35.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.35.self_attn.o_proj.weight
Extra in checkpoint: model.layers.35.mlp.gate_proj.weight
Extra in checkpoint: model.layers.35.mlp.up_proj.weight
Extra in checkpoint: model.layers.35.mlp.down_proj.weight
Extra in checkpoint: model.layers.35.input_layernorm.weight
Extra in checkpoint: model.layers.35.post_attention_layernorm.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.k_proj.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.36.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.36.self_attn.o_proj.weight
Extra in checkpoint: model.layers.36.mlp.gate_proj.weight
Extra in checkpoint: model.layers.36.mlp.up_proj.weight
Extra in checkpoint: model.layers.36.mlp.down_proj.weight
Extra in checkpoint: model.layers.36.input_layernorm.weight
Extra in checkpoint: model.layers.36.post_attention_layernorm.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.k_proj.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.37.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.37.self_attn.o_proj.weight
Extra in checkpoint: model.layers.37.mlp.gate_proj.weight
Extra in checkpoint: model.layers.37.mlp.up_proj.weight
Extra in checkpoint: model.layers.37.mlp.down_proj.weight
Extra in checkpoint: model.layers.37.input_layernorm.weight
Extra in checkpoint: model.layers.37.post_attention_layernorm.weight
Extra in checkpoint: model.layers.38.cross_attn_attn_gate
Extra in checkpoint: model.layers.38.cross_attn_mlp_gate
Extra in checkpoint: model.layers.38.cross_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.k_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.38.cross_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.38.cross_attn.o_proj.weight
Extra in checkpoint: model.layers.38.cross_attn.q_norm.weight
Extra in checkpoint: model.layers.38.cross_attn.k_norm.weight
Extra in checkpoint: model.layers.38.input_layernorm.weight
Extra in checkpoint: model.layers.38.mlp.gate_proj.weight
Extra in checkpoint: model.layers.38.mlp.up_proj.weight
Extra in checkpoint: model.layers.38.mlp.down_proj.weight
Extra in checkpoint: model.layers.38.post_attention_layernorm.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.q_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.k_proj.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.base_layer.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_A.default.weight
Extra in checkpoint: model.layers.39.self_attn.v_proj.lora_B.default.weight
Extra in checkpoint: model.layers.39.self_attn.o_proj.weight
Extra in checkpoint: model.layers.39.mlp.gate_proj.weight
Extra in checkpoint: model.layers.39.mlp.up_proj.weight
Extra in checkpoint: model.layers.39.mlp.down_proj.weight
Extra in checkpoint: model.layers.39.input_layernorm.weight
Extra in checkpoint: model.layers.39.post_attention_layernorm.weight
Extra in checkpoint: model.norm.weight
Extra in checkpoint: lm_head.weight
Some layers did not match.
LoRA model loaded successfully!
bbbbb
cuda:0
Map (num_proc=4):   0%|          | 0/560 [00:00<?, ? examples/s]Map (num_proc=4):  25%|██▌       | 140/560 [00:00<00:00, 1231.25 examples/s]Map (num_proc=4): 100%|██████████| 560/560 [00:00<00:00, 2679.41 examples/s]
Length of the updated dataset: 560

system

Today Date: 17 Nov 2024

You are a helpful AI that can generate tikz code from images.
user

This is a picture of a scientific figure Generate LaTeX code that draws this scientific figure using TikZ. Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. Don't forget to include \usepackage{tikz}! I understand that this is a challenging task, so do your best. Return your result in a ```latex code block.
assistant

\usepackage{tikz}
\begin{document}

\begin{tikzpicture}[scale=0.75, node distance=2cm]
\tikzstyle{every node}=[font=\small]

\draw[thick] (-2.5,0) -- (2.5,0);

\node (z1) at (-2.5,0) {$z_1=0$};
\node (z2) at (-1.5,0) {$z_2=a/N$};
\node (z3) at (-0.5,0) {$z_3=2a/N$};
\node (z4) at (0.5,0) {$z_4=3a/N$};
\node (z5) at (1.5,0) {$z_5=4a/N$};
\node (z6) at (2.5,0) {$z_6=a(N-1)/N$};

\node (k1) at (-2.5,-0.5) {$k_1=1$};
\node (k2) at (-1.5,-0.5) {$k_2=2$};
\node (k3) at (-0.5,-0.5) {$k_3=3$};
\node (k4) at (0.5,-0.5) {$k_4=4$};
\node (k5) at (1.5,-0.5) {$k_5=5$};
\node (k6) at (2.5,-0.5) {$k_6=N-1$};

\draw[thick] (-2.5,-0.5) -- (2.5,-0.5);

\draw[thick] (z1) -- (k1);
\draw[thick] (z2) -- (k2);
\draw[thick] (z3) -- (k3);
\draw[thick] (z4) -- (k4);
\draw[thick] (z5) -- (k5);
\draw[thick] (z6) -- (k6);

\end{tikzpicture}

\end{document}
########################################################
\begin{tikzpicture}[scale=0.75, node distance=2cm]
\tikzstyle{every node}=[font=\small]

\draw[thick] (-2.5,0) -- (2.5,0);

\node (z1) at (-2.5,0) {$z_1=0$};
\node (z2) at (-1.5,0) {$z_2=a/N$};
\node (z3) at (-0.5,0) {$z_3=2a/N$};
\node (z4) at (0.5,0) {$z_4=3a/N$};
\node (z5) at (1.5,0) {$z_5=4a/N$};
\node (z6) at (2.5,0) {$z_6=a(N-1)/N$};

\node (k1) at (-2.5,-0.5) {$k_1=1$};
\node (k2) at (-1.5,-0.5) {$k_2=2$};
\node (k3) at (-0.5,-0.5) {$k_3=3$};
\node (k4) at (0.5,-0.5) {$k_4=4$};
\node (k5) at (1.5,-0.5) {$k_5=5$};
\node (k6) at (2.5,-0.5) {$k_6=N-1$};

\draw[thick] (-2.5,-0.5) -- (2.5,-0.5);

\draw[thick] (z1) -- (k1);
\draw[thick] (z2) -- (k2);
\draw[thick] (z3) -- (k3);
\draw[thick] (z4) -- (k4);
\draw[thick] (z5) -- (k5);
\draw[thick] (z6) -- (k6);

\end{tikzpicture}
Processing sample 1/560 - Caption: The image depicts a segmented line representing a discretized interval from \( x = 0 \) to \( x = 1 \). The interval is divided into \( N \) segments, each of length \( \Delta x = 1/N \). The points along the line are labeled with their corresponding positions \( x = m \Delta x \) for \( m = 0, 1, 2, \ldots, N \). Below the line, two rows of indices are shown: the global index \( j \) and the interior index \( k \). The global index \( j \) ranges from 1 to \( N+1 \), while the interior index \( k \) ranges from 1 to \( N-1 \). The correspondence between the global and interior indices is indicated, with specific points highlighted such as \( x = 0 \) (global index \( j = 1 \)), \( x = \Delta x \) (global index \( j = 2 \), interior index \( k = 1 \)), and so on, up to \( x = 1 \) (global index \( j = N+1 \)).
TEX Edit Distance for sample 1: 0.8007503747940063
CrystalBLEU Score for sample 1: 0
Sample 1 processing time: 71.91 seconds

Processing sample 2/560 - Caption: This image depicts a directed graph with six nodes and various directed edges, including both solid and dashed lines. The nodes are labeled as follows: \( V_1 (H, a) \), \( V_2 (x) \), \( V_3 (\tilde{x}) \), \( V_4 (L) \), \( V_5 (L) \), and \( V_6 (G, \tilde{c}) \). The edges are as follows:

1. A solid edge from \( V_1 \) to \( V_2 \).
2. A solid edge from \( V_1 \) to \( V_3 \).
3. A solid edge from \( V_2 \) to \( V_4 \).
4. A solid edge from \( V_3 \) to \( V_4 \).
5. A solid edge from \( V_4 \) to \( V_6 \).
6. A solid edge from \( V_5 \) to \( V_6 \).
7. A dashed edge from \( V_1 \) to \( V_4 \).
8. A dashed edge from \( V_2 \) to \( V_5 \).
9. A dashed edge from \( V_3 \) to \( V_6 \).
10. A dashed edge from \( V_4 \) to \( V_5 \).

The nodes are arranged in a roughly horizontal layout, with \( V_1 \) on the left, \( V_6 \) on the right, and the other nodes positioned between them. The edges are directed and some are curved to avoid overlapping with other edges.
TEX Edit Distance for sample 2: 0.6188651919364929
CrystalBLEU Score for sample 2: 0.01644312266651638
Sample 2 processing time: 56.37 seconds

Processing sample 3/560 - Caption: This image is a line plot with three datasets: Multi-News, SamSUM, and CNN/DM. The x-axis represents the dataset size on a logarithmic scale, ranging from \(10^3\) to \(10^5\). The y-axis represents the average value, ranging from 0.5 to 0.7. The plot includes three lines: a blue line with square markers for Multi-News, a red line with triangle markers for SamSUM, and a green line with star markers for CNN/DM. The legend is located inside the plot area, towards the right side. The grid lines are visible for better readability.
slurmstepd: error: *** JOB 9451113 ON cn262 CANCELLED AT 2024-11-17T14:33:43 ***
