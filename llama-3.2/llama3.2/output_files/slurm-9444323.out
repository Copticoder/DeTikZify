Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
cuda
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:07<00:30,  7.51s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:10<00:15,  5.10s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:14<00:08,  4.26s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:17<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:18<00:00,  2.88s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:18<00:00,  3.71s/it]
WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.
Traceback (most recent call last):
  File "/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/run.py", line 66, in <module>
    full_model = MllamaForConditionalGeneration.from_pretrained(
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3142, in to
    raise ValueError(
ValueError: `.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
