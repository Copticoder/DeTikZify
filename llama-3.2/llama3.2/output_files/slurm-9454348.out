Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:02,  1.41it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:00<00:01,  2.72it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:00<00:00,  3.88it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:01<00:00,  4.83it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  5.89it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  4.17it/s]
Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:23<03:05, 23.24s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:46<02:42, 23.19s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [01:33<03:24, 34.16s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [01:55<02:26, 29.33s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [02:16<01:45, 26.44s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [02:39<01:15, 25.28s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [03:11<00:54, 27.22s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [03:42<00:28, 28.64s/it]Loading checkpoint shards: 100%|██████████| 9/9 [04:48<00:00, 40.24s/it]Loading checkpoint shards: 100%|██████████| 9/9 [04:48<00:00, 32.06s/it]
Some weights of the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant were not used when initializing MllamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.cross_attn.q_proj.base_layer.weight', 'model.layers.13.cross_attn.q_proj.lora_A.default.weight', 'model.layers.13.cross_attn.q_proj.lora_B.default.weight', 'model.layers.13.cross_attn.v_proj.base_layer.weight', 'model.layers.13.cross_attn.v_proj.lora_A.default.weight', 'model.layers.13.cross_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.cross_attn.q_proj.base_layer.weight', 'model.layers.18.cross_attn.q_proj.lora_A.default.weight', 'model.layers.18.cross_attn.q_proj.lora_B.default.weight', 'model.layers.18.cross_attn.v_proj.base_layer.weight', 'model.layers.18.cross_attn.v_proj.lora_A.default.weight', 'model.layers.18.cross_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.cross_attn.q_proj.base_layer.weight', 'model.layers.23.cross_attn.q_proj.lora_A.default.weight', 'model.layers.23.cross_attn.q_proj.lora_B.default.weight', 'model.layers.23.cross_attn.v_proj.base_layer.weight', 'model.layers.23.cross_attn.v_proj.lora_A.default.weight', 'model.layers.23.cross_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.cross_attn.q_proj.base_layer.weight', 'model.layers.28.cross_attn.q_proj.lora_A.default.weight', 'model.layers.28.cross_attn.q_proj.lora_B.default.weight', 'model.layers.28.cross_attn.v_proj.base_layer.weight', 'model.layers.28.cross_attn.v_proj.lora_A.default.weight', 'model.layers.28.cross_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.cross_attn.q_proj.base_layer.weight', 'model.layers.3.cross_attn.q_proj.lora_A.default.weight', 'model.layers.3.cross_attn.q_proj.lora_B.default.weight', 'model.layers.3.cross_attn.v_proj.base_layer.weight', 'model.layers.3.cross_attn.v_proj.lora_A.default.weight', 'model.layers.3.cross_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.cross_attn.q_proj.base_layer.weight', 'model.layers.33.cross_attn.q_proj.lora_A.default.weight', 'model.layers.33.cross_attn.q_proj.lora_B.default.weight', 'model.layers.33.cross_attn.v_proj.base_layer.weight', 'model.layers.33.cross_attn.v_proj.lora_A.default.weight', 'model.layers.33.cross_attn.v_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.cross_attn.q_proj.base_layer.weight', 'model.layers.38.cross_attn.q_proj.lora_A.default.weight', 'model.layers.38.cross_attn.q_proj.lora_B.default.weight', 'model.layers.38.cross_attn.v_proj.base_layer.weight', 'model.layers.38.cross_attn.v_proj.lora_A.default.weight', 'model.layers.38.cross_attn.v_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.cross_attn.q_proj.base_layer.weight', 'model.layers.8.cross_attn.q_proj.lora_A.default.weight', 'model.layers.8.cross_attn.q_proj.lora_B.default.weight', 'model.layers.8.cross_attn.v_proj.base_layer.weight', 'model.layers.8.cross_attn.v_proj.lora_A.default.weight', 'model.layers.8.cross_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']
- This IS expected if you are initializing MllamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MllamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MllamaForCausalLM were not initialized from the model checkpoint at /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.cross_attn.q_proj.weight', 'model.layers.13.cross_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.cross_attn.q_proj.weight', 'model.layers.18.cross_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.cross_attn.q_proj.weight', 'model.layers.23.cross_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.cross_attn.q_proj.weight', 'model.layers.28.cross_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.cross_attn.q_proj.weight', 'model.layers.3.cross_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.cross_attn.q_proj.weight', 'model.layers.33.cross_attn.v_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.cross_attn.q_proj.weight', 'model.layers.38.cross_attn.v_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.cross_attn.q_proj.weight', 'model.layers.8.cross_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from '/scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant' to 'mylesgoose/Llama-3.2-11B-Vision-Instruct'. Please ensure that the correct base model is loaded when loading this checkpoint.
  warnings.warn(
/scratch/oe2015/conda-envs/greedy/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)  # noqa: B028
base_model.model.model.embed_tokens.weight False
base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.0.self_attn.k_proj.weight False
base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.0.self_attn.o_proj.weight False
base_model.model.model.layers.0.mlp.gate_proj.weight False
base_model.model.model.layers.0.mlp.up_proj.weight False
base_model.model.model.layers.0.mlp.down_proj.weight False
base_model.model.model.layers.0.input_layernorm.weight False
base_model.model.model.layers.0.post_attention_layernorm.weight False
base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.1.self_attn.k_proj.weight False
base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.1.self_attn.o_proj.weight False
base_model.model.model.layers.1.mlp.gate_proj.weight False
base_model.model.model.layers.1.mlp.up_proj.weight False
base_model.model.model.layers.1.mlp.down_proj.weight False
base_model.model.model.layers.1.input_layernorm.weight False
base_model.model.model.layers.1.post_attention_layernorm.weight False
base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.2.self_attn.k_proj.weight False
base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.2.self_attn.o_proj.weight False
base_model.model.model.layers.2.mlp.gate_proj.weight False
base_model.model.model.layers.2.mlp.up_proj.weight False
base_model.model.model.layers.2.mlp.down_proj.weight False
base_model.model.model.layers.2.input_layernorm.weight False
base_model.model.model.layers.2.post_attention_layernorm.weight False
base_model.model.model.layers.3.cross_attn_attn_gate False
base_model.model.model.layers.3.cross_attn_mlp_gate False
base_model.model.model.layers.3.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.3.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.3.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.3.cross_attn.k_proj.weight False
base_model.model.model.layers.3.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.3.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.3.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.3.cross_attn.o_proj.weight False
base_model.model.model.layers.3.cross_attn.q_norm.weight False
base_model.model.model.layers.3.cross_attn.k_norm.weight False
base_model.model.model.layers.3.input_layernorm.weight False
base_model.model.model.layers.3.mlp.gate_proj.weight False
base_model.model.model.layers.3.mlp.up_proj.weight False
base_model.model.model.layers.3.mlp.down_proj.weight False
base_model.model.model.layers.3.post_attention_layernorm.weight False
base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.4.self_attn.k_proj.weight False
base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.4.self_attn.o_proj.weight False
base_model.model.model.layers.4.mlp.gate_proj.weight False
base_model.model.model.layers.4.mlp.up_proj.weight False
base_model.model.model.layers.4.mlp.down_proj.weight False
base_model.model.model.layers.4.input_layernorm.weight False
base_model.model.model.layers.4.post_attention_layernorm.weight False
base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.5.self_attn.k_proj.weight False
base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.5.self_attn.o_proj.weight False
base_model.model.model.layers.5.mlp.gate_proj.weight False
base_model.model.model.layers.5.mlp.up_proj.weight False
base_model.model.model.layers.5.mlp.down_proj.weight False
base_model.model.model.layers.5.input_layernorm.weight False
base_model.model.model.layers.5.post_attention_layernorm.weight False
base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.6.self_attn.k_proj.weight False
base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.6.self_attn.o_proj.weight False
base_model.model.model.layers.6.mlp.gate_proj.weight False
base_model.model.model.layers.6.mlp.up_proj.weight False
base_model.model.model.layers.6.mlp.down_proj.weight False
base_model.model.model.layers.6.input_layernorm.weight False
base_model.model.model.layers.6.post_attention_layernorm.weight False
base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.7.self_attn.k_proj.weight False
base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.7.self_attn.o_proj.weight False
base_model.model.model.layers.7.mlp.gate_proj.weight False
base_model.model.model.layers.7.mlp.up_proj.weight False
base_model.model.model.layers.7.mlp.down_proj.weight False
base_model.model.model.layers.7.input_layernorm.weight False
base_model.model.model.layers.7.post_attention_layernorm.weight False
base_model.model.model.layers.8.cross_attn_attn_gate False
base_model.model.model.layers.8.cross_attn_mlp_gate False
base_model.model.model.layers.8.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.8.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.8.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.8.cross_attn.k_proj.weight False
base_model.model.model.layers.8.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.8.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.8.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.8.cross_attn.o_proj.weight False
base_model.model.model.layers.8.cross_attn.q_norm.weight False
base_model.model.model.layers.8.cross_attn.k_norm.weight False
base_model.model.model.layers.8.input_layernorm.weight False
base_model.model.model.layers.8.mlp.gate_proj.weight False
base_model.model.model.layers.8.mlp.up_proj.weight False
base_model.model.model.layers.8.mlp.down_proj.weight False
base_model.model.model.layers.8.post_attention_layernorm.weight False
base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.9.self_attn.k_proj.weight False
base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.9.self_attn.o_proj.weight False
base_model.model.model.layers.9.mlp.gate_proj.weight False
base_model.model.model.layers.9.mlp.up_proj.weight False
base_model.model.model.layers.9.mlp.down_proj.weight False
base_model.model.model.layers.9.input_layernorm.weight False
base_model.model.model.layers.9.post_attention_layernorm.weight False
base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.10.self_attn.k_proj.weight False
base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.10.self_attn.o_proj.weight False
base_model.model.model.layers.10.mlp.gate_proj.weight False
base_model.model.model.layers.10.mlp.up_proj.weight False
base_model.model.model.layers.10.mlp.down_proj.weight False
base_model.model.model.layers.10.input_layernorm.weight False
base_model.model.model.layers.10.post_attention_layernorm.weight False
base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.11.self_attn.k_proj.weight False
base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.11.self_attn.o_proj.weight False
base_model.model.model.layers.11.mlp.gate_proj.weight False
base_model.model.model.layers.11.mlp.up_proj.weight False
base_model.model.model.layers.11.mlp.down_proj.weight False
base_model.model.model.layers.11.input_layernorm.weight False
base_model.model.model.layers.11.post_attention_layernorm.weight False
base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.12.self_attn.k_proj.weight False
base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.12.self_attn.o_proj.weight False
base_model.model.model.layers.12.mlp.gate_proj.weight False
base_model.model.model.layers.12.mlp.up_proj.weight False
base_model.model.model.layers.12.mlp.down_proj.weight False
base_model.model.model.layers.12.input_layernorm.weight False
base_model.model.model.layers.12.post_attention_layernorm.weight False
base_model.model.model.layers.13.cross_attn_attn_gate False
base_model.model.model.layers.13.cross_attn_mlp_gate False
base_model.model.model.layers.13.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.13.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.13.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.13.cross_attn.k_proj.weight False
base_model.model.model.layers.13.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.13.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.13.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.13.cross_attn.o_proj.weight False
base_model.model.model.layers.13.cross_attn.q_norm.weight False
base_model.model.model.layers.13.cross_attn.k_norm.weight False
base_model.model.model.layers.13.input_layernorm.weight False
base_model.model.model.layers.13.mlp.gate_proj.weight False
base_model.model.model.layers.13.mlp.up_proj.weight False
base_model.model.model.layers.13.mlp.down_proj.weight False
base_model.model.model.layers.13.post_attention_layernorm.weight False
base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.14.self_attn.k_proj.weight False
base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.14.self_attn.o_proj.weight False
base_model.model.model.layers.14.mlp.gate_proj.weight False
base_model.model.model.layers.14.mlp.up_proj.weight False
base_model.model.model.layers.14.mlp.down_proj.weight False
base_model.model.model.layers.14.input_layernorm.weight False
base_model.model.model.layers.14.post_attention_layernorm.weight False
base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.15.self_attn.k_proj.weight False
base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.15.self_attn.o_proj.weight False
base_model.model.model.layers.15.mlp.gate_proj.weight False
base_model.model.model.layers.15.mlp.up_proj.weight False
base_model.model.model.layers.15.mlp.down_proj.weight False
base_model.model.model.layers.15.input_layernorm.weight False
base_model.model.model.layers.15.post_attention_layernorm.weight False
base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.16.self_attn.k_proj.weight False
base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.16.self_attn.o_proj.weight False
base_model.model.model.layers.16.mlp.gate_proj.weight False
base_model.model.model.layers.16.mlp.up_proj.weight False
base_model.model.model.layers.16.mlp.down_proj.weight False
base_model.model.model.layers.16.input_layernorm.weight False
base_model.model.model.layers.16.post_attention_layernorm.weight False
base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.17.self_attn.k_proj.weight False
base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.17.self_attn.o_proj.weight False
base_model.model.model.layers.17.mlp.gate_proj.weight False
base_model.model.model.layers.17.mlp.up_proj.weight False
base_model.model.model.layers.17.mlp.down_proj.weight False
base_model.model.model.layers.17.input_layernorm.weight False
base_model.model.model.layers.17.post_attention_layernorm.weight False
base_model.model.model.layers.18.cross_attn_attn_gate False
base_model.model.model.layers.18.cross_attn_mlp_gate False
base_model.model.model.layers.18.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.18.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.18.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.18.cross_attn.k_proj.weight False
base_model.model.model.layers.18.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.18.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.18.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.18.cross_attn.o_proj.weight False
base_model.model.model.layers.18.cross_attn.q_norm.weight False
base_model.model.model.layers.18.cross_attn.k_norm.weight False
base_model.model.model.layers.18.input_layernorm.weight False
base_model.model.model.layers.18.mlp.gate_proj.weight False
base_model.model.model.layers.18.mlp.up_proj.weight False
base_model.model.model.layers.18.mlp.down_proj.weight False
base_model.model.model.layers.18.post_attention_layernorm.weight False
base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.19.self_attn.k_proj.weight False
base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.19.self_attn.o_proj.weight False
base_model.model.model.layers.19.mlp.gate_proj.weight False
base_model.model.model.layers.19.mlp.up_proj.weight False
base_model.model.model.layers.19.mlp.down_proj.weight False
base_model.model.model.layers.19.input_layernorm.weight False
base_model.model.model.layers.19.post_attention_layernorm.weight False
base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.20.self_attn.k_proj.weight False
base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.20.self_attn.o_proj.weight False
base_model.model.model.layers.20.mlp.gate_proj.weight False
base_model.model.model.layers.20.mlp.up_proj.weight False
base_model.model.model.layers.20.mlp.down_proj.weight False
base_model.model.model.layers.20.input_layernorm.weight False
base_model.model.model.layers.20.post_attention_layernorm.weight False
base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.21.self_attn.k_proj.weight False
base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.21.self_attn.o_proj.weight False
base_model.model.model.layers.21.mlp.gate_proj.weight False
base_model.model.model.layers.21.mlp.up_proj.weight False
base_model.model.model.layers.21.mlp.down_proj.weight False
base_model.model.model.layers.21.input_layernorm.weight False
base_model.model.model.layers.21.post_attention_layernorm.weight False
base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.22.self_attn.k_proj.weight False
base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.22.self_attn.o_proj.weight False
base_model.model.model.layers.22.mlp.gate_proj.weight False
base_model.model.model.layers.22.mlp.up_proj.weight False
base_model.model.model.layers.22.mlp.down_proj.weight False
base_model.model.model.layers.22.input_layernorm.weight False
base_model.model.model.layers.22.post_attention_layernorm.weight False
base_model.model.model.layers.23.cross_attn_attn_gate False
base_model.model.model.layers.23.cross_attn_mlp_gate False
base_model.model.model.layers.23.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.23.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.23.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.23.cross_attn.k_proj.weight False
base_model.model.model.layers.23.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.23.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.23.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.23.cross_attn.o_proj.weight False
base_model.model.model.layers.23.cross_attn.q_norm.weight False
base_model.model.model.layers.23.cross_attn.k_norm.weight False
base_model.model.model.layers.23.input_layernorm.weight False
base_model.model.model.layers.23.mlp.gate_proj.weight False
base_model.model.model.layers.23.mlp.up_proj.weight False
base_model.model.model.layers.23.mlp.down_proj.weight False
base_model.model.model.layers.23.post_attention_layernorm.weight False
base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.24.self_attn.k_proj.weight False
base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.24.self_attn.o_proj.weight False
base_model.model.model.layers.24.mlp.gate_proj.weight False
base_model.model.model.layers.24.mlp.up_proj.weight False
base_model.model.model.layers.24.mlp.down_proj.weight False
base_model.model.model.layers.24.input_layernorm.weight False
base_model.model.model.layers.24.post_attention_layernorm.weight False
base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.25.self_attn.k_proj.weight False
base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.25.self_attn.o_proj.weight False
base_model.model.model.layers.25.mlp.gate_proj.weight False
base_model.model.model.layers.25.mlp.up_proj.weight False
base_model.model.model.layers.25.mlp.down_proj.weight False
base_model.model.model.layers.25.input_layernorm.weight False
base_model.model.model.layers.25.post_attention_layernorm.weight False
base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.26.self_attn.k_proj.weight False
base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.26.self_attn.o_proj.weight False
base_model.model.model.layers.26.mlp.gate_proj.weight False
base_model.model.model.layers.26.mlp.up_proj.weight False
base_model.model.model.layers.26.mlp.down_proj.weight False
base_model.model.model.layers.26.input_layernorm.weight False
base_model.model.model.layers.26.post_attention_layernorm.weight False
base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.27.self_attn.k_proj.weight False
base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.27.self_attn.o_proj.weight False
base_model.model.model.layers.27.mlp.gate_proj.weight False
base_model.model.model.layers.27.mlp.up_proj.weight False
base_model.model.model.layers.27.mlp.down_proj.weight False
base_model.model.model.layers.27.input_layernorm.weight False
base_model.model.model.layers.27.post_attention_layernorm.weight False
base_model.model.model.layers.28.cross_attn_attn_gate False
base_model.model.model.layers.28.cross_attn_mlp_gate False
base_model.model.model.layers.28.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.28.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.28.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.28.cross_attn.k_proj.weight False
base_model.model.model.layers.28.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.28.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.28.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.28.cross_attn.o_proj.weight False
base_model.model.model.layers.28.cross_attn.q_norm.weight False
base_model.model.model.layers.28.cross_attn.k_norm.weight False
base_model.model.model.layers.28.input_layernorm.weight False
base_model.model.model.layers.28.mlp.gate_proj.weight False
base_model.model.model.layers.28.mlp.up_proj.weight False
base_model.model.model.layers.28.mlp.down_proj.weight False
base_model.model.model.layers.28.post_attention_layernorm.weight False
base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.29.self_attn.k_proj.weight False
base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.29.self_attn.o_proj.weight False
base_model.model.model.layers.29.mlp.gate_proj.weight False
base_model.model.model.layers.29.mlp.up_proj.weight False
base_model.model.model.layers.29.mlp.down_proj.weight False
base_model.model.model.layers.29.input_layernorm.weight False
base_model.model.model.layers.29.post_attention_layernorm.weight False
base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.30.self_attn.k_proj.weight False
base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.30.self_attn.o_proj.weight False
base_model.model.model.layers.30.mlp.gate_proj.weight False
base_model.model.model.layers.30.mlp.up_proj.weight False
base_model.model.model.layers.30.mlp.down_proj.weight False
base_model.model.model.layers.30.input_layernorm.weight False
base_model.model.model.layers.30.post_attention_layernorm.weight False
base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.31.self_attn.k_proj.weight False
base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.31.self_attn.o_proj.weight False
base_model.model.model.layers.31.mlp.gate_proj.weight False
base_model.model.model.layers.31.mlp.up_proj.weight False
base_model.model.model.layers.31.mlp.down_proj.weight False
base_model.model.model.layers.31.input_layernorm.weight False
base_model.model.model.layers.31.post_attention_layernorm.weight False
base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.32.self_attn.k_proj.weight False
base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.32.self_attn.o_proj.weight False
base_model.model.model.layers.32.mlp.gate_proj.weight False
base_model.model.model.layers.32.mlp.up_proj.weight False
base_model.model.model.layers.32.mlp.down_proj.weight False
base_model.model.model.layers.32.input_layernorm.weight False
base_model.model.model.layers.32.post_attention_layernorm.weight False
base_model.model.model.layers.33.cross_attn_attn_gate False
base_model.model.model.layers.33.cross_attn_mlp_gate False
base_model.model.model.layers.33.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.33.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.33.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.33.cross_attn.k_proj.weight False
base_model.model.model.layers.33.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.33.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.33.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.33.cross_attn.o_proj.weight False
base_model.model.model.layers.33.cross_attn.q_norm.weight False
base_model.model.model.layers.33.cross_attn.k_norm.weight False
base_model.model.model.layers.33.input_layernorm.weight False
base_model.model.model.layers.33.mlp.gate_proj.weight False
base_model.model.model.layers.33.mlp.up_proj.weight False
base_model.model.model.layers.33.mlp.down_proj.weight False
base_model.model.model.layers.33.post_attention_layernorm.weight False
base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.34.self_attn.k_proj.weight False
base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.34.self_attn.o_proj.weight False
base_model.model.model.layers.34.mlp.gate_proj.weight False
base_model.model.model.layers.34.mlp.up_proj.weight False
base_model.model.model.layers.34.mlp.down_proj.weight False
base_model.model.model.layers.34.input_layernorm.weight False
base_model.model.model.layers.34.post_attention_layernorm.weight False
base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.35.self_attn.k_proj.weight False
base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.35.self_attn.o_proj.weight False
base_model.model.model.layers.35.mlp.gate_proj.weight False
base_model.model.model.layers.35.mlp.up_proj.weight False
base_model.model.model.layers.35.mlp.down_proj.weight False
base_model.model.model.layers.35.input_layernorm.weight False
base_model.model.model.layers.35.post_attention_layernorm.weight False
base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.36.self_attn.k_proj.weight False
base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.36.self_attn.o_proj.weight False
base_model.model.model.layers.36.mlp.gate_proj.weight False
base_model.model.model.layers.36.mlp.up_proj.weight False
base_model.model.model.layers.36.mlp.down_proj.weight False
base_model.model.model.layers.36.input_layernorm.weight False
base_model.model.model.layers.36.post_attention_layernorm.weight False
base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.37.self_attn.k_proj.weight False
base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.37.self_attn.o_proj.weight False
base_model.model.model.layers.37.mlp.gate_proj.weight False
base_model.model.model.layers.37.mlp.up_proj.weight False
base_model.model.model.layers.37.mlp.down_proj.weight False
base_model.model.model.layers.37.input_layernorm.weight False
base_model.model.model.layers.37.post_attention_layernorm.weight False
base_model.model.model.layers.38.cross_attn_attn_gate False
base_model.model.model.layers.38.cross_attn_mlp_gate False
base_model.model.model.layers.38.cross_attn.q_proj.base_layer.weight False
base_model.model.model.layers.38.cross_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.38.cross_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.38.cross_attn.k_proj.weight False
base_model.model.model.layers.38.cross_attn.v_proj.base_layer.weight False
base_model.model.model.layers.38.cross_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.38.cross_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.38.cross_attn.o_proj.weight False
base_model.model.model.layers.38.cross_attn.q_norm.weight False
base_model.model.model.layers.38.cross_attn.k_norm.weight False
base_model.model.model.layers.38.input_layernorm.weight False
base_model.model.model.layers.38.mlp.gate_proj.weight False
base_model.model.model.layers.38.mlp.up_proj.weight False
base_model.model.model.layers.38.mlp.down_proj.weight False
base_model.model.model.layers.38.post_attention_layernorm.weight False
base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight False
base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight True
base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight True
base_model.model.model.layers.39.self_attn.k_proj.weight False
base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight False
base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight True
base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight True
base_model.model.model.layers.39.self_attn.o_proj.weight False
base_model.model.model.layers.39.mlp.gate_proj.weight False
base_model.model.model.layers.39.mlp.up_proj.weight False
base_model.model.model.layers.39.mlp.down_proj.weight False
base_model.model.model.layers.39.input_layernorm.weight False
base_model.model.model.layers.39.post_attention_layernorm.weight False
base_model.model.model.norm.weight False
base_model.model.lm_head.weight False
base_model.model.vision_model.class_embedding False
base_model.model.vision_model.patch_embedding.weight False
base_model.model.vision_model.gated_positional_embedding.gate False
base_model.model.vision_model.gated_positional_embedding.embedding False
base_model.model.vision_model.gated_positional_embedding.tile_embedding.weight False
base_model.model.vision_model.pre_tile_positional_embedding.gate False
base_model.model.vision_model.pre_tile_positional_embedding.embedding.weight False
base_model.model.vision_model.post_tile_positional_embedding.gate False
base_model.model.vision_model.post_tile_positional_embedding.embedding.weight False
base_model.model.vision_model.layernorm_pre.weight False
base_model.model.vision_model.layernorm_pre.bias False
base_model.model.vision_model.layernorm_post.weight False
base_model.model.vision_model.layernorm_post.bias False
base_model.model.vision_model.transformer.layers.0.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.0.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.0.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.0.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.0.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.0.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.0.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.0.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.0.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.0.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.0.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.0.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.0.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.0.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.0.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.0.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.1.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.1.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.1.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.1.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.1.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.1.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.1.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.1.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.1.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.1.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.1.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.1.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.1.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.1.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.1.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.1.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.2.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.2.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.2.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.2.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.2.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.2.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.2.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.2.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.2.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.2.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.2.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.2.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.2.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.2.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.2.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.2.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.3.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.3.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.3.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.3.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.3.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.3.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.3.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.3.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.3.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.3.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.3.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.3.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.3.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.3.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.3.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.3.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.4.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.4.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.4.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.4.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.4.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.4.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.4.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.4.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.4.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.4.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.4.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.4.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.4.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.4.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.4.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.4.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.5.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.5.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.5.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.5.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.5.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.5.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.5.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.5.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.5.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.5.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.5.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.5.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.5.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.5.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.5.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.5.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.6.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.6.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.6.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.6.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.6.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.6.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.6.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.6.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.6.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.6.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.6.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.6.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.6.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.6.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.6.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.6.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.7.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.7.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.7.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.7.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.7.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.7.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.7.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.7.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.7.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.7.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.7.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.7.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.7.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.7.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.7.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.7.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.8.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.8.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.8.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.8.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.8.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.8.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.8.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.8.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.8.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.8.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.8.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.8.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.8.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.8.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.8.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.8.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.9.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.9.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.9.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.9.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.9.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.9.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.9.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.9.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.9.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.9.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.9.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.9.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.9.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.9.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.9.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.9.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.10.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.10.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.10.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.10.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.10.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.10.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.10.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.10.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.10.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.10.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.10.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.10.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.10.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.10.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.10.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.10.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.11.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.11.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.11.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.11.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.11.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.11.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.11.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.11.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.11.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.11.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.11.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.11.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.11.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.11.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.11.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.11.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.12.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.12.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.12.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.12.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.12.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.12.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.12.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.12.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.12.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.12.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.12.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.12.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.12.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.12.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.12.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.12.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.13.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.13.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.13.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.13.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.13.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.13.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.13.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.13.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.13.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.13.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.13.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.13.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.13.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.13.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.13.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.13.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.14.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.14.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.14.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.14.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.14.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.14.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.14.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.14.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.14.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.14.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.14.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.14.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.14.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.14.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.14.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.14.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.15.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.15.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.15.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.15.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.15.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.15.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.15.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.15.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.15.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.15.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.15.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.15.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.15.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.15.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.15.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.15.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.16.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.16.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.16.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.16.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.16.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.16.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.16.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.16.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.16.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.16.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.16.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.16.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.16.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.16.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.16.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.16.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.17.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.17.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.17.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.17.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.17.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.17.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.17.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.17.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.17.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.17.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.17.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.17.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.17.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.17.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.17.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.17.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.18.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.18.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.18.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.18.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.18.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.18.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.18.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.18.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.18.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.18.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.18.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.18.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.18.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.18.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.18.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.18.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.19.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.19.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.19.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.19.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.19.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.19.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.19.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.19.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.19.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.19.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.19.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.19.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.19.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.19.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.19.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.19.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.20.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.20.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.20.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.20.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.20.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.20.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.20.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.20.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.20.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.20.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.20.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.20.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.20.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.20.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.20.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.20.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.21.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.21.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.21.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.21.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.21.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.21.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.21.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.21.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.21.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.21.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.21.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.21.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.21.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.21.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.21.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.21.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.22.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.22.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.22.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.22.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.22.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.22.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.22.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.22.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.22.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.22.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.22.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.22.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.22.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.22.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.22.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.22.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.23.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.23.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.23.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.23.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.23.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.23.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.23.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.23.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.23.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.23.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.23.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.23.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.23.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.23.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.23.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.23.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.24.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.24.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.24.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.24.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.24.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.24.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.24.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.24.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.24.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.24.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.24.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.24.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.24.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.24.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.24.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.24.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.25.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.25.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.25.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.25.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.25.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.25.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.25.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.25.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.25.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.25.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.25.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.25.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.25.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.25.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.25.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.25.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.26.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.26.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.26.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.26.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.26.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.26.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.26.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.26.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.26.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.26.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.26.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.26.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.26.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.26.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.26.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.26.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.27.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.27.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.27.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.27.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.27.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.27.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.27.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.27.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.27.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.27.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.27.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.27.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.27.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.27.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.27.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.27.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.28.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.28.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.28.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.28.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.28.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.28.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.28.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.28.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.28.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.28.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.28.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.28.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.28.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.28.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.28.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.28.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.29.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.29.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.29.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.29.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.29.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.29.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.29.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.29.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.29.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.29.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.29.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.29.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.29.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.29.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.29.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.29.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.30.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.30.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.30.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.30.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.30.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.30.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.30.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.30.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.30.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.30.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.30.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.30.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.30.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.30.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.30.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.30.post_attention_layernorm.bias False
base_model.model.vision_model.transformer.layers.31.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.31.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.31.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.31.self_attn.k_proj.weight False
base_model.model.vision_model.transformer.layers.31.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.transformer.layers.31.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.transformer.layers.31.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.transformer.layers.31.self_attn.o_proj.weight False
base_model.model.vision_model.transformer.layers.31.mlp.fc1.weight False
base_model.model.vision_model.transformer.layers.31.mlp.fc1.bias False
base_model.model.vision_model.transformer.layers.31.mlp.fc2.weight False
base_model.model.vision_model.transformer.layers.31.mlp.fc2.bias False
base_model.model.vision_model.transformer.layers.31.input_layernorm.weight False
base_model.model.vision_model.transformer.layers.31.input_layernorm.bias False
base_model.model.vision_model.transformer.layers.31.post_attention_layernorm.weight False
base_model.model.vision_model.transformer.layers.31.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.0.gate_attn False
base_model.model.vision_model.global_transformer.layers.0.gate_ffn False
base_model.model.vision_model.global_transformer.layers.0.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.0.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.0.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.0.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.0.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.0.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.0.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.0.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.0.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.0.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.0.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.0.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.0.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.0.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.0.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.0.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.1.gate_attn False
base_model.model.vision_model.global_transformer.layers.1.gate_ffn False
base_model.model.vision_model.global_transformer.layers.1.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.1.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.1.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.1.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.1.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.1.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.1.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.1.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.1.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.1.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.1.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.1.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.1.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.1.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.1.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.1.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.2.gate_attn False
base_model.model.vision_model.global_transformer.layers.2.gate_ffn False
base_model.model.vision_model.global_transformer.layers.2.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.2.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.2.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.2.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.2.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.2.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.2.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.2.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.2.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.2.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.2.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.2.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.2.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.2.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.2.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.2.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.3.gate_attn False
base_model.model.vision_model.global_transformer.layers.3.gate_ffn False
base_model.model.vision_model.global_transformer.layers.3.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.3.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.3.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.3.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.3.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.3.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.3.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.3.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.3.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.3.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.3.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.3.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.3.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.3.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.3.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.3.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.4.gate_attn False
base_model.model.vision_model.global_transformer.layers.4.gate_ffn False
base_model.model.vision_model.global_transformer.layers.4.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.4.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.4.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.4.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.4.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.4.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.4.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.4.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.4.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.4.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.4.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.4.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.4.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.4.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.4.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.4.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.5.gate_attn False
base_model.model.vision_model.global_transformer.layers.5.gate_ffn False
base_model.model.vision_model.global_transformer.layers.5.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.5.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.5.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.5.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.5.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.5.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.5.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.5.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.5.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.5.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.5.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.5.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.5.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.5.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.5.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.5.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.6.gate_attn False
base_model.model.vision_model.global_transformer.layers.6.gate_ffn False
base_model.model.vision_model.global_transformer.layers.6.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.6.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.6.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.6.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.6.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.6.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.6.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.6.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.6.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.6.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.6.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.6.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.6.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.6.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.6.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.6.post_attention_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.7.gate_attn False
base_model.model.vision_model.global_transformer.layers.7.gate_ffn False
base_model.model.vision_model.global_transformer.layers.7.self_attn.q_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.7.self_attn.q_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.7.self_attn.q_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.7.self_attn.k_proj.weight False
base_model.model.vision_model.global_transformer.layers.7.self_attn.v_proj.base_layer.weight False
base_model.model.vision_model.global_transformer.layers.7.self_attn.v_proj.lora_A.default.weight True
base_model.model.vision_model.global_transformer.layers.7.self_attn.v_proj.lora_B.default.weight True
base_model.model.vision_model.global_transformer.layers.7.self_attn.o_proj.weight False
base_model.model.vision_model.global_transformer.layers.7.mlp.fc1.weight False
base_model.model.vision_model.global_transformer.layers.7.mlp.fc1.bias False
base_model.model.vision_model.global_transformer.layers.7.mlp.fc2.weight False
base_model.model.vision_model.global_transformer.layers.7.mlp.fc2.bias False
base_model.model.vision_model.global_transformer.layers.7.input_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.7.input_layernorm.bias False
base_model.model.vision_model.global_transformer.layers.7.post_attention_layernorm.weight False
base_model.model.vision_model.global_transformer.layers.7.post_attention_layernorm.bias False
base_model.model.language_model.model.embed_tokens.weight False
base_model.model.language_model.model.layers.0.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.0.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.0.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.0.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.0.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.0.mlp.up_proj.weight False
base_model.model.language_model.model.layers.0.mlp.down_proj.weight False
base_model.model.language_model.model.layers.0.input_layernorm.weight False
base_model.model.language_model.model.layers.0.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.1.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.1.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.1.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.1.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.1.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.1.mlp.up_proj.weight False
base_model.model.language_model.model.layers.1.mlp.down_proj.weight False
base_model.model.language_model.model.layers.1.input_layernorm.weight False
base_model.model.language_model.model.layers.1.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.2.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.2.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.2.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.2.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.2.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.2.mlp.up_proj.weight False
base_model.model.language_model.model.layers.2.mlp.down_proj.weight False
base_model.model.language_model.model.layers.2.input_layernorm.weight False
base_model.model.language_model.model.layers.2.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.3.cross_attn_attn_gate False
base_model.model.language_model.model.layers.3.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.3.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.3.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.3.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.3.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.3.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.3.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.3.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.3.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.3.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.3.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.3.input_layernorm.weight False
base_model.model.language_model.model.layers.3.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.3.mlp.up_proj.weight False
base_model.model.language_model.model.layers.3.mlp.down_proj.weight False
base_model.model.language_model.model.layers.3.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.4.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.4.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.4.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.4.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.4.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.4.mlp.up_proj.weight False
base_model.model.language_model.model.layers.4.mlp.down_proj.weight False
base_model.model.language_model.model.layers.4.input_layernorm.weight False
base_model.model.language_model.model.layers.4.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.5.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.5.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.5.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.5.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.5.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.5.mlp.up_proj.weight False
base_model.model.language_model.model.layers.5.mlp.down_proj.weight False
base_model.model.language_model.model.layers.5.input_layernorm.weight False
base_model.model.language_model.model.layers.5.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.6.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.6.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.6.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.6.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.6.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.6.mlp.up_proj.weight False
base_model.model.language_model.model.layers.6.mlp.down_proj.weight False
base_model.model.language_model.model.layers.6.input_layernorm.weight False
base_model.model.language_model.model.layers.6.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.7.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.7.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.7.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.7.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.7.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.7.mlp.up_proj.weight False
base_model.model.language_model.model.layers.7.mlp.down_proj.weight False
base_model.model.language_model.model.layers.7.input_layernorm.weight False
base_model.model.language_model.model.layers.7.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.8.cross_attn_attn_gate False
base_model.model.language_model.model.layers.8.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.8.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.8.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.8.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.8.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.8.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.8.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.8.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.8.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.8.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.8.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.8.input_layernorm.weight False
base_model.model.language_model.model.layers.8.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.8.mlp.up_proj.weight False
base_model.model.language_model.model.layers.8.mlp.down_proj.weight False
base_model.model.language_model.model.layers.8.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.9.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.9.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.9.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.9.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.9.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.9.mlp.up_proj.weight False
base_model.model.language_model.model.layers.9.mlp.down_proj.weight False
base_model.model.language_model.model.layers.9.input_layernorm.weight False
base_model.model.language_model.model.layers.9.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.10.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.10.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.10.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.10.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.10.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.10.mlp.up_proj.weight False
base_model.model.language_model.model.layers.10.mlp.down_proj.weight False
base_model.model.language_model.model.layers.10.input_layernorm.weight False
base_model.model.language_model.model.layers.10.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.11.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.11.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.11.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.11.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.11.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.11.mlp.up_proj.weight False
base_model.model.language_model.model.layers.11.mlp.down_proj.weight False
base_model.model.language_model.model.layers.11.input_layernorm.weight False
base_model.model.language_model.model.layers.11.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.12.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.12.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.12.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.12.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.12.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.12.mlp.up_proj.weight False
base_model.model.language_model.model.layers.12.mlp.down_proj.weight False
base_model.model.language_model.model.layers.12.input_layernorm.weight False
base_model.model.language_model.model.layers.12.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.13.cross_attn_attn_gate False
base_model.model.language_model.model.layers.13.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.13.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.13.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.13.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.13.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.13.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.13.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.13.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.13.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.13.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.13.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.13.input_layernorm.weight False
base_model.model.language_model.model.layers.13.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.13.mlp.up_proj.weight False
base_model.model.language_model.model.layers.13.mlp.down_proj.weight False
base_model.model.language_model.model.layers.13.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.14.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.14.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.14.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.14.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.14.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.14.mlp.up_proj.weight False
base_model.model.language_model.model.layers.14.mlp.down_proj.weight False
base_model.model.language_model.model.layers.14.input_layernorm.weight False
base_model.model.language_model.model.layers.14.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.15.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.15.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.15.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.15.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.15.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.15.mlp.up_proj.weight False
base_model.model.language_model.model.layers.15.mlp.down_proj.weight False
base_model.model.language_model.model.layers.15.input_layernorm.weight False
base_model.model.language_model.model.layers.15.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.16.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.16.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.16.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.16.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.16.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.16.mlp.up_proj.weight False
base_model.model.language_model.model.layers.16.mlp.down_proj.weight False
base_model.model.language_model.model.layers.16.input_layernorm.weight False
base_model.model.language_model.model.layers.16.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.17.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.17.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.17.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.17.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.17.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.17.mlp.up_proj.weight False
base_model.model.language_model.model.layers.17.mlp.down_proj.weight False
base_model.model.language_model.model.layers.17.input_layernorm.weight False
base_model.model.language_model.model.layers.17.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.18.cross_attn_attn_gate False
base_model.model.language_model.model.layers.18.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.18.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.18.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.18.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.18.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.18.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.18.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.18.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.18.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.18.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.18.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.18.input_layernorm.weight False
base_model.model.language_model.model.layers.18.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.18.mlp.up_proj.weight False
base_model.model.language_model.model.layers.18.mlp.down_proj.weight False
base_model.model.language_model.model.layers.18.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.19.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.19.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.19.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.19.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.19.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.19.mlp.up_proj.weight False
base_model.model.language_model.model.layers.19.mlp.down_proj.weight False
base_model.model.language_model.model.layers.19.input_layernorm.weight False
base_model.model.language_model.model.layers.19.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.20.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.20.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.20.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.20.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.20.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.20.mlp.up_proj.weight False
base_model.model.language_model.model.layers.20.mlp.down_proj.weight False
base_model.model.language_model.model.layers.20.input_layernorm.weight False
base_model.model.language_model.model.layers.20.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.21.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.21.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.21.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.21.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.21.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.21.mlp.up_proj.weight False
base_model.model.language_model.model.layers.21.mlp.down_proj.weight False
base_model.model.language_model.model.layers.21.input_layernorm.weight False
base_model.model.language_model.model.layers.21.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.22.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.22.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.22.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.22.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.22.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.22.mlp.up_proj.weight False
base_model.model.language_model.model.layers.22.mlp.down_proj.weight False
base_model.model.language_model.model.layers.22.input_layernorm.weight False
base_model.model.language_model.model.layers.22.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.23.cross_attn_attn_gate False
base_model.model.language_model.model.layers.23.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.23.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.23.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.23.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.23.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.23.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.23.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.23.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.23.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.23.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.23.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.23.input_layernorm.weight False
base_model.model.language_model.model.layers.23.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.23.mlp.up_proj.weight False
base_model.model.language_model.model.layers.23.mlp.down_proj.weight False
base_model.model.language_model.model.layers.23.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.24.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.24.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.24.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.24.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.24.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.24.mlp.up_proj.weight False
base_model.model.language_model.model.layers.24.mlp.down_proj.weight False
base_model.model.language_model.model.layers.24.input_layernorm.weight False
base_model.model.language_model.model.layers.24.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.25.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.25.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.25.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.25.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.25.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.25.mlp.up_proj.weight False
base_model.model.language_model.model.layers.25.mlp.down_proj.weight False
base_model.model.language_model.model.layers.25.input_layernorm.weight False
base_model.model.language_model.model.layers.25.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.26.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.26.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.26.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.26.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.26.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.26.mlp.up_proj.weight False
base_model.model.language_model.model.layers.26.mlp.down_proj.weight False
base_model.model.language_model.model.layers.26.input_layernorm.weight False
base_model.model.language_model.model.layers.26.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.27.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.27.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.27.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.27.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.27.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.27.mlp.up_proj.weight False
base_model.model.language_model.model.layers.27.mlp.down_proj.weight False
base_model.model.language_model.model.layers.27.input_layernorm.weight False
base_model.model.language_model.model.layers.27.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.28.cross_attn_attn_gate False
base_model.model.language_model.model.layers.28.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.28.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.28.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.28.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.28.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.28.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.28.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.28.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.28.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.28.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.28.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.28.input_layernorm.weight False
base_model.model.language_model.model.layers.28.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.28.mlp.up_proj.weight False
base_model.model.language_model.model.layers.28.mlp.down_proj.weight False
base_model.model.language_model.model.layers.28.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.29.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.29.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.29.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.29.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.29.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.29.mlp.up_proj.weight False
base_model.model.language_model.model.layers.29.mlp.down_proj.weight False
base_model.model.language_model.model.layers.29.input_layernorm.weight False
base_model.model.language_model.model.layers.29.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.30.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.30.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.30.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.30.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.30.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.30.mlp.up_proj.weight False
base_model.model.language_model.model.layers.30.mlp.down_proj.weight False
base_model.model.language_model.model.layers.30.input_layernorm.weight False
base_model.model.language_model.model.layers.30.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.31.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.31.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.31.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.31.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.31.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.31.mlp.up_proj.weight False
base_model.model.language_model.model.layers.31.mlp.down_proj.weight False
base_model.model.language_model.model.layers.31.input_layernorm.weight False
base_model.model.language_model.model.layers.31.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.32.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.32.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.32.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.32.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.32.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.32.mlp.up_proj.weight False
base_model.model.language_model.model.layers.32.mlp.down_proj.weight False
base_model.model.language_model.model.layers.32.input_layernorm.weight False
base_model.model.language_model.model.layers.32.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.33.cross_attn_attn_gate False
base_model.model.language_model.model.layers.33.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.33.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.33.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.33.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.33.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.33.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.33.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.33.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.33.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.33.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.33.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.33.input_layernorm.weight False
base_model.model.language_model.model.layers.33.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.33.mlp.up_proj.weight False
base_model.model.language_model.model.layers.33.mlp.down_proj.weight False
base_model.model.language_model.model.layers.33.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.34.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.34.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.34.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.34.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.34.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.34.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.34.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.34.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.34.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.34.mlp.up_proj.weight False
base_model.model.language_model.model.layers.34.mlp.down_proj.weight False
base_model.model.language_model.model.layers.34.input_layernorm.weight False
base_model.model.language_model.model.layers.34.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.35.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.35.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.35.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.35.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.35.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.35.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.35.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.35.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.35.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.35.mlp.up_proj.weight False
base_model.model.language_model.model.layers.35.mlp.down_proj.weight False
base_model.model.language_model.model.layers.35.input_layernorm.weight False
base_model.model.language_model.model.layers.35.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.36.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.36.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.36.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.36.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.36.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.36.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.36.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.36.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.36.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.36.mlp.up_proj.weight False
base_model.model.language_model.model.layers.36.mlp.down_proj.weight False
base_model.model.language_model.model.layers.36.input_layernorm.weight False
base_model.model.language_model.model.layers.36.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.37.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.37.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.37.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.37.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.37.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.37.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.37.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.37.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.37.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.37.mlp.up_proj.weight False
base_model.model.language_model.model.layers.37.mlp.down_proj.weight False
base_model.model.language_model.model.layers.37.input_layernorm.weight False
base_model.model.language_model.model.layers.37.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.38.cross_attn_attn_gate False
base_model.model.language_model.model.layers.38.cross_attn_mlp_gate False
base_model.model.language_model.model.layers.38.cross_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.38.cross_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.38.cross_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.38.cross_attn.k_proj.weight False
base_model.model.language_model.model.layers.38.cross_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.38.cross_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.38.cross_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.38.cross_attn.o_proj.weight False
base_model.model.language_model.model.layers.38.cross_attn.q_norm.weight False
base_model.model.language_model.model.layers.38.cross_attn.k_norm.weight False
base_model.model.language_model.model.layers.38.input_layernorm.weight False
base_model.model.language_model.model.layers.38.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.38.mlp.up_proj.weight False
base_model.model.language_model.model.layers.38.mlp.down_proj.weight False
base_model.model.language_model.model.layers.38.post_attention_layernorm.weight False
base_model.model.language_model.model.layers.39.self_attn.q_proj.base_layer.weight False
base_model.model.language_model.model.layers.39.self_attn.q_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.39.self_attn.q_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.39.self_attn.k_proj.weight False
base_model.model.language_model.model.layers.39.self_attn.v_proj.base_layer.weight False
base_model.model.language_model.model.layers.39.self_attn.v_proj.lora_A.default.weight True
base_model.model.language_model.model.layers.39.self_attn.v_proj.lora_B.default.weight True
base_model.model.language_model.model.layers.39.self_attn.o_proj.weight False
base_model.model.language_model.model.layers.39.mlp.gate_proj.weight False
base_model.model.language_model.model.layers.39.mlp.up_proj.weight False
base_model.model.language_model.model.layers.39.mlp.down_proj.weight False
base_model.model.language_model.model.layers.39.input_layernorm.weight False
base_model.model.language_model.model.layers.39.post_attention_layernorm.weight False
base_model.model.language_model.model.norm.weight False
base_model.model.language_model.lm_head.weight False
base_model.model.multi_modal_projector.weight False
base_model.model.multi_modal_projector.bias False
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00001-of-00009.safetensors
Loading weights for: base_model.model.model.embed_tokens.weight
Loading weights for: base_model.model.model.layers.0.input_layernorm.weight
Loading weights for: base_model.model.model.layers.0.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.0.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.0.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.0.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.input_layernorm.weight
Loading weights for: base_model.model.model.layers.1.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.1.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.1.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.1.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.input_layernorm.weight
Loading weights for: base_model.model.model.layers.2.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.2.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.2.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.2.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.3.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.3.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.3.input_layernorm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00002-of-00009.safetensors
Loading weights for: base_model.model.model.layers.3.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.3.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.3.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.input_layernorm.weight
Loading weights for: base_model.model.model.layers.4.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.4.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.4.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.4.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.input_layernorm.weight
Loading weights for: base_model.model.model.layers.5.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.5.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.5.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.5.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.input_layernorm.weight
Loading weights for: base_model.model.model.layers.6.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.6.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.6.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.6.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.input_layernorm.weight
Loading weights for: base_model.model.model.layers.7.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.7.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.7.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.7.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.8.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.8.input_layernorm.weight
Loading weights for: base_model.model.model.layers.8.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.8.mlp.up_proj.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00003-of-00009.safetensors
Loading weights for: base_model.model.model.layers.10.input_layernorm.weight
Loading weights for: base_model.model.model.layers.10.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.10.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.10.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.10.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.input_layernorm.weight
Loading weights for: base_model.model.model.layers.11.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.11.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.11.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.11.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.input_layernorm.weight
Loading weights for: base_model.model.model.layers.12.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.12.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.12.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.12.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.13.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.13.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.13.input_layernorm.weight
Loading weights for: base_model.model.model.layers.13.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.13.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.13.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.14.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.8.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.8.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.input_layernorm.weight
Loading weights for: base_model.model.model.layers.9.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.9.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.9.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.9.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00004-of-00009.safetensors
Loading weights for: base_model.model.model.layers.14.input_layernorm.weight
Loading weights for: base_model.model.model.layers.14.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.14.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.14.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.input_layernorm.weight
Loading weights for: base_model.model.model.layers.15.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.15.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.15.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.15.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.input_layernorm.weight
Loading weights for: base_model.model.model.layers.16.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.16.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.16.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.16.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.input_layernorm.weight
Loading weights for: base_model.model.model.layers.17.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.17.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.17.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.17.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.18.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.18.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.18.input_layernorm.weight
Loading weights for: base_model.model.model.layers.18.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.18.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.18.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.19.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00005-of-00009.safetensors
Loading weights for: base_model.model.model.layers.19.input_layernorm.weight
Loading weights for: base_model.model.model.layers.19.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.19.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.input_layernorm.weight
Loading weights for: base_model.model.model.layers.20.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.20.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.20.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.20.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.input_layernorm.weight
Loading weights for: base_model.model.model.layers.21.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.21.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.21.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.21.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.input_layernorm.weight
Loading weights for: base_model.model.model.layers.22.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.22.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.22.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.22.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.23.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.23.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.23.input_layernorm.weight
Loading weights for: base_model.model.model.layers.23.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.23.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.23.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.input_layernorm.weight
Loading weights for: base_model.model.model.layers.24.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.24.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.24.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.24.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00006-of-00009.safetensors
Loading weights for: base_model.model.model.layers.25.input_layernorm.weight
Loading weights for: base_model.model.model.layers.25.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.25.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.25.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.input_layernorm.weight
Loading weights for: base_model.model.model.layers.26.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.26.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.26.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.26.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.input_layernorm.weight
Loading weights for: base_model.model.model.layers.27.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.27.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.27.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.27.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.28.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.28.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.28.input_layernorm.weight
Loading weights for: base_model.model.model.layers.28.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.28.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.28.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.input_layernorm.weight
Loading weights for: base_model.model.model.layers.29.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.29.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.29.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.29.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.30.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00007-of-00009.safetensors
Loading weights for: base_model.model.model.layers.30.input_layernorm.weight
Loading weights for: base_model.model.model.layers.30.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.30.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.input_layernorm.weight
Loading weights for: base_model.model.model.layers.31.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.31.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.31.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.31.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.input_layernorm.weight
Loading weights for: base_model.model.model.layers.32.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.32.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.32.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.32.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.33.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.33.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.33.input_layernorm.weight
Loading weights for: base_model.model.model.layers.33.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.33.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.33.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.input_layernorm.weight
Loading weights for: base_model.model.model.layers.34.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.34.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.34.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.34.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.input_layernorm.weight
Loading weights for: base_model.model.model.layers.35.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.35.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.35.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.35.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00008-of-00009.safetensors
Loading weights for: base_model.model.model.layers.36.input_layernorm.weight
Loading weights for: base_model.model.model.layers.36.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.36.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.36.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.input_layernorm.weight
Loading weights for: base_model.model.model.layers.37.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.37.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.37.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.37.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_norm.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.38.cross_attn_attn_gate
Loading weights for: base_model.model.model.layers.38.cross_attn_mlp_gate
Loading weights for: base_model.model.model.layers.38.input_layernorm.weight
Loading weights for: base_model.model.model.layers.38.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.38.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.38.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.input_layernorm.weight
Loading weights for: base_model.model.model.layers.39.mlp.down_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.gate_proj.weight
Loading weights for: base_model.model.model.layers.39.mlp.up_proj.weight
Loading weights for: base_model.model.model.layers.39.post_attention_layernorm.weight
Loading weights for: base_model.model.model.layers.39.self_attn.k_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.o_proj.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight
Loading weights for: base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight
Loading weights for: base_model.model.model.norm.weight
Loading shard: /scratch/oe2015/ai_project/DeTikZify/detikzify/evaluate/final_model_checkpoint_2048_captions_noquant/model-00009-of-00009.safetensors
Loading weights for: base_model.model.lm_head.weight
Missing in checkpoint: vision_model.class_embedding
Missing in checkpoint: vision_model.patch_embedding.weight
Missing in checkpoint: vision_model.gated_positional_embedding.gate
Missing in checkpoint: vision_model.gated_positional_embedding.embedding
Missing in checkpoint: vision_model.gated_positional_embedding.tile_embedding.weight
Missing in checkpoint: vision_model.pre_tile_positional_embedding.gate
Missing in checkpoint: vision_model.pre_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.post_tile_positional_embedding.gate
Missing in checkpoint: vision_model.post_tile_positional_embedding.embedding.weight
Missing in checkpoint: vision_model.layernorm_pre.weight
Missing in checkpoint: vision_model.layernorm_pre.bias
Missing in checkpoint: vision_model.layernorm_post.weight
Missing in checkpoint: vision_model.layernorm_post.bias
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.7.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.8.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.8.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.8.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.9.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.9.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.9.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.10.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.10.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.10.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.11.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.11.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.11.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.12.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.12.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.12.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.13.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.13.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.13.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.14.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.14.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.14.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.15.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.15.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.15.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.16.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.16.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.16.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.17.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.17.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.17.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.18.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.18.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.18.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.19.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.19.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.19.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.20.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.20.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.20.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.21.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.21.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.21.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.22.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.22.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.22.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.23.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.23.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.23.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.24.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.24.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.24.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.25.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.25.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.25.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.26.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.26.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.26.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.27.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.27.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.27.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.28.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.28.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.28.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.29.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.29.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.29.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.30.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.30.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.30.post_attention_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.k_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.transformer.layers.31.self_attn.o_proj.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc1.bias
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.weight
Missing in checkpoint: vision_model.transformer.layers.31.mlp.fc2.bias
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.input_layernorm.bias
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.weight
Missing in checkpoint: vision_model.transformer.layers.31.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.0.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.0.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.1.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.1.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.2.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.2.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.3.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.3.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.4.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.4.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.5.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.5.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.6.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.6.post_attention_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_attn
Missing in checkpoint: vision_model.global_transformer.layers.7.gate_ffn
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.q_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.k_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.base_layer.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_A.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.v_proj.lora_B.default.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.self_attn.o_proj.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc1.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.mlp.fc2.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.input_layernorm.bias
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.weight
Missing in checkpoint: vision_model.global_transformer.layers.7.post_attention_layernorm.bias
Matched: model.embed_tokens.weight
Matched: model.layers.0.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.0.self_attn.q_proj.lora_A.default.weight
Difference: 0.057849764823913574
MISMATCH: model.layers.0.self_attn.q_proj.lora_B.default.weight
Difference: 0.03377769887447357
Matched: model.layers.0.self_attn.k_proj.weight
Matched: model.layers.0.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.0.self_attn.v_proj.lora_A.default.weight
Difference: 0.053906846791505814
MISMATCH: model.layers.0.self_attn.v_proj.lora_B.default.weight
Difference: 0.021145479753613472
Matched: model.layers.0.self_attn.o_proj.weight
Matched: model.layers.0.mlp.gate_proj.weight
Matched: model.layers.0.mlp.up_proj.weight
Matched: model.layers.0.mlp.down_proj.weight
Matched: model.layers.0.input_layernorm.weight
Matched: model.layers.0.post_attention_layernorm.weight
Matched: model.layers.1.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.1.self_attn.q_proj.lora_A.default.weight
Difference: 0.0459972620010376
MISMATCH: model.layers.1.self_attn.q_proj.lora_B.default.weight
Difference: 0.026834214106202126
Matched: model.layers.1.self_attn.k_proj.weight
Matched: model.layers.1.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.1.self_attn.v_proj.lora_A.default.weight
Difference: 0.05711553990840912
MISMATCH: model.layers.1.self_attn.v_proj.lora_B.default.weight
Difference: 0.017216987907886505
Matched: model.layers.1.self_attn.o_proj.weight
Matched: model.layers.1.mlp.gate_proj.weight
Matched: model.layers.1.mlp.up_proj.weight
Matched: model.layers.1.mlp.down_proj.weight
Matched: model.layers.1.input_layernorm.weight
Matched: model.layers.1.post_attention_layernorm.weight
Matched: model.layers.2.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.2.self_attn.q_proj.lora_A.default.weight
Difference: 0.04750131815671921
MISMATCH: model.layers.2.self_attn.q_proj.lora_B.default.weight
Difference: 0.028973059728741646
Matched: model.layers.2.self_attn.k_proj.weight
Matched: model.layers.2.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.2.self_attn.v_proj.lora_A.default.weight
Difference: 0.05392784625291824
MISMATCH: model.layers.2.self_attn.v_proj.lora_B.default.weight
Difference: 0.01696699485182762
Matched: model.layers.2.self_attn.o_proj.weight
Matched: model.layers.2.mlp.gate_proj.weight
Matched: model.layers.2.mlp.up_proj.weight
Matched: model.layers.2.mlp.down_proj.weight
Matched: model.layers.2.input_layernorm.weight
Matched: model.layers.2.post_attention_layernorm.weight
Matched: model.layers.3.cross_attn_attn_gate
Matched: model.layers.3.cross_attn_mlp_gate
Matched: model.layers.3.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.3.cross_attn.q_proj.lora_A.default.weight
Difference: 0.03116544522345066
Matched: model.layers.3.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.3.cross_attn.k_proj.weight
Matched: model.layers.3.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.3.cross_attn.v_proj.lora_A.default.weight
Difference: 0.031088631600141525
Matched: model.layers.3.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.3.cross_attn.o_proj.weight
Matched: model.layers.3.cross_attn.q_norm.weight
Matched: model.layers.3.cross_attn.k_norm.weight
Matched: model.layers.3.input_layernorm.weight
Matched: model.layers.3.mlp.gate_proj.weight
Matched: model.layers.3.mlp.up_proj.weight
Matched: model.layers.3.mlp.down_proj.weight
Matched: model.layers.3.post_attention_layernorm.weight
Matched: model.layers.4.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.4.self_attn.q_proj.lora_A.default.weight
Difference: 0.04761471599340439
MISMATCH: model.layers.4.self_attn.q_proj.lora_B.default.weight
Difference: 0.030907349660992622
Matched: model.layers.4.self_attn.k_proj.weight
Matched: model.layers.4.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.4.self_attn.v_proj.lora_A.default.weight
Difference: 0.0559878908097744
MISMATCH: model.layers.4.self_attn.v_proj.lora_B.default.weight
Difference: 0.016812721267342567
Matched: model.layers.4.self_attn.o_proj.weight
Matched: model.layers.4.mlp.gate_proj.weight
Matched: model.layers.4.mlp.up_proj.weight
Matched: model.layers.4.mlp.down_proj.weight
Matched: model.layers.4.input_layernorm.weight
Matched: model.layers.4.post_attention_layernorm.weight
Matched: model.layers.5.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.5.self_attn.q_proj.lora_A.default.weight
Difference: 0.0458027645945549
MISMATCH: model.layers.5.self_attn.q_proj.lora_B.default.weight
Difference: 0.027890190482139587
Matched: model.layers.5.self_attn.k_proj.weight
Matched: model.layers.5.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.5.self_attn.v_proj.lora_A.default.weight
Difference: 0.05501429736614227
MISMATCH: model.layers.5.self_attn.v_proj.lora_B.default.weight
Difference: 0.014605578035116196
Matched: model.layers.5.self_attn.o_proj.weight
Matched: model.layers.5.mlp.gate_proj.weight
Matched: model.layers.5.mlp.up_proj.weight
Matched: model.layers.5.mlp.down_proj.weight
Matched: model.layers.5.input_layernorm.weight
Matched: model.layers.5.post_attention_layernorm.weight
Matched: model.layers.6.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.6.self_attn.q_proj.lora_A.default.weight
Difference: 0.050392020493745804
MISMATCH: model.layers.6.self_attn.q_proj.lora_B.default.weight
Difference: 0.02757621370255947
Matched: model.layers.6.self_attn.k_proj.weight
Matched: model.layers.6.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.6.self_attn.v_proj.lora_A.default.weight
Difference: 0.05043669790029526
MISMATCH: model.layers.6.self_attn.v_proj.lora_B.default.weight
Difference: 0.015358158387243748
Matched: model.layers.6.self_attn.o_proj.weight
Matched: model.layers.6.mlp.gate_proj.weight
Matched: model.layers.6.mlp.up_proj.weight
Matched: model.layers.6.mlp.down_proj.weight
Matched: model.layers.6.input_layernorm.weight
Matched: model.layers.6.post_attention_layernorm.weight
Matched: model.layers.7.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.7.self_attn.q_proj.lora_A.default.weight
Difference: 0.047839488834142685
MISMATCH: model.layers.7.self_attn.q_proj.lora_B.default.weight
Difference: 0.02639484405517578
Matched: model.layers.7.self_attn.k_proj.weight
Matched: model.layers.7.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.7.self_attn.v_proj.lora_A.default.weight
Difference: 0.0630173534154892
MISMATCH: model.layers.7.self_attn.v_proj.lora_B.default.weight
Difference: 0.01541776955127716
Matched: model.layers.7.self_attn.o_proj.weight
Matched: model.layers.7.mlp.gate_proj.weight
Matched: model.layers.7.mlp.up_proj.weight
Matched: model.layers.7.mlp.down_proj.weight
Matched: model.layers.7.input_layernorm.weight
Matched: model.layers.7.post_attention_layernorm.weight
Matched: model.layers.8.cross_attn_attn_gate
Matched: model.layers.8.cross_attn_mlp_gate
Matched: model.layers.8.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.8.cross_attn.q_proj.lora_A.default.weight
Difference: 0.031213708221912384
Matched: model.layers.8.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.8.cross_attn.k_proj.weight
Matched: model.layers.8.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.8.cross_attn.v_proj.lora_A.default.weight
Difference: 0.031189318746328354
Matched: model.layers.8.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.8.cross_attn.o_proj.weight
Matched: model.layers.8.cross_attn.q_norm.weight
Matched: model.layers.8.cross_attn.k_norm.weight
Matched: model.layers.8.input_layernorm.weight
Matched: model.layers.8.mlp.gate_proj.weight
Matched: model.layers.8.mlp.up_proj.weight
Matched: model.layers.8.mlp.down_proj.weight
Matched: model.layers.8.post_attention_layernorm.weight
Matched: model.layers.9.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.9.self_attn.q_proj.lora_A.default.weight
Difference: 0.04959522932767868
MISMATCH: model.layers.9.self_attn.q_proj.lora_B.default.weight
Difference: 0.03051300346851349
Matched: model.layers.9.self_attn.k_proj.weight
Matched: model.layers.9.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.9.self_attn.v_proj.lora_A.default.weight
Difference: 0.05815104395151138
MISMATCH: model.layers.9.self_attn.v_proj.lora_B.default.weight
Difference: 0.01652166247367859
Matched: model.layers.9.self_attn.o_proj.weight
Matched: model.layers.9.mlp.gate_proj.weight
Matched: model.layers.9.mlp.up_proj.weight
Matched: model.layers.9.mlp.down_proj.weight
Matched: model.layers.9.input_layernorm.weight
Matched: model.layers.9.post_attention_layernorm.weight
Matched: model.layers.10.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.10.self_attn.q_proj.lora_A.default.weight
Difference: 0.05585797131061554
MISMATCH: model.layers.10.self_attn.q_proj.lora_B.default.weight
Difference: 0.0239661056548357
Matched: model.layers.10.self_attn.k_proj.weight
Matched: model.layers.10.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.10.self_attn.v_proj.lora_A.default.weight
Difference: 0.059229157865047455
MISMATCH: model.layers.10.self_attn.v_proj.lora_B.default.weight
Difference: 0.017578408122062683
Matched: model.layers.10.self_attn.o_proj.weight
Matched: model.layers.10.mlp.gate_proj.weight
Matched: model.layers.10.mlp.up_proj.weight
Matched: model.layers.10.mlp.down_proj.weight
Matched: model.layers.10.input_layernorm.weight
Matched: model.layers.10.post_attention_layernorm.weight
Matched: model.layers.11.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.11.self_attn.q_proj.lora_A.default.weight
Difference: 0.0512072928249836
MISMATCH: model.layers.11.self_attn.q_proj.lora_B.default.weight
Difference: 0.029371146112680435
Matched: model.layers.11.self_attn.k_proj.weight
Matched: model.layers.11.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.11.self_attn.v_proj.lora_A.default.weight
Difference: 0.05880364775657654
MISMATCH: model.layers.11.self_attn.v_proj.lora_B.default.weight
Difference: 0.01805034838616848
Matched: model.layers.11.self_attn.o_proj.weight
Matched: model.layers.11.mlp.gate_proj.weight
Matched: model.layers.11.mlp.up_proj.weight
Matched: model.layers.11.mlp.down_proj.weight
Matched: model.layers.11.input_layernorm.weight
Matched: model.layers.11.post_attention_layernorm.weight
Matched: model.layers.12.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.12.self_attn.q_proj.lora_A.default.weight
Difference: 0.053224917501211166
MISMATCH: model.layers.12.self_attn.q_proj.lora_B.default.weight
Difference: 0.02535884454846382
Matched: model.layers.12.self_attn.k_proj.weight
Matched: model.layers.12.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.12.self_attn.v_proj.lora_A.default.weight
Difference: 0.05669710785150528
MISMATCH: model.layers.12.self_attn.v_proj.lora_B.default.weight
Difference: 0.01658770442008972
Matched: model.layers.12.self_attn.o_proj.weight
Matched: model.layers.12.mlp.gate_proj.weight
Matched: model.layers.12.mlp.up_proj.weight
Matched: model.layers.12.mlp.down_proj.weight
Matched: model.layers.12.input_layernorm.weight
Matched: model.layers.12.post_attention_layernorm.weight
Matched: model.layers.13.cross_attn_attn_gate
Matched: model.layers.13.cross_attn_mlp_gate
Matched: model.layers.13.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.13.cross_attn.q_proj.lora_A.default.weight
Difference: 0.031140606850385666
Matched: model.layers.13.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.13.cross_attn.k_proj.weight
Matched: model.layers.13.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.13.cross_attn.v_proj.lora_A.default.weight
Difference: 0.031120669096708298
Matched: model.layers.13.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.13.cross_attn.o_proj.weight
Matched: model.layers.13.cross_attn.q_norm.weight
Matched: model.layers.13.cross_attn.k_norm.weight
Matched: model.layers.13.input_layernorm.weight
Matched: model.layers.13.mlp.gate_proj.weight
Matched: model.layers.13.mlp.up_proj.weight
Matched: model.layers.13.mlp.down_proj.weight
Matched: model.layers.13.post_attention_layernorm.weight
Matched: model.layers.14.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.14.self_attn.q_proj.lora_A.default.weight
Difference: 0.052779246121644974
MISMATCH: model.layers.14.self_attn.q_proj.lora_B.default.weight
Difference: 0.02815534546971321
Matched: model.layers.14.self_attn.k_proj.weight
Matched: model.layers.14.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.14.self_attn.v_proj.lora_A.default.weight
Difference: 0.061142608523368835
MISMATCH: model.layers.14.self_attn.v_proj.lora_B.default.weight
Difference: 0.018196461722254753
Matched: model.layers.14.self_attn.o_proj.weight
Matched: model.layers.14.mlp.gate_proj.weight
Matched: model.layers.14.mlp.up_proj.weight
Matched: model.layers.14.mlp.down_proj.weight
Matched: model.layers.14.input_layernorm.weight
Matched: model.layers.14.post_attention_layernorm.weight
Matched: model.layers.15.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.15.self_attn.q_proj.lora_A.default.weight
Difference: 0.05338503420352936
MISMATCH: model.layers.15.self_attn.q_proj.lora_B.default.weight
Difference: 0.03025444969534874
Matched: model.layers.15.self_attn.k_proj.weight
Matched: model.layers.15.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.15.self_attn.v_proj.lora_A.default.weight
Difference: 0.06204020604491234
MISMATCH: model.layers.15.self_attn.v_proj.lora_B.default.weight
Difference: 0.017965102568268776
Matched: model.layers.15.self_attn.o_proj.weight
Matched: model.layers.15.mlp.gate_proj.weight
Matched: model.layers.15.mlp.up_proj.weight
Matched: model.layers.15.mlp.down_proj.weight
Matched: model.layers.15.input_layernorm.weight
Matched: model.layers.15.post_attention_layernorm.weight
Matched: model.layers.16.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.16.self_attn.q_proj.lora_A.default.weight
Difference: 0.057848792523145676
MISMATCH: model.layers.16.self_attn.q_proj.lora_B.default.weight
Difference: 0.027376636862754822
Matched: model.layers.16.self_attn.k_proj.weight
Matched: model.layers.16.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.16.self_attn.v_proj.lora_A.default.weight
Difference: 0.056971475481987
MISMATCH: model.layers.16.self_attn.v_proj.lora_B.default.weight
Difference: 0.01872752606868744
Matched: model.layers.16.self_attn.o_proj.weight
Matched: model.layers.16.mlp.gate_proj.weight
Matched: model.layers.16.mlp.up_proj.weight
Matched: model.layers.16.mlp.down_proj.weight
Matched: model.layers.16.input_layernorm.weight
Matched: model.layers.16.post_attention_layernorm.weight
Matched: model.layers.17.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.17.self_attn.q_proj.lora_A.default.weight
Difference: 0.05682934820652008
MISMATCH: model.layers.17.self_attn.q_proj.lora_B.default.weight
Difference: 0.02583548240363598
Matched: model.layers.17.self_attn.k_proj.weight
Matched: model.layers.17.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.17.self_attn.v_proj.lora_A.default.weight
Difference: 0.05646728351712227
MISMATCH: model.layers.17.self_attn.v_proj.lora_B.default.weight
Difference: 0.01826755329966545
Matched: model.layers.17.self_attn.o_proj.weight
Matched: model.layers.17.mlp.gate_proj.weight
Matched: model.layers.17.mlp.up_proj.weight
Matched: model.layers.17.mlp.down_proj.weight
Matched: model.layers.17.input_layernorm.weight
Matched: model.layers.17.post_attention_layernorm.weight
Matched: model.layers.18.cross_attn_attn_gate
Matched: model.layers.18.cross_attn_mlp_gate
Matched: model.layers.18.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.18.cross_attn.q_proj.lora_A.default.weight
Difference: 0.031114142388105392
Matched: model.layers.18.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.18.cross_attn.k_proj.weight
Matched: model.layers.18.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.18.cross_attn.v_proj.lora_A.default.weight
Difference: 0.03114936128258705
Matched: model.layers.18.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.18.cross_attn.o_proj.weight
Matched: model.layers.18.cross_attn.q_norm.weight
Matched: model.layers.18.cross_attn.k_norm.weight
Matched: model.layers.18.input_layernorm.weight
Matched: model.layers.18.mlp.gate_proj.weight
Matched: model.layers.18.mlp.up_proj.weight
Matched: model.layers.18.mlp.down_proj.weight
Matched: model.layers.18.post_attention_layernorm.weight
Matched: model.layers.19.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.19.self_attn.q_proj.lora_A.default.weight
Difference: 0.058192119002342224
MISMATCH: model.layers.19.self_attn.q_proj.lora_B.default.weight
Difference: 0.024755559861660004
Matched: model.layers.19.self_attn.k_proj.weight
Matched: model.layers.19.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.19.self_attn.v_proj.lora_A.default.weight
Difference: 0.06142039597034454
MISMATCH: model.layers.19.self_attn.v_proj.lora_B.default.weight
Difference: 0.018465476110577583
Matched: model.layers.19.self_attn.o_proj.weight
Matched: model.layers.19.mlp.gate_proj.weight
Matched: model.layers.19.mlp.up_proj.weight
Matched: model.layers.19.mlp.down_proj.weight
Matched: model.layers.19.input_layernorm.weight
Matched: model.layers.19.post_attention_layernorm.weight
Matched: model.layers.20.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.20.self_attn.q_proj.lora_A.default.weight
Difference: 0.050311628729104996
MISMATCH: model.layers.20.self_attn.q_proj.lora_B.default.weight
Difference: 0.02837684378027916
Matched: model.layers.20.self_attn.k_proj.weight
Matched: model.layers.20.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.20.self_attn.v_proj.lora_A.default.weight
Difference: 0.06361141800880432
MISMATCH: model.layers.20.self_attn.v_proj.lora_B.default.weight
Difference: 0.020800314843654633
Matched: model.layers.20.self_attn.o_proj.weight
Matched: model.layers.20.mlp.gate_proj.weight
Matched: model.layers.20.mlp.up_proj.weight
Matched: model.layers.20.mlp.down_proj.weight
Matched: model.layers.20.input_layernorm.weight
Matched: model.layers.20.post_attention_layernorm.weight
Matched: model.layers.21.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.21.self_attn.q_proj.lora_A.default.weight
Difference: 0.05620123818516731
MISMATCH: model.layers.21.self_attn.q_proj.lora_B.default.weight
Difference: 0.029203858226537704
Matched: model.layers.21.self_attn.k_proj.weight
Matched: model.layers.21.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.21.self_attn.v_proj.lora_A.default.weight
Difference: 0.06150035932660103
MISMATCH: model.layers.21.self_attn.v_proj.lora_B.default.weight
Difference: 0.019731447100639343
Matched: model.layers.21.self_attn.o_proj.weight
Matched: model.layers.21.mlp.gate_proj.weight
Matched: model.layers.21.mlp.up_proj.weight
Matched: model.layers.21.mlp.down_proj.weight
Matched: model.layers.21.input_layernorm.weight
Matched: model.layers.21.post_attention_layernorm.weight
Matched: model.layers.22.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.22.self_attn.q_proj.lora_A.default.weight
Difference: 0.05530471354722977
MISMATCH: model.layers.22.self_attn.q_proj.lora_B.default.weight
Difference: 0.0301032904535532
Matched: model.layers.22.self_attn.k_proj.weight
Matched: model.layers.22.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.22.self_attn.v_proj.lora_A.default.weight
Difference: 0.0592370480298996
MISMATCH: model.layers.22.self_attn.v_proj.lora_B.default.weight
Difference: 0.020034050568938255
Matched: model.layers.22.self_attn.o_proj.weight
Matched: model.layers.22.mlp.gate_proj.weight
Matched: model.layers.22.mlp.up_proj.weight
Matched: model.layers.22.mlp.down_proj.weight
Matched: model.layers.22.input_layernorm.weight
Matched: model.layers.22.post_attention_layernorm.weight
Matched: model.layers.23.cross_attn_attn_gate
Matched: model.layers.23.cross_attn_mlp_gate
Matched: model.layers.23.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.23.cross_attn.q_proj.lora_A.default.weight
Difference: 0.03118886798620224
Matched: model.layers.23.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.23.cross_attn.k_proj.weight
Matched: model.layers.23.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.23.cross_attn.v_proj.lora_A.default.weight
Difference: 0.031112462282180786
Matched: model.layers.23.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.23.cross_attn.o_proj.weight
Matched: model.layers.23.cross_attn.q_norm.weight
Matched: model.layers.23.cross_attn.k_norm.weight
Matched: model.layers.23.input_layernorm.weight
Matched: model.layers.23.mlp.gate_proj.weight
Matched: model.layers.23.mlp.up_proj.weight
Matched: model.layers.23.mlp.down_proj.weight
Matched: model.layers.23.post_attention_layernorm.weight
Matched: model.layers.24.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.24.self_attn.q_proj.lora_A.default.weight
Difference: 0.05246216058731079
MISMATCH: model.layers.24.self_attn.q_proj.lora_B.default.weight
Difference: 0.03962392359972
Matched: model.layers.24.self_attn.k_proj.weight
Matched: model.layers.24.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.24.self_attn.v_proj.lora_A.default.weight
Difference: 0.06232964247465134
MISMATCH: model.layers.24.self_attn.v_proj.lora_B.default.weight
Difference: 0.01864292100071907
Matched: model.layers.24.self_attn.o_proj.weight
Matched: model.layers.24.mlp.gate_proj.weight
Matched: model.layers.24.mlp.up_proj.weight
Matched: model.layers.24.mlp.down_proj.weight
Matched: model.layers.24.input_layernorm.weight
Matched: model.layers.24.post_attention_layernorm.weight
Matched: model.layers.25.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.25.self_attn.q_proj.lora_A.default.weight
Difference: 0.0547025166451931
MISMATCH: model.layers.25.self_attn.q_proj.lora_B.default.weight
Difference: 0.0280839242041111
Matched: model.layers.25.self_attn.k_proj.weight
Matched: model.layers.25.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.25.self_attn.v_proj.lora_A.default.weight
Difference: 0.06679685413837433
MISMATCH: model.layers.25.self_attn.v_proj.lora_B.default.weight
Difference: 0.021194882690906525
Matched: model.layers.25.self_attn.o_proj.weight
Matched: model.layers.25.mlp.gate_proj.weight
Matched: model.layers.25.mlp.up_proj.weight
Matched: model.layers.25.mlp.down_proj.weight
Matched: model.layers.25.input_layernorm.weight
Matched: model.layers.25.post_attention_layernorm.weight
Matched: model.layers.26.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.26.self_attn.q_proj.lora_A.default.weight
Difference: 0.049445416778326035
MISMATCH: model.layers.26.self_attn.q_proj.lora_B.default.weight
Difference: 0.030428748577833176
Matched: model.layers.26.self_attn.k_proj.weight
Matched: model.layers.26.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.26.self_attn.v_proj.lora_A.default.weight
Difference: 0.06369596719741821
MISMATCH: model.layers.26.self_attn.v_proj.lora_B.default.weight
Difference: 0.018913254141807556
Matched: model.layers.26.self_attn.o_proj.weight
Matched: model.layers.26.mlp.gate_proj.weight
Matched: model.layers.26.mlp.up_proj.weight
Matched: model.layers.26.mlp.down_proj.weight
Matched: model.layers.26.input_layernorm.weight
Matched: model.layers.26.post_attention_layernorm.weight
Matched: model.layers.27.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.27.self_attn.q_proj.lora_A.default.weight
Difference: 0.05411997437477112
MISMATCH: model.layers.27.self_attn.q_proj.lora_B.default.weight
Difference: 0.026944447308778763
Matched: model.layers.27.self_attn.k_proj.weight
Matched: model.layers.27.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.27.self_attn.v_proj.lora_A.default.weight
Difference: 0.06539178639650345
MISMATCH: model.layers.27.self_attn.v_proj.lora_B.default.weight
Difference: 0.020452260971069336
Matched: model.layers.27.self_attn.o_proj.weight
Matched: model.layers.27.mlp.gate_proj.weight
Matched: model.layers.27.mlp.up_proj.weight
Matched: model.layers.27.mlp.down_proj.weight
Matched: model.layers.27.input_layernorm.weight
Matched: model.layers.27.post_attention_layernorm.weight
Matched: model.layers.28.cross_attn_attn_gate
Matched: model.layers.28.cross_attn_mlp_gate
Matched: model.layers.28.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.28.cross_attn.q_proj.lora_A.default.weight
Difference: 0.03123377077281475
Matched: model.layers.28.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.28.cross_attn.k_proj.weight
Matched: model.layers.28.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.28.cross_attn.v_proj.lora_A.default.weight
Difference: 0.031076369807124138
Matched: model.layers.28.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.28.cross_attn.o_proj.weight
Matched: model.layers.28.cross_attn.q_norm.weight
Matched: model.layers.28.cross_attn.k_norm.weight
Matched: model.layers.28.input_layernorm.weight
Matched: model.layers.28.mlp.gate_proj.weight
Matched: model.layers.28.mlp.up_proj.weight
Matched: model.layers.28.mlp.down_proj.weight
Matched: model.layers.28.post_attention_layernorm.weight
Matched: model.layers.29.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.29.self_attn.q_proj.lora_A.default.weight
Difference: 0.057508356869220734
MISMATCH: model.layers.29.self_attn.q_proj.lora_B.default.weight
Difference: 0.029014887288212776
Matched: model.layers.29.self_attn.k_proj.weight
Matched: model.layers.29.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.29.self_attn.v_proj.lora_A.default.weight
Difference: 0.06960923969745636
MISMATCH: model.layers.29.self_attn.v_proj.lora_B.default.weight
Difference: 0.017590168863534927
Matched: model.layers.29.self_attn.o_proj.weight
Matched: model.layers.29.mlp.gate_proj.weight
Matched: model.layers.29.mlp.up_proj.weight
Matched: model.layers.29.mlp.down_proj.weight
Matched: model.layers.29.input_layernorm.weight
Matched: model.layers.29.post_attention_layernorm.weight
Matched: model.layers.30.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.30.self_attn.q_proj.lora_A.default.weight
Difference: 0.05006854981184006
MISMATCH: model.layers.30.self_attn.q_proj.lora_B.default.weight
Difference: 0.03413061052560806
Matched: model.layers.30.self_attn.k_proj.weight
Matched: model.layers.30.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.30.self_attn.v_proj.lora_A.default.weight
Difference: 0.06159806251525879
MISMATCH: model.layers.30.self_attn.v_proj.lora_B.default.weight
Difference: 0.018692955374717712
Matched: model.layers.30.self_attn.o_proj.weight
Matched: model.layers.30.mlp.gate_proj.weight
Matched: model.layers.30.mlp.up_proj.weight
Matched: model.layers.30.mlp.down_proj.weight
Matched: model.layers.30.input_layernorm.weight
Matched: model.layers.30.post_attention_layernorm.weight
Matched: model.layers.31.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.31.self_attn.q_proj.lora_A.default.weight
Difference: 0.05195913836359978
MISMATCH: model.layers.31.self_attn.q_proj.lora_B.default.weight
Difference: 0.029879728332161903
Matched: model.layers.31.self_attn.k_proj.weight
Matched: model.layers.31.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.31.self_attn.v_proj.lora_A.default.weight
Difference: 0.06319398432970047
MISMATCH: model.layers.31.self_attn.v_proj.lora_B.default.weight
Difference: 0.01846458949148655
Matched: model.layers.31.self_attn.o_proj.weight
Matched: model.layers.31.mlp.gate_proj.weight
Matched: model.layers.31.mlp.up_proj.weight
Matched: model.layers.31.mlp.down_proj.weight
Matched: model.layers.31.input_layernorm.weight
Matched: model.layers.31.post_attention_layernorm.weight
Matched: model.layers.32.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.32.self_attn.q_proj.lora_A.default.weight
Difference: 0.05036133900284767
MISMATCH: model.layers.32.self_attn.q_proj.lora_B.default.weight
Difference: 0.030315523967146873
Matched: model.layers.32.self_attn.k_proj.weight
Matched: model.layers.32.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.32.self_attn.v_proj.lora_A.default.weight
Difference: 0.06937479972839355
MISMATCH: model.layers.32.self_attn.v_proj.lora_B.default.weight
Difference: 0.01808261312544346
Matched: model.layers.32.self_attn.o_proj.weight
Matched: model.layers.32.mlp.gate_proj.weight
Matched: model.layers.32.mlp.up_proj.weight
Matched: model.layers.32.mlp.down_proj.weight
Matched: model.layers.32.input_layernorm.weight
Matched: model.layers.32.post_attention_layernorm.weight
Matched: model.layers.33.cross_attn_attn_gate
Matched: model.layers.33.cross_attn_mlp_gate
Matched: model.layers.33.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.33.cross_attn.q_proj.lora_A.default.weight
Difference: 0.03117307461798191
Matched: model.layers.33.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.33.cross_attn.k_proj.weight
Matched: model.layers.33.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.33.cross_attn.v_proj.lora_A.default.weight
Difference: 0.03124401532113552
Matched: model.layers.33.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.33.cross_attn.o_proj.weight
Matched: model.layers.33.cross_attn.q_norm.weight
Matched: model.layers.33.cross_attn.k_norm.weight
Matched: model.layers.33.input_layernorm.weight
Matched: model.layers.33.mlp.gate_proj.weight
Matched: model.layers.33.mlp.up_proj.weight
Matched: model.layers.33.mlp.down_proj.weight
Matched: model.layers.33.post_attention_layernorm.weight
Matched: model.layers.34.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.34.self_attn.q_proj.lora_A.default.weight
Difference: 0.05214491859078407
MISMATCH: model.layers.34.self_attn.q_proj.lora_B.default.weight
Difference: 0.036823973059654236
Matched: model.layers.34.self_attn.k_proj.weight
Matched: model.layers.34.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.34.self_attn.v_proj.lora_A.default.weight
Difference: 0.0616597905755043
MISMATCH: model.layers.34.self_attn.v_proj.lora_B.default.weight
Difference: 0.020842716097831726
Matched: model.layers.34.self_attn.o_proj.weight
Matched: model.layers.34.mlp.gate_proj.weight
Matched: model.layers.34.mlp.up_proj.weight
Matched: model.layers.34.mlp.down_proj.weight
Matched: model.layers.34.input_layernorm.weight
Matched: model.layers.34.post_attention_layernorm.weight
Matched: model.layers.35.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.35.self_attn.q_proj.lora_A.default.weight
Difference: 0.05851789563894272
MISMATCH: model.layers.35.self_attn.q_proj.lora_B.default.weight
Difference: 0.02826552465558052
Matched: model.layers.35.self_attn.k_proj.weight
Matched: model.layers.35.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.35.self_attn.v_proj.lora_A.default.weight
Difference: 0.06617836654186249
MISMATCH: model.layers.35.self_attn.v_proj.lora_B.default.weight
Difference: 0.019583215937018394
Matched: model.layers.35.self_attn.o_proj.weight
Matched: model.layers.35.mlp.gate_proj.weight
Matched: model.layers.35.mlp.up_proj.weight
Matched: model.layers.35.mlp.down_proj.weight
Matched: model.layers.35.input_layernorm.weight
Matched: model.layers.35.post_attention_layernorm.weight
Matched: model.layers.36.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.36.self_attn.q_proj.lora_A.default.weight
Difference: 0.051516275852918625
MISMATCH: model.layers.36.self_attn.q_proj.lora_B.default.weight
Difference: 0.03320004418492317
Matched: model.layers.36.self_attn.k_proj.weight
Matched: model.layers.36.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.36.self_attn.v_proj.lora_A.default.weight
Difference: 0.06326775997877121
MISMATCH: model.layers.36.self_attn.v_proj.lora_B.default.weight
Difference: 0.018644867464900017
Matched: model.layers.36.self_attn.o_proj.weight
Matched: model.layers.36.mlp.gate_proj.weight
Matched: model.layers.36.mlp.up_proj.weight
Matched: model.layers.36.mlp.down_proj.weight
Matched: model.layers.36.input_layernorm.weight
Matched: model.layers.36.post_attention_layernorm.weight
Matched: model.layers.37.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.37.self_attn.q_proj.lora_A.default.weight
Difference: 0.06406336277723312
MISMATCH: model.layers.37.self_attn.q_proj.lora_B.default.weight
Difference: 0.02838628552854061
Matched: model.layers.37.self_attn.k_proj.weight
Matched: model.layers.37.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.37.self_attn.v_proj.lora_A.default.weight
Difference: 0.07102330029010773
MISMATCH: model.layers.37.self_attn.v_proj.lora_B.default.weight
Difference: 0.038343869149684906
Matched: model.layers.37.self_attn.o_proj.weight
Matched: model.layers.37.mlp.gate_proj.weight
Matched: model.layers.37.mlp.up_proj.weight
Matched: model.layers.37.mlp.down_proj.weight
Matched: model.layers.37.input_layernorm.weight
Matched: model.layers.37.post_attention_layernorm.weight
Matched: model.layers.38.cross_attn_attn_gate
Matched: model.layers.38.cross_attn_mlp_gate
Matched: model.layers.38.cross_attn.q_proj.base_layer.weight
MISMATCH: model.layers.38.cross_attn.q_proj.lora_A.default.weight
Difference: 0.031154807657003403
Matched: model.layers.38.cross_attn.q_proj.lora_B.default.weight
Matched: model.layers.38.cross_attn.k_proj.weight
Matched: model.layers.38.cross_attn.v_proj.base_layer.weight
MISMATCH: model.layers.38.cross_attn.v_proj.lora_A.default.weight
Difference: 0.031026044860482216
Matched: model.layers.38.cross_attn.v_proj.lora_B.default.weight
Matched: model.layers.38.cross_attn.o_proj.weight
Matched: model.layers.38.cross_attn.q_norm.weight
Matched: model.layers.38.cross_attn.k_norm.weight
Matched: model.layers.38.input_layernorm.weight
Matched: model.layers.38.mlp.gate_proj.weight
Matched: model.layers.38.mlp.up_proj.weight
Matched: model.layers.38.mlp.down_proj.weight
Matched: model.layers.38.post_attention_layernorm.weight
Matched: model.layers.39.self_attn.q_proj.base_layer.weight
MISMATCH: model.layers.39.self_attn.q_proj.lora_A.default.weight
Difference: 0.05270768702030182
MISMATCH: model.layers.39.self_attn.q_proj.lora_B.default.weight
Difference: 0.031192896887660027
Matched: model.layers.39.self_attn.k_proj.weight
Matched: model.layers.39.self_attn.v_proj.base_layer.weight
MISMATCH: model.layers.39.self_attn.v_proj.lora_A.default.weight
Difference: 0.07221033424139023
MISMATCH: model.layers.39.self_attn.v_proj.lora_B.default.weight
Difference: 0.02826838195323944
Matched: model.layers.39.self_attn.o_proj.weight
Matched: model.layers.39.mlp.gate_proj.weight
Matched: model.layers.39.mlp.up_proj.weight
Matched: model.layers.39.mlp.down_proj.weight
Matched: model.layers.39.input_layernorm.weight
Matched: model.layers.39.post_attention_layernorm.weight
Matched: model.norm.weight
Missing in checkpoint: language_model.lm_head.weight
Missing in checkpoint: multi_modal_projector.weight
Missing in checkpoint: multi_modal_projector.bias
Extra in checkpoint: lm_head.weight
Some layers did not match.
LoRA model loaded successfully!
bbbbb
cuda:0
Map (num_proc=4):   0%|          | 0/49660 [00:00<?, ? examples/s]Map (num_proc=4):   1%|          | 378/49660 [00:00<00:18, 2594.39 examples/s]Map (num_proc=4):   7%|▋         | 3397/49660 [00:00<00:04, 10584.18 examples/s]Map (num_proc=4):  13%|█▎        | 6630/49660 [00:00<00:02, 17413.01 examples/s]Map (num_proc=4):  18%|█▊        | 8904/49660 [00:00<00:02, 15640.63 examples/s]Map (num_proc=4):  23%|██▎       | 11641/49660 [00:00<00:02, 16688.74 examples/s]Map (num_proc=4):  30%|██▉       | 14717/49660 [00:00<00:01, 20246.63 examples/s]Map (num_proc=4):  35%|███▍      | 17344/49660 [00:01<00:01, 18929.25 examples/s]Map (num_proc=4):  40%|███▉      | 19687/49660 [00:01<00:01, 17600.17 examples/s]Map (num_proc=4):  46%|████▌     | 22717/49660 [00:01<00:01, 20583.93 examples/s]Map (num_proc=4):  51%|█████     | 25373/49660 [00:01<00:01, 19268.98 examples/s]Map (num_proc=4):  56%|█████▌    | 27705/49660 [00:01<00:01, 18275.86 examples/s]Map (num_proc=4):  62%|██████▏   | 30715/49660 [00:01<00:00, 21009.13 examples/s]Map (num_proc=4):  67%|██████▋   | 33368/49660 [00:01<00:00, 19756.41 examples/s]Map (num_proc=4):  72%|███████▏  | 35673/49660 [00:01<00:00, 19344.82 examples/s]Map (num_proc=4):  77%|███████▋  | 38255/49660 [00:02<00:00, 20747.96 examples/s]Map (num_proc=4):  81%|████████▏ | 40467/49660 [00:02<00:00, 19115.71 examples/s]Map (num_proc=4):  88%|████████▊ | 43747/49660 [00:02<00:00, 20380.03 examples/s]Map (num_proc=4):  95%|█████████▍| 47170/49660 [00:02<00:00, 23019.50 examples/s]Map (num_proc=4): 100%|█████████▉| 49659/49660 [00:02<00:00, 19851.20 examples/s]Map (num_proc=4): 100%|██████████| 49660/49660 [00:02<00:00, 17907.98 examples/s]
Length of the updated dataset: 49660

system

Today Date: 18 Nov 2024

You are a helpful AI that can generate tikz code from images.
user

This is a picture of a scientific figure  as well as its caption. The caption reads: 'The image depicts a diagram with two grey semicircles, one at the top and one at the bottom, connected by a vertical line. Each semicircle has four black dots on its circumference. Several black lines radiate from the center of the diagram, intersecting the semicircles. A red curve labeled "c" runs from the lower semicircle to the upper semicircle, intersecting the vertical line and some of the black lines. The diagram appears to represent a geometric or physical concept involving symmetry and intersections.

This description should help you write the TikZ code for the figure.'.Generate LaTeX code that draws this scientific figure using TikZ. Ensure that the LaTeX code is self-contained and does not require any packages except TikZ-related imports. Don't forget to include \usepackage{tikz}! I understand that this is a challenging task, so do your best. Return your result in a ```latex code block.
assistant

\documentclass{article}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}
\section*{Diagram}
\begin{tikzpicture}[scale=0.5]
  \draw[fill=gray] (0,0) circle (0.5) -- (-1,1) -- (-2,0) -- (-1,-1) -- (0,0);
  \draw[fill=gray] (0,-4) circle (0.5) -- (1,1) -- (2,0) -- (1,-1) -- (0,-4);
  \draw[very thick] (-1,1) -- (-1,-1);
  \draw[very thick] (1,1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0,0);
  \draw[very thick] (-1,1) -- (1,1);
  \draw[very thick] (-1,-1) -- (1,-1);
  \draw[very thick] (-2,0) -- (2,0);
  \draw[very thick] (0,-4) -- (0
########################################################
Error: Could not locate LaTeX code markers.
Processing sample 1/49660 - Caption: The image depicts a diagram with two grey semicircles, one at the top and one at the bottom, connected by a vertical line. Each semicircle has four black dots on its circumference. Several black lines radiate from the center of the diagram, intersecting the semicircles. A red curve labeled "c" runs from the lower semicircle to the upper semicircle, intersecting the vertical line and some of the black lines. The diagram appears to represent a geometric or physical concept involving symmetry and intersections.

This description should help you write the TikZ code for the figure.
TEX Edit Distance for sample 1: 0.8216123580932617
slurmstepd: error: *** JOB 9454348 ON cn259 CANCELLED AT 2024-11-18T07:49:16 ***
